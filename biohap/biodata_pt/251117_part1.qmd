---
authors: David PaleÄek [dpalecek@ualg.pt](mailto:dpalecek@ualg.pt)
editor: source
---

There are many parallel technologies to employ in each step. The pipeline here relies on `fuseki` for exposing the SPARQL endpoint and uses `python` to query the graph.

### Outline

1. RO-Crates
2. fuseki setup
3. generate and ingest graph
4. basic SPARQL queries
5. Jupyter NB for local SPARQL
6. Combine local and public endpoints

::: {.panel-tabset}

#### RO-Crates

A RO-Crate is an integrated view through which you can see an entire Research Object; the methods, the data, the output and the outcomes of a project or a piece of work. Linking all this together enables the sharing of research outputs with their context, as a coherent whole. [https://www.researchobject.org](https://www.researchobject.org/ro-crate/about_ro_crate)

![Image credit: Goble, C. (2024, February 16). FAIR Digital Research Objects: Metadata Journeys. University of Auckland Seminar, Auckland. Zenodo. https://doi.org/10.5281/zenodo.10710142](figs/ro-crate_packaging.png)

Note that we already have some of the EMO-BON RO-Crates locall in `/emobon_demo/ro-crates/analysis-results-cluster-01-crate/`.

#### fuseki

Expose your triples as a SPARQL end-point accessible over HTTP. Fuseki provides REST-style interaction with your RDF data. Navigate to your downloaded fuseki files, open the archive if you have not done it yet and start the server as follows

```bash
cd apache-jena-fuseki-5.6.0/

# start the server
./fuseki-server
```

This should show you similar to

```bash
15:46:47 INFO  Config          :: Fuseki Base = /home/david-palecek/coding/apache-jena-fuseki-5.6.0/run
15:46:47 INFO  Config          :: No databases: dir=/home/david-palecek/coding/apache-jena-fuseki-5.6.0/run/configuration
15:46:48 INFO  Config          :: UI Base = fuseki-server.jar
15:46:48 INFO  Shiro           :: Shiro configuration: file:/home/david-palecek/coding/apache-jena-fuseki-5.6.0/run/shiro.ini
```

Now opening your localhost, http://localhost:3030/, you should see the following page

![fuseki server](figs/fuseki_screenshot.png)

##### Upload data and generate a graph

In `fuseki`, go `new dataset` -> give it a name `emobon` -> `add data` -> select the `.ttl` files and `upload all` or one by one. This is a shortcut, because many times you will not have access to the `.ttl` serialized version of the RO-Crate. In that case, the starting point is `ro-crate-metadata.json` file, which is always at the root of every RO-Crate.

::: {.callout-note title="Pseudo python code"}

  ```python
  # read the json file and convert it to a graph
  file = ".../ro-crate-metadata.json"
  with open(file, "r", encoding="utf-8") as f:
      jsonld_text = f.read()
  g = jsonld_to_rdflib(jsonld_text)

  # upload data to the endpoint
  resp = requests.put(
    fuseki_url,
    data=g.serialize(format="turtle").encode("utf-8"),
    headers={"Content-Type": "text/turtle"},
    timeout=60,
  )
  ```

:::

The full example is in this [notebook](https://github.com/emo-bon/momics-demos/blob/main/wfs_extra/fuseki_create_sparql_endpoint_GH.ipynb).

#### SPARQL

The direct way from `fuseki` is to edit the query in `actions`. See more detailed but accesible introduction with [examples](https://www.w3.org/TR/sparql11-query/). When you click `query`, the default query is shown

```sql
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT * WHERE {
  ?sub ?pred ?obj .
} LIMIT 10
```

, which queries all the triples in the graph.

::: {.callout-note collapse="true" title="How many triples do we have?"}

  ```sql
  SELECT (COUNT(*) as ?c)
  WHERE {
    ?subject ?predicate ?object .
  }
  LIMIT 10
  ```

:::

::: {.callout-note collapse="true" title="Filter out all the `text/html` files."}

  ```sql
  PREFIX sdo: <http://schema.org/>

  SELECT ?x ?dtype
  WHERE {
    ?x sdo:encodingFormat ?dtype .
    FILTER regex(str(?dtype), "^text/html", "i")
  }
  ```

:::

::: {.callout-note collapse="true" title="Return also the `sdo:downloadUrl` of those files."}

  ```sql
  PREFIX sdo: <http://schema.org/>

  SELECT ?x ?dtype ?durl
  WHERE {
    ?x sdo:encodingFormat ?dtype ;
      sdo:downloadUrl ?durl .
    FILTER regex(str(?dtype), "^text/html", "i")
  }
  ```

:::

Now you can click the link of one of the `Krona` files, open them in the browser and with no surprise, it is a Krona plot.

Now let's get the real metaGOflow outputs, specifically SSU taxonomy tables. There are several ways how to do it.

**1. RO-Crate browser**
EMBRC hosts the [RO-Crate viewer](https://data.emobon.embrc.eu/) for the EMO-BON data

**2. Local SPARQL**
Write a query to get the `sdo:downloadUrl` links, put them into your browser, which automatically triggers the download. *Hint for the exercise below, match `regex` of the `object` on "SSU-taxonomy-summary".*

::: {.callout-note collapse="true" title="Return SSU taxonomy download links."}

  ```sql
  PREFIX sdo: <http://schema.org/>

  SELECT ?subject ?predicate ?object ?durl
  WHERE {
    ?subject ?predicate ?object .
    FILTER regex(str(?object), "SSU-taxonomy-summary", "i")
    OPTIONAL { ?object sdo:downloadUrl ?durl }
  }
  LIMIT 50
  ```

:::

**3. Use data version control (`DVC`) tool**
Shown in the python implementation of the above, which follows in the next section. For more on dvc, check its [documentation](https://dvc.org/doc).

#### Python implementation

It is possible to export the tables form the `fuseki` for subsequent work, but let's do everything seamlessly from a jupyter notebook.

First we reproduce what we did until now from [Jupyter notebook](https://github.com/emo-bon/momics-demos/blob/main/wfs_extra/02_fuseki_emobon_queries.ipynb) which is part of the `momics-demos` repository, therefore you have it already locally with all installed dependencies too.

Since we have already ingested the triples from the RO-Crates, we just need to query the existing endpoint

```python
q = """
SELECT (COUNT(*) AS ?c) 
WHERE { 
  ?s ?p ?o
}
"""

r = requests.get("http://localhost:3030/emobon/query", params={"query": q}, headers={"Accept": "application/sparql-results+json"})
print(r.json())
```

returned `json` is relatively easy to convert to a dataframe (`sparql_json_to_df` function)

```python
def sparql_json_to_df(sparql_json):
    """
    Convert a SPARQL SELECT query JSON result to a pandas DataFrame.
    
    Parameters
    ----------
    sparql_json : dict
        JSON returned by Fuseki / SPARQL endpoint with Accept: application/sparql-results+json
    
    Returns
    -------
    pd.DataFrame
    """
    vars_ = sparql_json.get("head", {}).get("vars", [])
    rows = []

    for binding in sparql_json.get("results", {}).get("bindings", []):
        row = {}
        for var in vars_:
            # Some results might not bind all variables
            if var in binding:
                row[var] = binding[var]["value"]
            else:
                row[var] = None
        rows.append(row)

    df = pd.DataFrame(rows, columns=vars_)
    return df
```

[Second notebook](https://github.com/emo-bon/momics-demos/blob/main/wfs_extra/01_fuseki_emobon_full.ipynb) includes all the steps of setting up the SparQL endpoint and also combining local queries with public endpoints from wikidata and UniProt.

::: {.callout-tip}
There is a python wrapper for SPARQL [SPARQLWrapper](https://sparqlwrapper.readthedocs.io/en/latest/main.html), which is `pip` installable and can be used as a standard python module but also as a command line script.
:::

:::

------------------------------------------------------------------------
