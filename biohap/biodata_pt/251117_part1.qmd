---
authors: David PaleÄek [dpalecek@ualg.pt](mailto:dpalecek@ualg.pt)
editor: source
---

There are many parallel technologies to employ in each step. The pipeline here relies on `fuseki` for exposing the SPARQL endpoint and uses `python` to query the graph.

::: {.panel-tabset}

#### RO-Crates

A RO-Crate is an integrated view through which you can see an entire Research Object; the methods, the data, the output and the outcomes of a project or a piece of work. Linking all this together enables the sharing of research outputs with their context, as a coherent whole. [https://www.researchobject.org](https://www.researchobject.org/ro-crate/about_ro_crate)

![Image credit: Goble, C. (2024, February 16). FAIR Digital Research Objects: Metadata Journeys. University of Auckland Seminar, Auckland. Zenodo. https://doi.org/10.5281/zenodo.10710142](figs/ro-crate_packaging.png)

Note that we already have some of the EMO-BON RO-Crates locall in `/emobon_demo/ro-crates/analysis-results-cluster-01-crate/`.

Description of all the contents of a RO-Crate is contained in its root directory in the `ro-crate-metadata.json`, strictly with this name. We cannot upload directly the `ro-crate-metadata.json` files to the SPARQL endpoint, first they need to be serialized. We do that in the jupyter notebook.

Initialize the `jupyterlab` server

```bash
# ideally in the biohap folder
python -m jupyterlab
```

Open `biohap/biodata_pt/python_tools/01_fuseki_emobon.ipynb`. Run section 0., imports and function definitions. Section 1. serializes ro-crate0metadata.json files to turtle (`.ttl`) compatible with graph ingestion. Do note forget to change the relative path from your `home` to the `analysis-results-cluster-01-crate` folder.

#### Fuseki Server

Expose your triples as a SPARQL end-point accessible over HTTP. Fuseki provides REST-style interaction with your RDF data, here running locally on localhost. Navigate to your downloaded fuseki files, open the archive if you have not done it yet and start the server

```bash
cd apache-jena-fuseki-5.6.0/

# start the server
./fuseki-server
```

This should show you similar to

```bash
15:46:47 INFO  Config          :: Fuseki Base = /home/david-palecek/coding/apache-jena-fuseki-5.6.0/run
15:46:47 INFO  Config          :: No databases: dir=/home/david-palecek/coding/apache-jena-fuseki-5.6.0/run/configuration
15:46:48 INFO  Config          :: UI Base = fuseki-server.jar
15:46:48 INFO  Shiro           :: Shiro configuration: file:/home/david-palecek/coding/apache-jena-fuseki-5.6.0/run/shiro.ini
```

Now opening a default fuseki port 3030 at http://localhost:3030/, you should see something like

![fuseki server](figs/fuseki_screenshot.png)

##### Upload Dataset / Graph

In `fuseki`, go `new dataset` -> give it a name `emobon` -> `add data` -> select the `.ttl` files we serialized in section RO-Crates and `upload all` or one by one.

How to create new dataset and upload data to it from python is shown in section 3. and 4. of the `biohap/biodata_pt/python_tools/01_fuseki_emobon.ipynb` notebook.

Pseudo code for serialization of `ro-crate-metadata.json` and data upload (no new dataset creation) would look like

```python
# read the json file and convert it to a graph
with open(".../ro-crate-metadata.json", "r", encoding="utf-8") as f:
    jsonld_text = f.read()

g = Graph()
# rdflib accepts a JSON-LD string as input; base is optional
g.parse(data=jsonld_text, format="json-ld", publicID=base)

# upload data to the endpoint
resp = requests.put(
    FUSEKI_URL,
    data=g.serialize(format="turtle").encode("utf-8"),
    headers={"Content-Type": "text/turtle"},
    timeout=60,
)
```

The example which integrates getting rocrates from GH directly is in `biohap/biodata_pt/python_tools/02_fuseki_emobon_GH.ipynb` or [online](https://github.com/Py-ualg/biohap/blob/main/biohap/biodata_pt/python_tools/02_fuseki_emobon_GH.ipynb).

#### SPARQL

The direct way from `fuseki` is to edit the query in `actions`. See more detailed but accesible introduction with [examples](https://www.w3.org/TR/sparql11-query/). When you click `query`, the default query is shown

```sql
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT * WHERE {
  ?sub ?pred ?obj .
} LIMIT 10
```

, which queries all the triples in the graph.

::: {.callout-note collapse="true" title="How many triples do we have?"}

  ```sql
  SELECT (COUNT(*) as ?c)
  WHERE {
    ?subject ?predicate ?object .
  }
  LIMIT 10
  ```

:::

::: {.callout-note collapse="true" title="Filter out all the `text/html` files."}

  ```sql
  PREFIX sdo: <http://schema.org/>

  SELECT ?x ?dtype
  WHERE {
    ?x sdo:encodingFormat ?dtype .
    FILTER regex(str(?dtype), "^text/html", "i")
  }
  ```

:::

::: {.callout-note collapse="true" title="Return also the `sdo:downloadUrl` of those files."}

  ```sql
  PREFIX sdo: <http://schema.org/>

  SELECT ?x ?dtype ?durl
  WHERE {
    ?x sdo:encodingFormat ?dtype ;
      sdo:downloadUrl ?durl .
    FILTER regex(str(?dtype), "^text/html", "i")
  }
  ```

:::

Now you can click the link of one of the `Krona` files, open them in the browser and with no surprise, it is a Krona plot.

Now let's get the real metaGOflow outputs, specifically SSU taxonomy tables. There are several ways how to do it.

**1. RO-Crate browser**
EMBRC hosts the [RO-Crate viewer](https://data.emobon.embrc.eu/) for the EMO-BON data

**2. Local SPARQL**
Write a query to get the `sdo:downloadUrl` links, put them into your browser, which automatically triggers the download. *Hint for the exercise below, match `regex` of the `object` on "SSU-taxonomy-summary".*

::: {.callout-note collapse="true" title="Return SSU taxonomy download links."}

  ```sql
  PREFIX sdo: <http://schema.org/>

  SELECT ?subject ?predicate ?object ?durl
  WHERE {
    ?subject ?predicate ?object .
    FILTER regex(str(?object), "SSU-taxonomy-summary", "i")
    OPTIONAL { ?object sdo:downloadUrl ?durl }
  }
  LIMIT 50
  ```

:::

**3. Use data version control (`DVC`) tool**
Shown in the python implementation of the above, which follows in the next section. For more on dvc, check its [documentation](https://dvc.org/doc).

#### Python SPARQL

It is possible to export the tables form the `fuseki` for subsequent work, but let's do everything seamlessly from the jupyter notebook.

First we reproduce the queries in section 2 of `biohap/biodata_pt/python_tools/01_fuseki_emobon.ipynb` notebook (or [online](https://github.com/Py-ualg/biohap/blob/main/biohap/biodata_pt/python_tools/01_fuseki_emobon.ipynb))

The jupyterlab server should be still running, if not start it again in the `.../biohap` folder.

```bash
python -m jupyterlab
```

Since we have already ingested the triples from the RO-Crates, we just need to query the existing endpoint

```python
q = """
SELECT (COUNT(*) AS ?c) 
WHERE { 
  ?s ?p ?o
}
"""

r = requests.get("http://localhost:3030/emobon/query", params={"query": q}, headers={"Accept": "application/sparql-results+json"})
print(r.json())
```

In the requests target, you see we query `emobon` dataset, which we created earlier. The returned `json` is relatively easy to convert to a dataframe (`sparql_json_to_df` function)

```python
def sparql_json_to_df(sparql_json):
    """
    Convert a SPARQL SELECT query JSON result to a pandas DataFrame.
    
    Parameters
    ----------
    sparql_json : dict
        JSON returned by Fuseki / SPARQL endpoint with Accept: application/sparql-results+json
    
    Returns
    -------
    pd.DataFrame
    """
    vars_ = sparql_json.get("head", {}).get("vars", [])
    rows = []

    for binding in sparql_json.get("results", {}).get("bindings", []):
        row = {}
        for var in vars_:
            # Some results might not bind all variables
            if var in binding:
                row[var] = binding[var]["value"]
            else:
                row[var] = None
        rows.append(row)

    df = pd.DataFrame(rows, columns=vars_)
    return df
```

#### Local + Public Endpoints

The motivation behind setting up SPARQL graph database is in Section 5 of the `biohap/biodata_pt/python_tools/01_fuseki_emobon.ipynb`, which demonstrates how to combine local queries with public endpoints from wikidata and UniProt.

::: {.callout-tip}
There is a python wrapper for SPARQL [SPARQLWrapper](https://sparqlwrapper.readthedocs.io/en/latest/main.html), which is `pip` installable and can be used as a standard python module but also as a command line script.
:::

:::

------------------------------------------------------------------------
