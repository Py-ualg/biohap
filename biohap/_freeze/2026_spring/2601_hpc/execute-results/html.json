{
  "hash": "afb47d72237051fb614f396c0fe5dd17",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"High-Performance Computing: CCMAR's CETA & National Infrastructure\"\ndate: 26 January 2026\nauthor: David Paleček [dpalecek@ualg.pt](mailto:dpalecek@ualg.pt)\neditor: source\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    toc-depth: 4\n    toc-location: left\n    code-fold: show\n    code-summary: \"Code\"\n    fig-width: 12\n    fig-height: 6\n    out.width: \"100%\"\n    code-overflow: wrap\n---\n\n::: {.callout-note collapse=\"true\" title=\"Last change\"}\nJanuary 21, 2025\n:::\n\nThis hands-on workshop introduces participants to the power of high-performance computing (HPC), guiding them toward independent use through practical examples on the CCMAR's `CETA` server. The workshop also includes a presentation from HPCVLAB at UAlg on the national `Deucalion` FCCN infrastructure.\n\n## Outline\n\n1. Hands on SLURM cases on `CETA`\n    - Basics of editing files on HPC in `Vim`\n    - *Hello world* example submitting a python script\n    - Create custom database and run `BLAST`\n    - Nextflow pipelines from `nf-core`\n2. Presentation introducing `Deucalion`, a national FCCN HPC infrastructure, access schemes, support and Graphical interfaces `CETA` does not provide.\n\n## Tutorial\n\nBefore the hands-on, you can check out the introductory presentation to [HPC in general](https://github.com/Py-ualg/biohap/blob/main/assets/2601_hpc/hpc_intro.pdf) and specific to [Deucalion](https://github.com/Py-ualg/biohap/blob/main/assets/2601_hpc/national_HPC_infrastructure.pdf).\n\n:::{.panel-tabset}\n\n### Setup\n\n1. Please get yourself familiar with the spirit of working on command line from last years [Andrzej's introducion](https://py-ualg.github.io/biohap/2025_spring/250416_atkacz.html)\n2. To get an account on `CETA`, you need to send your request, together with your **public ssh key** to *cymon[at].ualg.pt*.\n\nBelow are the OS dependent instructions on how to setup the `ssh` including the `ssh-keygen`.\n\n:::{.panel-tabset}\n\n#### Linux and Mac\n\nssh should be already installed by default. If not, install it via your package manager such as:\n\n```bash\n# linux\nsudo apt install openssh-client\n\n# mac\nbrew install openssh\n```\n\n#### Windows\n\nIf unsure whether you have `OpenSSH` available, go to\n\n```\nSettings → System → Optional Features\n```\n\n- Check if `OpenSSH` is installed. If not, select Add a feature, then find and install both OpenSSH Client and Server.\n- Confirm `OpenSSH` is listed in System and Optional Features.\n\n:::\n\nOpen a command line or power shell and execute. To generate the key, execute\n\n```bash\nssh-keygen -t rsa -b 4096 -C \"yourname@ceta\"\n```\n\nTwo files will be created in your `/home/.ssh/` directory (on Windows in `users/<name>/.ssh`). The file named `<key>.pub` is the public key and is copied to the server to identify you. The file named `<key>` (without an extension) is your private key; it must be kept secret and never shared with anyone.\n\n<button class=\"btn btn-primary next-tab\">Next Step →</button>\n\n### Downloads\n\nFiles necessary to complete this tutorial can be found and downloaded from the biohap [GitHub](https://github.com/Py-ualg/biohap/tree/main/assets/2601_hpc).\n\n### File management\n\nOnce you have your account confirmed, you will be able to login using\n\n```bash\nssh <username>@ceta.ualg.pt\n```\n\nIf you set up a password to access the private key during its generation step, you will be promted to enter it. Otherwise press enter.\n\n::: {.callout-tip collapse=\"true\" title=\"SSH config\"}\nIn the `.ssh` folder you can create/edit a `config` file\n\n```bash\nHost ceta\n    HostName ceta.ualg.pt\n    User <your-ceta-user-name>\n    IdentityFile ~/.ssh/<your-ceta-key-filanme>\n```\n\nWith such setup, you can login using only `ssh ceta`, your username and server address will get substituted from the config file information.\n\nIf you need to access `CETA` from another PC in the future, generate a new private/public key pair and add the public key to `.ssh/authorized_keys` file.\n:::\n\nThere is no graphical interface available on the server. Typically prepare all your files on your local machine and `scp` them over to the server. Small edits are performed in `vi/vim` editor\n\n#### Vim\n\ncreate a new file (as `touch` command would as well) or open an existing one\n\n```bash\nvim <file>\n```\n\nDefault mode is a command mode. To enter editing mode, press `insert` or `Shift i`\n\nto exit the edit mode, press `ESC`. In the this `normal` mode, pressing combination of letters execute commands\n\n::: {.callout-note collapse=\"true\" title=\"Examples\"}\n\n- `dd` → delete a line your cursor is at\n- `yy` → yank (copy) the current line\n- `p` → paste after the cursor\n- `P` → paste before the cursor\n- `:%d` → to delete everything\n- `u` → undo the last change\n\nand so on and on\n:::\n\nTo copy text in editing mode (`shift I`), select text and right-click to paste at the position of the cursor\n\nTo save and exit: `ESC` to enter command mode `:wq` (write and quit)\n\nTo exit without saving:  `ESC` and `:q!`\n\n### SLURM\n\nIn this section, the hello-world example on submitting a job for exeution is shown. First copy two files over to `CETA` using `scp`. In your PC command line, navigate to the folder containing these files:\n\n- add_2_to_2.py\n- HPC_addition.sh\n\n```bash\n# if you create folder in your /home/<name>/\nmkdir hpc_workshop\n\n# to copy files to that location is done as\nscp add_2_to_2.py HPC_addition.sh <your-ceta-username>@ceta.ualg.pt:/home/<name>/hpc_workshop/\n\n# you can replace /home/<name> with ~\nscp add_2_to_2.py HPC_addition.sh <your-ceta-username>@ceta.ualg.pt:~/hpc_workshop/\n```\n\n::: {.callout-tip collapse=\"true\" title=\"Copy folders\"}\nTo copy folders use `scp -r`, which means you can copy the whole material with\n\n```bash\nscp -r HPCcourse <your-ceta-username>@ceta.ualg.pt:/home/hpc_workshop/\n```\n\n:::\n\n`vim` each file and pay attention to these `#SBATCH` lines\n\n```bash\n#SBATCH --job-name=adding_2_and_2\n#SBATCH --output=addition.log\n#SBATCH --error=addition.err\n#SBATCH --nodes=1\n##SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=48\n#SBATCH --partition=all\n#SBATCH --nodelist=ceta1\n#SBATCH --mem=80G\n```\n\nFirst three lines are to set up how the job appears in the queue and where it wirtes outputs. `CETA` nodes have 56 CPUs, you allocate total of `ntasks-per-node` x `cpus-per-task`\n\n::: {.callout-note collapse=\"true\" title=\"Nodes memmory\"}\nMemory is allowcated for the whole job. Nodes 1-5 have 200 GB of memory, `bigmem` node has 900 GB RAM, which is not useful for most of the jobs. Even if your database has 500 GB, it does not mean it gets all loaded into the RAM, most probably you do not need to run on `bigmem`.\n:::\n\n`#SBATCH --partition=all` means that the job is submitted to all nodes, if there is free compute it will run immediately. This option is overwritten by `#SBATCH --nodelist=ceta1` where we pick specific node for submission. If the resources are not available on `ceta1`, the job will wait in the queue until the resources are freed.\n\nIf you wish to pick a node, check the current job queue with:\n\n```bash\nsqueue\n\nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n12575       all   UDI305    cymon  R 1-03:41:34      1 ceta4\n12578       all many-aa-   leonor  R 1-18:03:38      1 ceta5\n12592       all   saniau    bruno  R   19:49:24      1 ceta1\n12593       all    nano1  jvarela  R   17:04:09      1 ceta1\n12609       all nfcoreMa  jbentes  R      40:12      1 ceta3\n12499    bigmem   UDI299    cymon  R    9:00:25      1 ceta-gen\n```\n\n::: {.callout-note collapse=\"true\" title=\"+x\"}\nUnlike running the script directly in bash with `./HPC_addition.sh`, which needs\n\n```bash\nchmod +x HPC_addition.sh\n```\n\n, SLURM script does not require *executable* +x, if you have `#!/bin/bash` at the top of the script:\n:::\n\nTo run the job:\n\n```bash\nsbatch HPC_addition.sh\n```\n\nTo cancel a job (do not worry, you can cancel only your own jobs)\n\n```bash\n# for example scancel 12543 \nscancel <job number> \n```\n\nExecution of this job depends on the queu and resources which are currently used for the jobs. In the setup above, next to our job can run another job on the same node using `#SBATCH --cpus-per-task=6` and `#SBATCH --mem=100G`. Do you agree?\n\nlet's investigate the output files, compare\n\n```bash\nls\n\n# or\nls -lhrt\n\n# or\nls -la\n\n# or\n\nll\n```\n\n### BLAST\n\nRule of thumbs are:\n\n1. **Do not store large BLAST databases in your home directory.** Home directories usually have strict size and performance limits. Large reference databases should be placed in shared or high-performance storage locations provided by the HPC system. On `CETA` in `/share/data/ncbi-blast/`.\n2. **Database size directly affects performance.** The larger the database, the slower the search. If your analysis does not require certain taxa (e.g. vertebrates), use a filtered or custom database to reduce runtime and resource usage.\n3. **Ask HPC support staff for guidance.** Asking early often saves significant time and compute resources by choosing the right sources, and optimizeing SLURM scripts.\n4. **More CPUs do not always mean faster BLAST.** BLAST performance is often limited by disk I/O and database size rather than CPU. Increasing the number of threads beyond a certain point may yield little or no speedup and can waste allocated resources.\n\n[BLAST](https://blast.ncbi.nlm.nih.gov/Blast.cgi) is a sequence alignment tool used to identify similarities between query sequences (nucleotide or protein, `-query`) and sequences stored in a reference database (`-db`). It performs local alignments, allowing for mismatches, insertions, and deletions, and reports the best-scoring matches along with alignment statistics such as scores, identities, and E-values.\n\nStart by copying a `small_SILVA.txt` to your workshop folder as before, which is set of sequences we want to represent as a database compatible with the `blast`. First we setup a database with `makeblastdb`. This software comes with the `blast` as a tool and should be installed for you already (it is possible to install your own software in your account `home` folder but it is a bit tricky)\n\n```bash\nmakeblastdb -dbtype nucl -in small_SILVA.txt -out small_SILVA.db\n```\n\n#### New submission script\n\nLet's prepare slurm input file to run local BLAST. First let's use the previous template\n\n```bash\ncp HPC_addition.sh run_blast.sh\n```\n\n```bash\nvim run_blast.sh\n```\n\n#### Changes\n\n1. Instead of python command, replace it with\n\n```bash\nblastn -db small_SILVA.db -query SRR12031251_300bp.fasta -max_target_seqs 1 -outfmt 6  > my_blast_results.txt\n```\n\n2. Blast is not parallelized, unless you ask for more threads with `--num-threads`, therefore you can ask for a single CPU, `--cpus-per-task=1`, than 54 user can run their blasts on the same node.\n\n3. Change the job name, log and error files names to appropriatelly describe your job!!! Do squeue, if the node is busy avoid using it, or remove the `#SBATCH --nodelist=ceta1`. In normal bash, `#` is a comment symbol, in a batch script to uncomment the `#BATCH`, use `##` instead.\n\nThe blast job should take less than 1min to finish\n\n### Parallel BLAST\n\nJust as an example for your reference of more complicated version, below we want to blast all fasta files in the directory with filenames ending with `.merged.fasta` against two separate databases (`VFDB_setA_nt.fa` and `megares_database_v3.00.fasta`). Doing this one by one is not efficient because of little gain on using many CPUs for single `blast`s, therefore we keep running `MAX_JOBS` archives in parallel.\n\nSlurm script can look like this with logs being written to `blastn_%j.out`:\n\n```bash\n#!/bin/bash\n\n#SBATCH --partition=all\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=54\n#SBATCH --job-name=blastn\n#SBATCH --output=blastn_%j.out\n#SBATCH --error=blastn_%j.err\n\nWORD_SIZE=28\nEVALUE=1e-5\n\n# Input directory with FASTA files (example)\nINDIR=\"/home/davidp/archives\"\n\n# Output directory\nOUTDIR=\"results/blastn-ws${WORD_SIZE}-$(date +%Y%m%d_%H%M%S)\"\nmkdir -p \"${OUTDIR}\"\n\nMAX_JOBS=6\ncurrent_jobs=0\n\nfor FASTA in ${INDIR}/*/*.merged.fasta; do\n    ARCHIVE=$(basename \"$(dirname \"$FASTA\")\")\n    echo \"Processing ${ARCHIVE}\"\n\n    (\n        start_time=$(date +%s)\n\n        blastn \\\n            -query \"${FASTA}\" \\\n            -db VFDB_setA/VFDB_setA_nt.fas \\\n            -outfmt \"6 qseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\" \\\n            -max_target_seqs 10 -max_hsps 1 \\\n            -evalue ${EVALUE} -word_size ${WORD_SIZE} \\\n            -num_threads 4 \\\n            -out \"${OUTDIR}/${ARCHIVE}_merged_vfdb-setA.out\" &\n\n        blastn \\\n            -query \"${FASTA}\" \\\n            -db megares_v3_DB/megares_database_v3.00.fasta \\\n            -outfmt \"6 qseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\" \\\n            -max_target_seqs 10 -max_hsps 1 \\\n            -evalue ${EVALUE} -word_size ${WORD_SIZE} \\\n            -num_threads 4 \\\n            -out \"${OUTDIR}/${ARCHIVE}_merged_megares_v3.out\" &\n\n        wait\n\n        end_time=$(date +%s)\n        echo \"Completed ${ARCHIVE} in $((end_time - start_time)) seconds\"\n\n    ) &\n\n    current_jobs=$((current_jobs + 1))\n\n    if [ \"$current_jobs\" -ge \"$MAX_JOBS\" ]; then\n        wait -n\n        current_jobs=$((current_jobs - 1))\n    fi\ndone\n\nwait\necho \"All BLAST jobs completed.\"\n```\n\nIn the above example the user, which is not optimal. Better way is to submit all jobs as an array\n\n```bash\n#!/bin/bash\n\nINDIR=\"/home/davidp/archives\"\nFILELIST=\"blast_inputs.txt\"\n\n# Build input list\nfind \"$INDIR\" -type f -name \"*.merged.fasta\" | sort > \"$FILELIST\"\n\nN=$(wc -l < \"$FILELIST\")\n\necho \"Submitting $N BLAST jobs as a SLURM array\"\n\nsbatch \\\n  --array=1-\"$N\" \\\n  blast_array_job.sh \"$FILELIST\"\n```\n\n, where SLURM triggers the worker `blast_array_job.sh` per archive. The worker script looks then something like this\n\n```bash\n#!/bin/bash\n#SBATCH --partition=all\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --job-name=blastn\n#SBATCH --output=blastn_%A_%a.out\n#SBATCH --error=blastn_%A_%a.err\n\nset -euo pipefail\n\nWORD_SIZE=28\nEVALUE=1e-5\n\nFILELIST=\"$1\"\nFASTA=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" \"$FILELIST\")\n\nARCHIVE=$(basename \"$(dirname \"$FASTA\")\")\n\nOUTDIR=\"results/blastn-ws${WORD_SIZE}\"\nmkdir -p \"$OUTDIR\"\n\necho \"[$(date)] Processing $ARCHIVE\"\necho \"FASTA: $FASTA\"\necho \"CPUs allocated: $SLURM_CPUS_PER_TASK\"\n\nstart_time=$(date +%s)\n\n# Use all CPUs SLURM gives us, split between BLASTs\nTHREADS_PER_BLAST=$((SLURM_CPUS_PER_TASK / 2))\n\nblastn \\\n  -query \"$FASTA\" \\\n  -db VFDB_setA/VFDB_setA_nt.fas \\\n  -outfmt \"6 qseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\" \\\n  -max_target_seqs 10 -max_hsps 1 \\\n  -evalue \"$EVALUE\" -word_size \"$WORD_SIZE\" \\\n  -num_threads \"$THREADS_PER_BLAST\" \\\n  -out \"$OUTDIR/${ARCHIVE}_merged_vfdb-setA.out\" &\n\nblastn \\\n  -query \"$FASTA\" \\\n  -db megares_v3_DB/megares_database_v3.00.fasta \\\n  -outfmt \"6 qseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\" \\\n  -max_target_seqs 10 -max_hsps 1 \\\n  -evalue \"$EVALUE\" -word_size \"$WORD_SIZE\" \\\n  -num_threads \"$THREADS_PER_BLAST\" \\\n  -out \"$OUTDIR/${ARCHIVE}_merged_megares_v3.out\" &\n\nwait\n\nend_time=$(date +%s)\necho \"[$(date)] Completed $ARCHIVE in $((end_time - start_time)) seconds\"\n```\n\n- each FASTA is isolated\n- failures don’t affect others\n- clean logs per archive\n- SLURM enforces CPU limits\n- scalable to thousands of inputs.\n\n### Nextflow\n\n![nf-core](figs/nfcore.png)\n\nBrowse the [141 pipelines](https://nf-co.re/pipelines/) that are currently available as part of nf-core. Why it is a good idea to use them:\n\n1. **Reproducibility**: You start with the raw data, and get all the preprocessing and processsing channeled into single workflow.\n2. **Integration**: Many tools setup in the containers to use out of the box, which might be hard to setup individually.\n3. **Reporting**: Supports comprehensive logs and result reports. If pipeline fails, it can be often salvaged from the cached intermediate state adding `-resume` to the script and rerun.\n\n::: {.callout-caution collapse=\"true\" title=\"Careful\"}\nYou might easily generate lots of outputs. Use\n\n```bash\nnextflow clean [run_name|session_id] [options]\n```\n\nafter succesful pipeline completion.\n:::\n\n#### Installation\n\nNormally, you will not need, nor have permissions to install system-wide `nextflow` on the cluster. For you own account or PC, you will need java > 17, if needed follow these [instructions](https://www.nextflow.io/docs/latest/install.html)\n\nThen do the following\n\n::: {.callout-note collapse=\"true\" title=\"Install\"}\n\n```bash\n# Install nextflow\ncurl -s https://get.nextflow.io | bash\nmv nextflow ~/bin/\n```\n\n:::\n\nTypically nf-core pipeline will run in the container, such as `apptainer` or `docker`, for HPC. Different pipelines need different inputs however general structure of calling the pipeline is:\n\n```bash\n# Launch the pipeline\nnextflow run nf-core/<PIPELINE-NAME> -v <XXX> \\\n    --input samplesheet.csv \\\n    --output ./results/ \\\n    -profile apptainer\n```\n\n::: {.callout-tip collapse=\"true\" title=\"test first\"}\nIt is highly recommended to test the pipeline first, before the full run, which is done by adding `test` to the profile parameter, resulting in `-profile test,apptainer`\n:::\n\nUnder the hood, if you are using the pipeline for the first time, it will get pulled from GH into your `~/.nextflow` folder\n\nIn the full example, we set up kraken2 database and will run a [taxprofiler](https://nf-co.re/taxprofiler/1.2.5/) pipeline for the same sample as the blast before.\n\nTwo kraken databases already exists on `CETA` in `/home/share/kraken/`, so that everybody can use them.\n\n::: {.callout-tip collapse=\"true\" title=\"Quiz\"}\nCan you tell me what is the size of those DBs?\n:::\n\n`Taxprofiler` needs minimal input files which are `samples sheet` and `database sheet` to point where your sequence files and databases you want run against are, respectively.\n\nFor sample, we will reuse the blast file, copy the following into a new 'samplesheet_biohap.csv', careful it needs to be gzipped file\n\n```bash\ngzip SRR12031251_300bp.fasta\n\ntouch samplesheet_biohap.csv\nvi samplesheet_biohap.csv\n\n# copy the full path to your SRR12031251_300bp.fasta\nsample,run_accession,instrument_platform,fastq_1,fastq_2,fasta\nsample1,0,ILLUMINA,,,<rest-of-the-full-path>/SRR12031251_300bp.fasta.gz,\n```\n\n, save and close with `:wq`.\n\nFor the database we use the smaller `minusb`, copy the below to the `databases_input.csv`:\n\n```bash\ntool,db_name,db_params,db_path\nkraken2,minusb,,/share/data/kraken/k2_minusb_20250714.tar.gz\n```\n\n```bash\n#!/bin/bash\n\n#SBATCH --partition=all\n#SBATCH --nodelist=ceta2\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=taxprof\n#SBATCH --output=nfcore-taxprof_%j.out\n#SBATCH --error=nfcore-taxprof_%j.err\n\n# this is HPC setup dependent\nexport JAVA_HOME=/usr/lib/jvm/java-17-openjdk-17.0.17.0.10-1.el8.x86_64\nexport NXF_APPTAINER_CACHEDIR=/share/apps/share/nextflow/apptainer_cache\n\n# Set output directory\nOUTDIR=\"results/nfcore-taxprofiler-$(date +%Y%m%d_%H%M%S)\"\nSAMPLE_SHEET=\"samplesheet_biohap.csv\"\nDATABASE_SHEET=\"databases_input.csv\"\n\n# Create output directory if it doesn't exist\nmkdir -p \"$OUTDIR\"\n\n# Run Nextflow pipeline\nnextflow run nf-core/taxprofiler -r 1.2.5 \\\n    -profile apptainer \\\n    --databases \"$DATABASE_SHEET\" \\\n    --outdir \"$OUTDIR\" \\\n    --input \"$SAMPLE_SHEET\" \\\n    --run_kraken2 \\\n    --run_krona \\\n```\n\n### Conda\n\nThis is a minimal example to run `QIIME2` analysis in conda environment. Creating your own environment is beyond the scope of this workshop and you can consult with us any time. We are going to use existing environment\n\n```bash\n# first time, you might need to init conda\nconda init\n\n# activate conda\nconda activate\n\n# list all the environments\nconda env list\n\n# we activate the qiime2-amplicon-2024.5\nconda activate qiime2-amplicon-2024.5\n```\n\n```bash\n#!/bin/bash\n\n#SBATCH --partition=all\n##SBATCH --nodelist=ceta2\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --job-qiime2\n#SBATCH --output=qiime2_%j.out\n#SBATCH --error=qiime2_%j.err\n\n# Set output directory\nINDIR=\"/home/davidp/biohap/qiime2\"\nOUTDIR=\"results/qiime2-$(date +%Y%m%d_%H%M%S)\"\n\nsource ~/.bashrc\nconda activate qiime2-amplicon-2024.5\n\n# Create output directory if it doesn't exist\nmkdir -p \"$OUTDIR\"\n\n# this is initial step of the qiime2 analysis\nqiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path Manifest_file_Species.txt \\\n    --output-path demux.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\n\n# below you would mount the rest of your pipeline\n```\n\nNow you can implement the QIIME2 tutorial from the [last year](https://py-ualg.github.io/biohap/2025_spring/250507_taires.html), but on HPC.\n\n:::\n\n```{js, echo=FALSE}\ndocument.addEventListener(\"DOMContentLoaded\", function() {\n  // Find all buttons with the 'next-tab' class\n  const buttons = document.querySelectorAll('.next-tab');\n  \n  buttons.forEach(button => {\n    button.addEventListener('click', function() {\n      // Find the currently active tab link\n      const currentTabList = this.closest('.tab-content').previousElementSibling;\n      const activeTab = currentTabList.querySelector('.nav-link.active');\n      const nextTab = activeTab.parentElement.nextElementSibling?.querySelector('.nav-link');\n      \n      if (nextTab) {\n        nextTab.click(); // Trigger a click on the next tab header\n      }\n    });\n  });\n});\n```\n\n",
    "supporting": [
      "2601_hpc_files"
    ],
    "filters": [],
    "includes": {}
  }
}