[
  {
    "objectID": "2025_fall/call.html",
    "href": "2025_fall/call.html",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "Since the world is spinning, the next edition is uncertain. Therefore, we call for your ideas and contributions already now for the Autumn 2025.",
    "crumbs": [
      "Fall 2025",
      "Call for contributions"
    ]
  },
  {
    "objectID": "2025_fall/call.html#call-for-contributions",
    "href": "2025_fall/call.html#call-for-contributions",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "Since the world is spinning, the next edition is uncertain. Therefore, we call for your ideas and contributions already now for the Autumn 2025.",
    "crumbs": [
      "Fall 2025",
      "Call for contributions"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html",
    "href": "2025_spring/250416_atkacz.html",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Large files or datasets—especially those containing genomic data—no problem. This workshop introduces essential Linux commands and simple Bash scripts to streamline data manipulation tasks. We’ll cover key operations such as searching for patterns, globally modifying content, and aligning DNA sequences, such as FASTAQ and FASTA files.\n\n\nNo extra setup needed for Linux and MacOS machines. For windows the recommended way is to install VBox. See the steps to follow:\nHere are the instructions: Installing Ubuntu on VirtualBox\n\n\n\nRun the Installer:\nAfter downloading the VirtualBox installer (.exe file for WinOS, .dmg for MacOS), double-click on it to launch the setup wizard and follow default installation instructions.\nLaunch VirtualBox.\n\n\n\n\n\nOpen VirtualBox and Create a New VM: Click on the New button.\nName and Operating System: Give your VM a name (e.g., “Ubuntu”), select “Linux” as the type, and choose Ubuntu (64-bit) as the version.\nAssign Memory: Allocate at least 2048 MB (2 GB) of RAM (or higher if your system permits) to ensure smooth operation.\nCreate a Virtual Hard Disk:\n\nChoose Create a virtual hard disk now.\nSelect the VDI (VirtualBox Disk Image) format, and set the disk size (a minimum of 20 GB is recommended).\n\n\n\n\n\n\nAttach the Ubuntu ISO after downloading:\n\nSelect your new VM, click on Settings, then navigate to the Storage section.\nUnder the “Controller: IDE” (or SATA), click on the empty optical drive.\nClick the small disk icon on the right and choose Choose a disk file. Browse to and select the Ubuntu ISO you downloaded.\n\nStart the Virtual Machine:\n\nWith the ISO attached, click Start.\n\n\n\n\n\n\nNote\n\n\n\nYou might need to restart your machine\n\n\nThe VM will boot from the ISO, and you should see the Ubuntu welcome screen.\n\nInstall Ubuntu:\n\nFollow the on-screen instructions in the Ubuntu installer:\n\nChoose your language and keyboard layout.\nSelect Install Ubuntu.\nFollow prompts to set up your timezone, create a user account, and configure your disk (use the default settings for a virtual disk).\nOnce the installation is complete, you’ll be prompted to restart the VM.\n\n\n\n\n\n\n\n\n\n\nEventually you will need bigger compute than you can get on your machine and you will be forced to work on Cloud such as High Power cluster (HPC) which allow you to parallelize and run heavy computations.\nThings can be done faster on a command line with practice.\n\n\n\n\nOpen the terminal and let’s create a file called maria_joanna_song\ntouch maria_joanna_song\nlet’s just copy in this text - open the file with double click and paste this song into the maria_joanna_song file, also found here\nE virou!\nOnde anda essa Maria?\nEu só tenho essa Maria\nEu vim do norte direto a Lisboa\nAtrás de um sonho\nQue eu nem sei se voa\nTanto quanto nós voávamos\nDebaixo dos lencóis\nA francesinha já não tem picante\nE isso faz-me lembrar os instantes\nEm que tu mordias os meus lábios\nDepois do amor\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque não estás\nPorque não estás aqui\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMinha mãe nunca me viu partir\nMas todo o filho um dia voa\nSaudade, saudade\nMamã cuida dela por mim\nDiz-me porque não estás aqui\nMaria, sem ti fico à toa\nSaudade, saudade\nMaria, eu quero-te ver\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque a saudade aperta o meu peito\nE dói demais (dói demais)\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana (Maria), Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nQuantas noites não dormi\nCada gota que eu deitei\nFez um rio que me leva a ti\nMaria Joana\nMaria Joana (quantas lágrimas chorei)\nApanha o primeiro autocarro\nVem ficar pra sempre\nDo meu lado (quantas noites não dormi)\nMaria Joana (ai, Maria)\nMaria Joana (cada gota que eu deitei)\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nTo see the full lyrics (TIP: press tab to autocomplete the file name)\ncat maria_joanna_song\nShow the first 5 lines\nhead -5 maria_joanna_song\nTo see only the last 5 lines\ntail -5 maria_joanna_song\nTo see only the last 10 to 15 lines (note the “|” sign it is a pipe and let’s you combine the commands into one, without dumping the information onto your harddrive)\nhead -15 maria_joanna_song | tail -5\nIf you want to put information into your disk you can do as follows\nhead -15 maria_joanna_song &gt; top15_maria\ntail -5 top15_maria &gt; lines10-15_maria\nTo see the output we need to either click on the file or use the terminal\ncat lines10-15_maria\n\n\n\nHow to find something in the text?\ngrep Maria maria_joanna_song\nAnd how to see only the matched pattern - commands have “options”\ngrep -o Maria maria_joanna_song\nYou can always get help ( command --help or shortcut version -h) - generally not much usefull - better to google it or ask ChatGPT\ngrep --help \nLet’s count word Maria - three different ways to count something\ngrep Maria maria_joanna_song | wc -l\ngrep Maria maria_joanna_song | wc -w\ngrep Maria maria_joanna_song | wc -c\nNow count appearances of Maria Joanna - it is important to use “” if the your search contains a space or some special character\ngrep \"Maria Joana\" maria_joanna_song | wc -l\ngrep \"Maria Joana\" maria_joanna_song | wc -w\ngrep \"Maria Joana\" maria_joanna_song | wc -c\nHow to deal with special characters\ngrep ( maria_joanna_song   #note the error\ngrep \"(\" maria_joanna_song\ngrep \\( maria_joanna_song\n\n\n\n“.” and “*” can represent any characters and therefore represent patterns rather than exact matches\ngrep Eu maria_joanna_song\ngrep Eu.* maria_joanna_song\nPlaying with multiple choices\ngrep se maria_joanna_song \ngrep se[i] maria_joanna_song \ngrep se[i,m] maria_joanna_song \nBut I am in love with Donald Trump and not with Maria Joana - how to I fix the song?\nsed 's/Maria Joana/Donald Trump/g'  maria_joanna_song | sed 's/Maria/Donald/g' &gt; donald_trump_song\nLet’s see what happened - now it doesn’t rhyme as good\ncat donald_trump_song\nmore donald_trump_song\nOk, but are we sure Donald replaced Maria?\ndiff -u *song \ndiff maria_joanna_song donald_trump_song\nvimdiff *song\n\n\n\n\n\n\nNote\n\n\n\nexit vim using Esc + :q which closes the file, alternatively Esc + :wq which saves the file.\n\n\n\n\n\nHave a look how a loop is being used here\ngrep Lisboa *song\nfor f in *song ; do grep Lisboa $f ; done\nTo show which file Lisboa is found in\nfor f in *song ; do grep -l Lisboa $f ; done\nTo show where is it found (i.e. line number)\nfor f in *song ; do grep -n Lisboa $f ; done\nI have just realised that “joanna” in portuguese is written as “joana” - how do I fix the name of my file\n\noption - fix it but keep the original\n\ncp maria_joanna_song maria_joana_song\n\n# list contents of directory\nls\n\n# other variants\nls -l\nls -la\n\noption - neh, I don’t need the previous version - it will just cause the confusion later on\n\nmv maria_joanna_song maria_joana_song\nNow let’s try previous code\ngrep Maria maria_joanna_song  # of course it doesn't work ... - what shall I do now ?\n\n\n\n\n\n\nTaken from official website:\nEDirect will run on Unix and Macintosh computers, and under the Cygwin Unix-emulation environment on Windows PCs. To install the EDirect software, open a terminal window and execute one of the following two commands:\n  sh -c \"$(curl -fsSL https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n\n  sh -c \"$(wget -q https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh -O -)\"\nThis will download a number of scripts and several precompiled programs into an “edirect” folder in the user’s home directory. It may then print an additional command for updating the PATH environment variable in the user’s configuration file. The editing instructions will look something like:\n  echo \"export PATH=\\$HOME/edirect:\\$PATH\" &gt;&gt; $HOME/.bash_profile\nAs a convenience, the installation process ends by offering to run the PATH update command for you. Answer “y” and press the Return key if you want it run. If the PATH is already set correctly, or if you prefer to make any editing changes manually, just press Return.\nOnce installation is complete, run:\n  export PATH=${HOME}/edirect:${PATH}\nto set the PATH for the current terminal session.\n\n\n\n\n\n\nWindows\n\n\n\nYou can try to install SRA-toolkit from here\n\n\n\n\n\nLet’s download some data from the Genbank - it is 16S V4 amplicon data (here the metadata)\nesearch -db sra -query PRJNA783372 | efetch -format runinfo &gt; SraRunTable.csv\nWe only need the SRRnumber, -f1 means first field.\ncut -f1 -d, SraRunTable*.csv  &gt; runs.txt\nTo save time we will work on four samples only\ntail -4 runs.txt &gt; runs4.txt\n\n\n\nOk, now we download the actual raw data\nprefetch --option-file runs4.txt\nIn case of\n\n\n\n\n\n\nNote\n\n\n\nIn case of Command 'prefetch' not found run\nsudo apt install sra-toolkit\n\n\nLet’s unpack it\nfor f in SRR* ; do fastq-dump --split-files  $f ; done\n\n\n\n\n\n\nTip\n\n\n\nTo visualize quality scores, small GUI tool fastqc could be used\n\n\n\n\n\nsudo apt install flash\nFirst merge the reads\n# merge\nflash -m 50 -M 500 -x 0.1 SRR17045222_1.fastq SRR17045222_2.fastq\n\n# rename merged file to index it\nmv out.extendedFrags.fastq file1.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045223_1.fastq SRR17045223_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045226_1.fastq SRR17045226_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045227_2.fastq SRR17045227_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nNow we filter low quality reads (here the data is kind of a fake as the fastq download needs to be adjusted wit the prefetch script)\n# if not installed, do\nsudo apt install vsearch\n\nfor f in file* ; do vsearch --fastq_filter $f --fastq_maxee 1.0 --fastqout $f\\_filtered ; done\nIt is getting messy here (delete unwanted files)\nrm *notCombined*\nrm *hist\nrm *histogram\nrm *fastq\n\n\n\n\n\n\nTip\n\n\n\nToo lazy to empty the trash yourself ? the terminal will do that for us gio trash --empty\n\n\nLet’s convert fastq to fasta\n# if not installed, do\nsudo apt install seqtk\n\nfor f in *fastq_filtered ; do seqtk seq -a $f &gt; $f.fasta ; done ; \n\n\n\n\n\n\nCaution\n\n\n\nBlast is commented out as it is unlikely to work - we could try with internal database one day\n\n\nTo save time let’s just use 10 reads from each file for taxonomic identification - we will use GenBank nt database\nfor f in *fastq_filtered.fasta ; do head -20 $f &gt; $f\\head ; done\nSimplify the naming\n\n# if not installed, do\nsudo apt install rename\n\nrename 's/.fastq_filtered.fastahead/.fasta/g' *fastq_filtered.fastahead ;\n\n\n\nLet’s try - it may not work at all, first install\nsudo apt install ncbi-blast+\nblastn -query file1.fasta -db nt -remote -outfmt 6 -max_target_seqs 1 -perc_identity 95 &gt; file1_blast.txt \n\nblastn -query file1.fasta -db nr -remote  &gt; file1_blast2.txt\n\n\nUse the web-based blast, for example on the ncbi server.\nLet’s create ASV table and visualise the data\ncat file[0-9]*.fastq_filtered.fasta &gt; all_samples.fasta\n# if not installed, do\nsudo apt install vsearch\n\nvsearch --derep_fulllength all_samples.fasta   --output dereplicated.fasta  --sizeout  --relabel ASV_\nvsearch --sortbysize dereplicated.fasta  --output dereplicated_no_singletons.fasta  --minsize 2\nvsearch --cluster_unoise dereplicated_no_singletons.fasta --minsize 2  --unoise_alpha 2   --centroids ASVs.fasta\nvsearch --usearch_global all_samples.fasta --db ASVs.fasta   --id 0.97     --otutabout ASV_table.txt\nWe will use MicrobiomeAnalyst - every tool needs the input in some specific format - here we need to change the table as well\nsed 's/#OTU ID/#NAME/g' ASV_table.txt &gt; ASV_table_MA.txt\nWe need some metadata (all invented here)\nhead -1 ASV_table_MA.txt | fmt -1 | sed 's/NAME/NAME, sample/g'  | sed 's/11$/11,fish/g' | sed 's/13$/13,fish/g' | sed 's/21$/21,fish/g' | sed 's/22$/22,fish/g' |sed 's/23$/23,not_a_fish/g' | sed 's/24$/24,not_a_fish/g' |sed 's/25$/25,not_a_fish/g' | sed 's/27$/27,not_a_fish/g' &gt; metadata.csv\n\n\n\n\n\nFollowing this link you can interrogate the same table (ASV_table_MA.txt) in the interactive jupyter notebook.\nTo compute Braycurtis distances \nTo visualize the PCoA \n\n\n\nIf you have python installed you can run scripts which are in this folder.\npython append_braycurtis3.py ASV_table_MA.txt &&\npython pcoa_with_metadata_quick_legend.py\n\n\n\nThe power of command line lies in the fact, that once you test your pipeline command by command, you can copy all the commands into a so-called bash script and rerun the whole thing at once.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nYou may want to create a separate new virtual environment (example uses conda) and install necessary dependencies first\n# create virtual environment\nconda create -n gen-test python\n\n# activate it\nconda activate gen-test\n\n# install deps (pip or conda install)\npip install dask\npip install --upgrade --force-reinstall scipy dask\npip install pyarrow\n\npython3 append_braycurtis3.py ASV_table_MA.txt\n\n# dependencies for the second script\npip install numpy==1.24.3\npip install seaborn\npip install panel\npip install scikit-bio\n\npython3 pcoa_with_metadata_quick_work.py \n\n\n\n# make a new folder for a test\nmkdir pipeline_test\n\n# copy python scripts too, if you want to visualize the results.\ncp *.py ./pipeline_test\n\n# take the commands from the command list file\ngrep -A 1000 \"genomics part\" course_linux_commands | sed '/fastqc/d' | sed '/\\*\\*\\*/d' &gt; ./pipeline_test/my_first_pipeline.sh \nRun the pipeline with\nbash my_fist_pipeline.sh\n\n\n\n\n\n\nCaution\n\n\n\nYou might need to make the script executable\nchmod +x my_fist_pipeline.sh\n\n# then you can run directly\n./my_fist_pipeline.sh",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#setup",
    "href": "2025_spring/250416_atkacz.html#setup",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "No extra setup needed for Linux and MacOS machines. For windows the recommended way is to install VBox. See the steps to follow:\nHere are the instructions: Installing Ubuntu on VirtualBox\n\n\n\nRun the Installer:\nAfter downloading the VirtualBox installer (.exe file for WinOS, .dmg for MacOS), double-click on it to launch the setup wizard and follow default installation instructions.\nLaunch VirtualBox.\n\n\n\n\n\nOpen VirtualBox and Create a New VM: Click on the New button.\nName and Operating System: Give your VM a name (e.g., “Ubuntu”), select “Linux” as the type, and choose Ubuntu (64-bit) as the version.\nAssign Memory: Allocate at least 2048 MB (2 GB) of RAM (or higher if your system permits) to ensure smooth operation.\nCreate a Virtual Hard Disk:\n\nChoose Create a virtual hard disk now.\nSelect the VDI (VirtualBox Disk Image) format, and set the disk size (a minimum of 20 GB is recommended).\n\n\n\n\n\n\nAttach the Ubuntu ISO after downloading:\n\nSelect your new VM, click on Settings, then navigate to the Storage section.\nUnder the “Controller: IDE” (or SATA), click on the empty optical drive.\nClick the small disk icon on the right and choose Choose a disk file. Browse to and select the Ubuntu ISO you downloaded.\n\nStart the Virtual Machine:\n\nWith the ISO attached, click Start.\n\n\n\n\n\n\nNote\n\n\n\nYou might need to restart your machine\n\n\nThe VM will boot from the ISO, and you should see the Ubuntu welcome screen.\n\nInstall Ubuntu:\n\nFollow the on-screen instructions in the Ubuntu installer:\n\nChoose your language and keyboard layout.\nSelect Install Ubuntu.\nFollow prompts to set up your timezone, create a user account, and configure your disk (use the default settings for a virtual disk).\nOnce the installation is complete, you’ll be prompted to restart the VM.",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#command-line",
    "href": "2025_spring/250416_atkacz.html#command-line",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Eventually you will need bigger compute than you can get on your machine and you will be forced to work on Cloud such as High Power cluster (HPC) which allow you to parallelize and run heavy computations.\nThings can be done faster on a command line with practice.\n\n\n\n\nOpen the terminal and let’s create a file called maria_joanna_song\ntouch maria_joanna_song\nlet’s just copy in this text - open the file with double click and paste this song into the maria_joanna_song file, also found here\nE virou!\nOnde anda essa Maria?\nEu só tenho essa Maria\nEu vim do norte direto a Lisboa\nAtrás de um sonho\nQue eu nem sei se voa\nTanto quanto nós voávamos\nDebaixo dos lencóis\nA francesinha já não tem picante\nE isso faz-me lembrar os instantes\nEm que tu mordias os meus lábios\nDepois do amor\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque não estás\nPorque não estás aqui\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMinha mãe nunca me viu partir\nMas todo o filho um dia voa\nSaudade, saudade\nMamã cuida dela por mim\nDiz-me porque não estás aqui\nMaria, sem ti fico à toa\nSaudade, saudade\nMaria, eu quero-te ver\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque a saudade aperta o meu peito\nE dói demais (dói demais)\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana (Maria), Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nQuantas noites não dormi\nCada gota que eu deitei\nFez um rio que me leva a ti\nMaria Joana\nMaria Joana (quantas lágrimas chorei)\nApanha o primeiro autocarro\nVem ficar pra sempre\nDo meu lado (quantas noites não dormi)\nMaria Joana (ai, Maria)\nMaria Joana (cada gota que eu deitei)\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nTo see the full lyrics (TIP: press tab to autocomplete the file name)\ncat maria_joanna_song\nShow the first 5 lines\nhead -5 maria_joanna_song\nTo see only the last 5 lines\ntail -5 maria_joanna_song\nTo see only the last 10 to 15 lines (note the “|” sign it is a pipe and let’s you combine the commands into one, without dumping the information onto your harddrive)\nhead -15 maria_joanna_song | tail -5\nIf you want to put information into your disk you can do as follows\nhead -15 maria_joanna_song &gt; top15_maria\ntail -5 top15_maria &gt; lines10-15_maria\nTo see the output we need to either click on the file or use the terminal\ncat lines10-15_maria\n\n\n\nHow to find something in the text?\ngrep Maria maria_joanna_song\nAnd how to see only the matched pattern - commands have “options”\ngrep -o Maria maria_joanna_song\nYou can always get help ( command --help or shortcut version -h) - generally not much usefull - better to google it or ask ChatGPT\ngrep --help \nLet’s count word Maria - three different ways to count something\ngrep Maria maria_joanna_song | wc -l\ngrep Maria maria_joanna_song | wc -w\ngrep Maria maria_joanna_song | wc -c\nNow count appearances of Maria Joanna - it is important to use “” if the your search contains a space or some special character\ngrep \"Maria Joana\" maria_joanna_song | wc -l\ngrep \"Maria Joana\" maria_joanna_song | wc -w\ngrep \"Maria Joana\" maria_joanna_song | wc -c\nHow to deal with special characters\ngrep ( maria_joanna_song   #note the error\ngrep \"(\" maria_joanna_song\ngrep \\( maria_joanna_song\n\n\n\n“.” and “*” can represent any characters and therefore represent patterns rather than exact matches\ngrep Eu maria_joanna_song\ngrep Eu.* maria_joanna_song\nPlaying with multiple choices\ngrep se maria_joanna_song \ngrep se[i] maria_joanna_song \ngrep se[i,m] maria_joanna_song \nBut I am in love with Donald Trump and not with Maria Joana - how to I fix the song?\nsed 's/Maria Joana/Donald Trump/g'  maria_joanna_song | sed 's/Maria/Donald/g' &gt; donald_trump_song\nLet’s see what happened - now it doesn’t rhyme as good\ncat donald_trump_song\nmore donald_trump_song\nOk, but are we sure Donald replaced Maria?\ndiff -u *song \ndiff maria_joanna_song donald_trump_song\nvimdiff *song\n\n\n\n\n\n\nNote\n\n\n\nexit vim using Esc + :q which closes the file, alternatively Esc + :wq which saves the file.\n\n\n\n\n\nHave a look how a loop is being used here\ngrep Lisboa *song\nfor f in *song ; do grep Lisboa $f ; done\nTo show which file Lisboa is found in\nfor f in *song ; do grep -l Lisboa $f ; done\nTo show where is it found (i.e. line number)\nfor f in *song ; do grep -n Lisboa $f ; done\nI have just realised that “joanna” in portuguese is written as “joana” - how do I fix the name of my file\n\noption - fix it but keep the original\n\ncp maria_joanna_song maria_joana_song\n\n# list contents of directory\nls\n\n# other variants\nls -l\nls -la\n\noption - neh, I don’t need the previous version - it will just cause the confusion later on\n\nmv maria_joanna_song maria_joana_song\nNow let’s try previous code\ngrep Maria maria_joanna_song  # of course it doesn't work ... - what shall I do now ?",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#genomics",
    "href": "2025_spring/250416_atkacz.html#genomics",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Taken from official website:\nEDirect will run on Unix and Macintosh computers, and under the Cygwin Unix-emulation environment on Windows PCs. To install the EDirect software, open a terminal window and execute one of the following two commands:\n  sh -c \"$(curl -fsSL https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n\n  sh -c \"$(wget -q https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh -O -)\"\nThis will download a number of scripts and several precompiled programs into an “edirect” folder in the user’s home directory. It may then print an additional command for updating the PATH environment variable in the user’s configuration file. The editing instructions will look something like:\n  echo \"export PATH=\\$HOME/edirect:\\$PATH\" &gt;&gt; $HOME/.bash_profile\nAs a convenience, the installation process ends by offering to run the PATH update command for you. Answer “y” and press the Return key if you want it run. If the PATH is already set correctly, or if you prefer to make any editing changes manually, just press Return.\nOnce installation is complete, run:\n  export PATH=${HOME}/edirect:${PATH}\nto set the PATH for the current terminal session.\n\n\n\n\n\n\nWindows\n\n\n\nYou can try to install SRA-toolkit from here\n\n\n\n\n\nLet’s download some data from the Genbank - it is 16S V4 amplicon data (here the metadata)\nesearch -db sra -query PRJNA783372 | efetch -format runinfo &gt; SraRunTable.csv\nWe only need the SRRnumber, -f1 means first field.\ncut -f1 -d, SraRunTable*.csv  &gt; runs.txt\nTo save time we will work on four samples only\ntail -4 runs.txt &gt; runs4.txt\n\n\n\nOk, now we download the actual raw data\nprefetch --option-file runs4.txt\nIn case of\n\n\n\n\n\n\nNote\n\n\n\nIn case of Command 'prefetch' not found run\nsudo apt install sra-toolkit\n\n\nLet’s unpack it\nfor f in SRR* ; do fastq-dump --split-files  $f ; done\n\n\n\n\n\n\nTip\n\n\n\nTo visualize quality scores, small GUI tool fastqc could be used\n\n\n\n\n\nsudo apt install flash\nFirst merge the reads\n# merge\nflash -m 50 -M 500 -x 0.1 SRR17045222_1.fastq SRR17045222_2.fastq\n\n# rename merged file to index it\nmv out.extendedFrags.fastq file1.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045223_1.fastq SRR17045223_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045226_1.fastq SRR17045226_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045227_2.fastq SRR17045227_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nNow we filter low quality reads (here the data is kind of a fake as the fastq download needs to be adjusted wit the prefetch script)\n# if not installed, do\nsudo apt install vsearch\n\nfor f in file* ; do vsearch --fastq_filter $f --fastq_maxee 1.0 --fastqout $f\\_filtered ; done\nIt is getting messy here (delete unwanted files)\nrm *notCombined*\nrm *hist\nrm *histogram\nrm *fastq\n\n\n\n\n\n\nTip\n\n\n\nToo lazy to empty the trash yourself ? the terminal will do that for us gio trash --empty\n\n\nLet’s convert fastq to fasta\n# if not installed, do\nsudo apt install seqtk\n\nfor f in *fastq_filtered ; do seqtk seq -a $f &gt; $f.fasta ; done ; \n\n\n\n\n\n\nCaution\n\n\n\nBlast is commented out as it is unlikely to work - we could try with internal database one day\n\n\nTo save time let’s just use 10 reads from each file for taxonomic identification - we will use GenBank nt database\nfor f in *fastq_filtered.fasta ; do head -20 $f &gt; $f\\head ; done\nSimplify the naming\n\n# if not installed, do\nsudo apt install rename\n\nrename 's/.fastq_filtered.fastahead/.fasta/g' *fastq_filtered.fastahead ;\n\n\n\nLet’s try - it may not work at all, first install\nsudo apt install ncbi-blast+\nblastn -query file1.fasta -db nt -remote -outfmt 6 -max_target_seqs 1 -perc_identity 95 &gt; file1_blast.txt \n\nblastn -query file1.fasta -db nr -remote  &gt; file1_blast2.txt\n\n\nUse the web-based blast, for example on the ncbi server.\nLet’s create ASV table and visualise the data\ncat file[0-9]*.fastq_filtered.fasta &gt; all_samples.fasta\n# if not installed, do\nsudo apt install vsearch\n\nvsearch --derep_fulllength all_samples.fasta   --output dereplicated.fasta  --sizeout  --relabel ASV_\nvsearch --sortbysize dereplicated.fasta  --output dereplicated_no_singletons.fasta  --minsize 2\nvsearch --cluster_unoise dereplicated_no_singletons.fasta --minsize 2  --unoise_alpha 2   --centroids ASVs.fasta\nvsearch --usearch_global all_samples.fasta --db ASVs.fasta   --id 0.97     --otutabout ASV_table.txt\nWe will use MicrobiomeAnalyst - every tool needs the input in some specific format - here we need to change the table as well\nsed 's/#OTU ID/#NAME/g' ASV_table.txt &gt; ASV_table_MA.txt\nWe need some metadata (all invented here)\nhead -1 ASV_table_MA.txt | fmt -1 | sed 's/NAME/NAME, sample/g'  | sed 's/11$/11,fish/g' | sed 's/13$/13,fish/g' | sed 's/21$/21,fish/g' | sed 's/22$/22,fish/g' |sed 's/23$/23,not_a_fish/g' | sed 's/24$/24,not_a_fish/g' |sed 's/25$/25,not_a_fish/g' | sed 's/27$/27,not_a_fish/g' &gt; metadata.csv",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#visualise-the-data-with-python",
    "href": "2025_spring/250416_atkacz.html#visualise-the-data-with-python",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Following this link you can interrogate the same table (ASV_table_MA.txt) in the interactive jupyter notebook.\nTo compute Braycurtis distances \nTo visualize the PCoA",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#local-python-scripts",
    "href": "2025_spring/250416_atkacz.html#local-python-scripts",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "If you have python installed you can run scripts which are in this folder.\npython append_braycurtis3.py ASV_table_MA.txt &&\npython pcoa_with_metadata_quick_legend.py",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#finally",
    "href": "2025_spring/250416_atkacz.html#finally",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "The power of command line lies in the fact, that once you test your pipeline command by command, you can copy all the commands into a so-called bash script and rerun the whole thing at once.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nYou may want to create a separate new virtual environment (example uses conda) and install necessary dependencies first\n# create virtual environment\nconda create -n gen-test python\n\n# activate it\nconda activate gen-test\n\n# install deps (pip or conda install)\npip install dask\npip install --upgrade --force-reinstall scipy dask\npip install pyarrow\n\npython3 append_braycurtis3.py ASV_table_MA.txt\n\n# dependencies for the second script\npip install numpy==1.24.3\npip install seaborn\npip install panel\npip install scikit-bio\n\npython3 pcoa_with_metadata_quick_work.py \n\n\n\n# make a new folder for a test\nmkdir pipeline_test\n\n# copy python scripts too, if you want to visualize the results.\ncp *.py ./pipeline_test\n\n# take the commands from the command list file\ngrep -A 1000 \"genomics part\" course_linux_commands | sed '/fastqc/d' | sed '/\\*\\*\\*/d' &gt; ./pipeline_test/my_first_pipeline.sh \nRun the pipeline with\nbash my_fist_pipeline.sh\n\n\n\n\n\n\nCaution\n\n\n\nYou might need to make the script executable\nchmod +x my_fist_pipeline.sh\n\n# then you can run directly\n./my_fist_pipeline.sh",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "basics/abundance_tables.html",
    "href": "basics/abundance_tables.html",
    "title": "Abundance table preprocessing",
    "section": "",
    "text": "Work in progress\nThis is a personal commitment to understand the effect (variance) of abundance table normalization and scaling methods on downstream tasks, which may be differential analysis etc.\nMost well-known packages have the normalization methods implemented so raw data tables can be supplied to them, such as QIIME2 or refseq. For EMO-BON analysis, I do not use those (might be a mistake, because of bug risks), so I need to understand them properly.\nLoad SSU combined taxonomy from 181 EMO-BON samplings.\nCode\nimport pandas as pd\nimport numpy as np\nfrom skbio.diversity import beta_diversity\n\n#| code-fold: false\n# read the data from github\nssu_url = \"https://github.com/emo-bon/momics-demos/raw/refs/heads/main/data/parquet_files/metagoflow_analyses.SSU.parquet\"\n\nssu = pd.read_parquet(ssu_url)\n\n# change abundance to int\nssu['abundance'] = ssu['abundance'].astype(int)\nssu.head()\n\n\n\n\n\n\n\n\n\nref_code\nncbi_tax_id\nabundance\nsuperkingdom\nkingdom\nphylum\nclass\norder\nfamily\ngenus\nspecies\n\n\n\n\n0\nEMOBON00084\n2157\n7\nArchaea\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n1\nEMOBON00084\n1801616\n1\nArchaea\n\nCandidatus_Woesearchaeota\nNone\nNone\nNone\nNone\nNone\n\n\n2\nEMOBON00084\n28890\n1\nArchaea\n\nEuryarchaeota\nNone\nNone\nNone\nNone\nNone\n\n\n3\nEMOBON00084\n183968\n1\nArchaea\n\nEuryarchaeota\nThermococci\nNone\nNone\nNone\nNone\n\n\n4\nEMOBON00084\n192989\n3\nArchaea\n\nNanoarchaeota\nNone\nNone\nNone\nNone\nNone\nLet’s order them by abundance\nCode\nssu.sort_values(by='abundance', inplace=True, ascending=False)\n\nssu\n\n\n\n\n\n\n\n\n\nref_code\nncbi_tax_id\nabundance\nsuperkingdom\nkingdom\nphylum\nclass\norder\nfamily\ngenus\nspecies\n\n\n\n\n9377\nEMOBON00009\n1236\n26938\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n\n\n9014\nEMOBON00010\n1236\n26431\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n\n\n10691\nEMOBON00008\n1236\n14402\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n\n\n68804\nEMOBON00125\n54526\n13108\nBacteria\n\nProteobacteria\nAlphaproteobacteria\nPelagibacterales\nNone\nNone\nNone\n\n\n17321\nEMOBON00003\n72037\n12545\nEukaryota\nMetazoa\nArthropoda\nHexanauplia\nNone\nNone\nNone\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n111303\nEMOBON00179\n39236\n1\nEukaryota\nMetazoa\nPlatyhelminthes\nCatenulida\nNone\nNone\nNone\nNone\n\n\n111305\nEMOBON00179\n27901\n1\nEukaryota\nMetazoa\nPlatyhelminthes\nRhabditophora\nRhabdocoela\nNone\nNone\nNone\n\n\n111308\nEMOBON00179\n3166\n1\nEukaryota\nViridiplantae\nChlorophyta\nChlorophyceae\nNone\nNone\nNone\nNone\n\n\n58993\nEMOBON00136\n2682468\n1\nEukaryota\nViridiplantae\nChlorophyta\n\nPyramimonadales\nPyramimonadaceae\nNone\nNone\n\n\n58999\nEMOBON00136\n70447\n1\nEukaryota\nViridiplantae\nChlorophyta\nMamiellophyceae\nMamiellales\nBathycoccaceae\nOstreococcus\nNone\n\n\n\n\n111320 rows × 11 columns",
    "crumbs": [
      "Basics",
      "Abundance table preprocessing"
    ]
  },
  {
    "objectID": "basics/abundance_tables.html#total-sum-scaling-tss-followed-by-square-root-transformation",
    "href": "basics/abundance_tables.html#total-sum-scaling-tss-followed-by-square-root-transformation",
    "title": "Abundance table preprocessing",
    "section": "Total Sum Scaling (TSS) followed by Square Root Transformation",
    "text": "Total Sum Scaling (TSS) followed by Square Root Transformation\n\nTSS\n\nconverts raw counts into relative abundances, alternative name Relative Abundance Normalization. Simple division by sum of abundances in each sample separately.\nPurpose: Adjusts for varying sequencing depths between samples.\nreference, McMurdie, P. J., & Holmes, S. (2014). Waste not, want not: why rarefying microbiome data is inadmissible. PLoS computational biology, 10(4), e1003531.\n\n\n\nSquare root transformation to relative abundances\n\nThis is a variance-stabilizing transformation — it reduces the effect of highly abundant taxa and improves comparability across samples.\nIt’s commonly used before distance-based analyses like Bray–Curtis dissimilarity or ordination (e.g., NMDS, PCoA).\nreference, Legendre, P., & Gallagher, E. D. (2001). Ecologically meaningful transformations for ordination of species data. Oecologia, 129(2), 271–280.\n\nHere is a function to pivot the taxonomy:\n\n\nCode\ndef pivot_taxonomic_data(df: pd.DataFrame, values_col='abundance') -&gt; pd.DataFrame:\n    \"\"\"\n    Prepares the taxonomic data (LSU and SSU tables) for analysis.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing taxonomic information.\n\n    Returns:\n        pd.DataFrame: A pivot table with taxonomic data.\n    \"\"\"\n    # Select relevant columns\n    df['taxonomic_concat'] = (\n        df['ncbi_tax_id'].astype(str) + \n        ';sk_' + df['superkingdom'].fillna('') + \n        ';k_' + df['kingdom'].fillna('') + \n        ';p_' + df['phylum'].fillna('') + \n        ';c_' + df['class'].fillna('') + \n        ';o_' + df['order'].fillna('') + \n        ';f_' + df['family'].fillna('') + \n        ';g_' + df['genus'].fillna('') + \n        ';s_' + df['species'].fillna('')\n    )\n    pivot_table = df.pivot_table(\n        index=['ncbi_tax_id','taxonomic_concat'], \n        columns='ref_code', \n        values=values_col,\n    ).fillna(0)\n    pivot_table = pivot_table.reset_index()\n    # change inex name\n    pivot_table.columns.name = None\n\n    return pivot_table\n\n\n, and methods to calculate to apply various scaling and normalization methods:\n\ndef TSS(df, sampleIds='ref_code'):\n    \"\"\" Calculate TSS\"\"\"\n    df['abundance_TSS'] = df.groupby(sampleIds)['abundance'].transform(lambda x: x / x.sum())\n    return df",
    "crumbs": [
      "Basics",
      "Abundance table preprocessing"
    ]
  },
  {
    "objectID": "basics/abundance_tables.html#now-i-want-to-systematically-transform-and-send-downstream",
    "href": "basics/abundance_tables.html#now-i-want-to-systematically-transform-and-send-downstream",
    "title": "Abundance table preprocessing",
    "section": "Now I want to systematically transform and send downstream",
    "text": "Now I want to systematically transform and send downstream\nDownstream tasks are\n\nBeta diversity\nPCoA\n???\n\n\nssu = TSS(ssu)\n\nassert ssu[ssu['ref_code'] == 'EMOBON00009']['abundance_TSS'].sum() == 1.0\nssu.head()\n\n\n\n\n\n\n\n\nref_code\nncbi_tax_id\nabundance\nsuperkingdom\nkingdom\nphylum\nclass\norder\nfamily\ngenus\nspecies\nabundance_TSS\n\n\n\n\n9377\nEMOBON00009\n1236\n26938\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n0.457989\n\n\n9014\nEMOBON00010\n1236\n26431\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n0.458879\n\n\n10691\nEMOBON00008\n1236\n14402\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n0.216178\n\n\n68804\nEMOBON00125\n54526\n13108\nBacteria\n\nProteobacteria\nAlphaproteobacteria\nPelagibacterales\nNone\nNone\nNone\n0.376753\n\n\n17321\nEMOBON00003\n72037\n12545\nEukaryota\nMetazoa\nArthropoda\nHexanauplia\nNone\nNone\nNone\nNone\n0.337676\n\n\n\n\n\n\n\nCalculate and plot Beta diversity\n\n\nCode\nimport seaborn as sns\n\npivot = pivot_taxonomic_data(ssu, values_col='abundance_TSS')\nmetric = 'braycurtis'\npivot.head()\n\n\n\n\n\n\n\n\n\nncbi_tax_id\ntaxonomic_concat\nEMOBON00001\nEMOBON00003\nEMOBON00004\nEMOBON00005\nEMOBON00006\nEMOBON00007\nEMOBON00008\nEMOBON00009\n...\nEMOBON00242\nEMOBON00243\nEMOBON00244\nEMOBON00245\nEMOBON00246\nEMOBON00247\nEMOBON00248\nEMOBON00249\nEMOBON00250\nEMOBON00251\n\n\n\n\n0\n2\n2;sk_Bacteria;k_;p_;c_;o_;f_;g_;s_\n0.018025\n0.017335\n0.027319\n0.047492\n0.04602\n0.069328\n0.060912\n0.00692\n...\n0.017378\n0.013971\n0.051088\n0.016756\n0.053871\n0.019248\n0.041221\n0.025231\n0.039109\n0.026800\n\n\n1\n6\n6;sk_Bacteria;k_;p_Proteobacteria;c_Alphaprote...\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n0.00000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\n10\n10;sk_Bacteria;k_;p_Proteobacteria;c_Gammaprot...\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000017\n0.000000\n0.00000\n...\n0.000000\n0.000000\n0.000201\n0.000051\n0.000145\n0.000050\n0.000123\n0.000118\n0.000088\n0.000182\n\n\n3\n16\n16;sk_Bacteria;k_;p_Proteobacteria;c_Betaprote...\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n0.00000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n4\n18\n18;sk_Bacteria;k_;p_Proteobacteria;c_Deltaprot...\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n0.00000\n...\n0.000097\n0.000219\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n5 rows × 183 columns\n\n\n\n\n\nCode\nbeta = beta_diversity(metric, pivot.iloc[:, 2:].T)\nsns.heatmap(beta.to_data_frame(), vmin=0, vmax=1.0, cmap=\"viridis\")\n\n\n\n\n\n\n\n\n\n\nHow do I evaluate difference between methods?",
    "crumbs": [
      "Basics",
      "Abundance table preprocessing"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOinformatics: Handy and Practical",
    "section": "",
    "text": "BIOinformatics: Handy and Practical\nThis is a compilation of bioinformatics knowledge at CCMAR-Algarve. First series of hands-on workshop happened in Spring 2025.\nUntil this day, three hands-on workshops designed to equip researchers with essential data analysis skills for bioinformatics have been announced. Every month, you will get served a workshop which cover a key aspect of biological data analysis, from RNA-seq to microbiome analysis, helping you turn raw sequence files into meaningful biological insights.\nTo visit the source files for the workshops and the webpage, visit our Github. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Introduction",
      "BIOinformatics: Handy and Practical"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "2025_spring/250604_iduarte/250604_iduarte.html",
    "href": "2025_spring/250604_iduarte/250604_iduarte.html",
    "title": "RNA-Seq Data Analysis in R | From Counts to Biological Insights",
    "section": "",
    "text": "RNA-seq Analysis Crash Course\nWelcome! This crash course walks you through RNA-seq data analysis in R: From raw count matrices to functional interpretation of differentially expressed genes.\n\nLearning Objectives\n\nPerform quality control of the data.\nIdentify the R commands needed to run a differential expression analysis using edgeR and DESeq2.\nVisualize the DE results.\nUndertake functional annotation of data, and perform functional enrichment analysis.\nVisualize functionally enriched biological categories.\n\n\nReferences\nSelf-learning & Training | Differential Gene Expression Analysis (bulk RNA-seq) (here).\nThese course materials are based on the tutorials developed by the teaching team at Harvard Chan Bioinformatics Core (HBC).\n\nTheoryTutorial HBCTutorial NF-core\n\n\nRNA-seq is a leading method for quantifying RNA levels in biological samples, leveraging next-generation sequencing (NGS) technologies. The process begins with RNA extraction and conversion to cDNA, followed by sequencing to produce reads representing the RNA present in a sample.\n\nRNA-seq OverviewDifferential ExpressionFunctional Analysis\n\n\nThese data are processed through a standard workflow with three main stages, as summarized in the figure below:\n\nData pre-processing – improves read quality by removing contaminants and adapters.\nAlignment and quantification – maps reads to a reference genome and estimates gene expression, either through traditional or faster lightweight methods.\nDifferential expression analysis – identifies and visualizes genes with significant expression differences.\n\nAdditional downstream analyses (e.g., functional enrichment, co-expression, or multi-omics integration) are popular ways to derive biological insights from these analyses.\n\n\n\nAdapted from: https://nf-co.re/rnaseq/dev/docs/usage/differential_expression_analysis/theory\n\n\nThis course will not cover the first two steps. It will begin with a gene count matrix and proceed with differential expression analysis, visualization, and a brief overview of functional enrichment.\n\n\n\nDifferential expression (DE) analysis compares gene expression levels across conditions (e.g., disease vs. healthy) to identify genes with statistically significant changes. This is typically done using tools like DESeq2, a robust R package designed for analyzing RNA-seq count data.\n\nInput Requirements:\n\nA count matrix (genes × samples).\nA metadata table describing sample attributes.\n\nQuality Control:\n\nUse PCA and hierarchical clustering to explore variation and detect outliers.\nTransform counts using variance stabilizing transformation (vst) or regularized log (rlog) to ensure comparable variance across genes, improving downstream analysis.\n\nFiltering:\n\nRemove genes with low or zero counts to improve sensitivity and reduce false positives.\n\nDesign Formula:\n\nSpecifies how gene counts depend on experimental factors.\nCan include main conditions and covariates (e.g., sex, batch, stage).\nExample:\ndesign = ~ condition\ndesign = ~ sex + developmental_stage + condition\nThe main factor of interest is usually placed last for clarity.\n\n\n\nDE with DESeq2\nDESeq2 is a widely used R package for identifying differentially expressed (DE) genes from RNA-seq count data. RNA-seq data typically exhibit many low-count genes and a long-tailed distribution due to highly expressed genes, requiring specialized statistical modeling. The major steps in DESeq2 are the following:\n\nNormalization\n\nAdjusts for sequencing depth and RNA composition using size factors calculated via the median ratio method.\nNormalized counts are used for visualization but raw counts must be used for DESeq2 modeling.\n\nDispersion Estimation\n\nRNA-seq data show overdispersion (variance &gt; mean).\nDESeq2 models count data using the negative binomial distribution.\nDispersion is estimated:\n\nGlobally (common dispersion),\nPer gene (gene-wise dispersion),\nThen refined through shrinkage toward a fitted mean-dispersion curve to improve stability, especially with small sample sizes.\n\nGenes with extreme variability are not shrunk to avoid false positives.\n\nModel Fitting and Hypothesis Testing\n\nA generalized linear model (GLM) is fit to each gene’s normalized counts.\nDESeq2 tests whether gene expression differs significantly between groups:\n\nWald test for simple comparisons (e.g., treated vs. control),\nLikelihood Ratio Test (LRT) for more complex designs with multiple variables.\n\nEach test returns a log2 fold change and a p-value.\n\nMultiple Testing Correction\n\nTo control for false positives from testing thousands of genes, DESeq2 adjusts p-values using Benjamini-Hochberg FDR correction.\nAn FDR cutoff of &lt;0.05 means that 5% of DE genes may be false positives.\n\n\n\n\n\n\nAfter identifying differentially expressed (DE) genes, functional analysis helps interpret their biological relevance by uncovering the pathways, processes, or interactions they may be involved in. This includes:\n\nFunctional enrichment analysis – identifies overrepresented biological processes, molecular functions, cellular components, or pathways.\nNetwork analysis – groups genes with similar expression patterns to reveal potential interactions.\n\nThis course focuses on Over-Representation Analysis (ORA), a common enrichment method that uses the hypergeometric test to assess whether certain biological pathways or gene sets are statistically enriched in the DE gene list.\nKey components of ORA:\n\nUniverse – the full set of genes considered (e.g., all genes in the genome).\nGene Set – a group of genes annotated to a particular function or pathway (e.g., from Gene Ontology).\nGene List – the list of DE genes identified in the analysis.\n\nThe test evaluates whether the overlap between the DE gene list and a gene set exceeds what would be expected by chance, pointing to potentially meaningful biological mechanisms.\nTools commonly used for functional enrichment include Gene Ontology, KEGG, Reactome, clusterProfiler, and g:Profiler. These support the biological interpretation of DE results and help uncover pathways affected by the experimental condition.\n\n\n\n\n\n\nQuality control\n\nSetup\n\nLoad packages and data\n\n\n\nCode\n# Load required packages\nlibrary(here)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(edgeR)\nlibrary(limma)\nlibrary(DESeq2)\n\n# Create list to save the analysis objects\nde_edger &lt;- list()\nde_deseq &lt;- list()\n\n# Load gene counts data and sample metadata\ncounts &lt;- readxl::read_xlsx(here::here(\"2025_spring/250604_iduarte/data/diet_mice_counts.xlsx\"), col_names = TRUE, sheet = 1)\n\nmetadata &lt;- read.table(file=here::here(\"2025_spring/250604_iduarte/data/diet_mice_metadata.txt\"), \n                        header = TRUE,\n                        sep = \"\\t\", dec = \".\",\n                        stringsAsFactors = TRUE)\n\n\n\nCheck if the data and metadata sample ids match\n\n\n\nCode\n### Ensure the sample metadata matches the identity and order of the columns in the expression data\n\n# Order the sample ids from the metadata (smaller file) by the colnames from the counts\nif (setequal(colnames(counts)[-c(1, 2)], metadata$sample_id)) {\n  metadata &lt;- metadata[match(colnames(counts)[-c(1, 2)], metadata$sample_id),]\n} else {\n  stop(\"Error: The set of sample ids is not equal in both datasets.\")\n}\n\n\n\nTidy the data\n\n\n\nCode\n# Transform count data-frame to matrix with row names\n# and remove NAs (if they exist)\ncounts_matrix &lt;- counts[-1] %&gt;%\nna.omit() %&gt;%\ncolumn_to_rownames(var = \"gene_symbol\") %&gt;%\nas.matrix()\n\n\n\nPCA | validation of experimental design\nHierarchical clustering of the correlation matrix\n\n\n\n\nDifferential expression\n\nedgeR analysis\n\nCreate design and contrast matrices | Modelling Diet and Gender\n\n\nCode\n# Design matrix using the model for categorical variables diet and gender\ndesign_diet &lt;- model.matrix( ~ 0 + diet + gender, data = metadata)\n#design_diet &lt;- model.matrix( ~ 0 + diet, data = metadata)\n\n# Contrasts matrix: Differences between diets\ncontrasts_diet &lt;- limma::makeContrasts(\n(dietfat - dietlean),\nlevels=colnames(design_diet)\n)\n\n\n\nExtract differentially expressed genes\n\n\n\nCode\n# Create a list \n\n# Create a DGEList object\nde_edger$dge_data &lt;- DGEList(counts = counts_matrix)\n\n# Filter low-expression genes\nde_edger$keep &lt;- filterByExpr(de_edger$dge_data,\ndesign = design_diet)\n\nde_edger$dge_data_filtered &lt;- de_edger$dge_data[de_edger$keep, , \n                                                keep.lib.sizes=FALSE]\n\n# Perform Library Size Normalization | Slow step\nde_edger$dge_data_filtered &lt;- calcNormFactors(de_edger$dge_data_filtered)\n\n# Estimate dispersions | Slow step\nde_edger$dge_data_filtered &lt;- estimateDisp(de_edger$dge_data_filtered,\ndesign = design_diet)\n\n### To perform likelihood ratio tests\n# Fit the negative binomial generalized log-linear model\nde_edger$fit &lt;- glmFit(de_edger$dge_data_filtered,\ndesign=design_diet,\ncontrast = contrasts_diet)\n\n# Perform likelihood ratio tests\nde_edger$lrt &lt;- glmLRT(de_edger$fit)\n\n# Extract the differentially expressed genes\nde_edger$topGenes &lt;- topTags(de_edger$lrt, n=NULL,\nadjust.method = \"BH\", \nsort.by = \"PValue\", \np.value = 0.05)\n\n# Look at the Differentially expressed genes\nde_edger$topGenes\n\n\ndata frame with 0 columns and 0 rows\n\n\n\nDESeq2 analysis\n\nDetailed explanations can be found here.\n\n\nCode\n# Step 1: Create a DESeqDataSet object\n# The matrix is generated by the function\nde_deseq$dds &lt;- DESeqDataSetFromMatrix(countData = counts_matrix,\n                              colData = metadata,\n                              design = ~ 0 + diet + gender)\n\n# Step 2: Run the DESeq function to perform the analysis\nde_deseq$dds &lt;- DESeq(de_deseq$dds)\n\n# Step 3: Extract results\n# Replace 'condition_treated_vs_untreated' with the actual comparison you are interested in\nde_deseq$results &lt;- results(de_deseq$dds, contrast = c(\"diet\", \"fat\", \"lean\"))\n\n# Step 4: Apply multiple testing correction\n# The results function by default applies the Benjamini-Hochberg procedure to control FDR\n# Extract results with adjusted p-value (padj) less than 0.05 (common threshold for significance)\nde_deseq$significant_results &lt;- de_deseq$results[which(de_deseq$results$padj &lt; 0.05), ]\n\n# View the differentially expressed genes\nde_deseq$significant_results[order(de_deseq$significant_results$padj), ]\n\n\nlog2 fold change (MLE): diet fat vs lean \nWald test p-value: diet fat vs lean \nDataFrame with 69 rows and 6 columns\n        baseMean log2FoldChange     lfcSE      stat      pvalue        padj\n       &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;   &lt;numeric&gt;   &lt;numeric&gt;\nACSF3    31.3372       -2.02757  0.409439  -4.95208 7.34246e-07 5.50685e-05\nACSM3    33.0905       -2.04259  0.434317  -4.70299 2.56379e-06 9.61422e-05\nACAD10   34.4381       -1.90459  0.444229  -4.28741 1.80769e-05 1.45713e-04\nTRIAP1   34.0932       -1.96599  0.450039  -4.36850 1.25104e-05 1.45713e-04\nECI1     27.5814       -1.81494  0.408689  -4.44087 8.95956e-06 1.45713e-04\n...          ...            ...       ...       ...         ...         ...\nPHYH     24.3400      -1.063285  0.430853  -2.46786   0.0135923   0.0156834\nACOT11   21.5426      -0.956752  0.393676  -2.43030   0.0150862   0.0171434\nTHEM4    21.9723      -0.958742  0.402069  -2.38452   0.0171014   0.0191433\nHINT2    22.3808      -0.959613  0.434832  -2.20686   0.0273238   0.0301365\nDECR1    29.0965      -1.013954  0.467673  -2.16808   0.0301523   0.0327743\n\n\n\nVisualize the data\n\n\nCode\n# DESeq2\n\n# DESeq2 creates a matrix when you use the counts() function\n## First convert normalized_counts to a data frame and transfer the row names to a new column called \"gene\"\nnormalized_counts &lt;- counts(de_deseq$dds, normalized=T) %&gt;% \n                     data.frame() %&gt;%\n                     rownames_to_column(var=\"gene_symbol\") %&gt;%\n                     as_tibble() \n\n# Plot expression for single gene\nplotCounts(de_deseq$dds, gene=\"TAMM41\", intgroup=\"diet\") \n\n\n\n\n\n\n\n\n\nCode\nplotCounts(de_deseq$dds, gene=\"TAMM41\", intgroup=\"gender\") \n\n\n\n\n\n\n\n\n\nCode\n# # Save plotcounts to a data frame object to use ggplots\nd &lt;- plotCounts(de_deseq$dds, gene=\"TAMM41\", intgroup=\"diet\", returnData=TRUE)\n\n# View d\nhead(d)\n\n\n          count diet\nmus48 20.373530  fat\nmus47 28.605416  fat\nmus54  0.500000  fat\nmus28  3.120844 lean\nmus52  8.149325  fat\nmus38  7.889512  fat\n\n\nCode\n# Draw with ggplot a single gene\nggplot(d, aes(x = diet, y = count, color = diet)) + \n    geom_point(position=position_jitter(w = 0.1,h = 0)) +\n    ggrepel::geom_text_repel(aes(label = rownames(d))) + \n    theme_bw() +\n    ggtitle(\"TAMM41\") +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\n# View the top 20 genes\n## Order results by padj values\ntop12_sigOE_genes &lt;- rownames(as.data.frame(de_deseq$significant_results))[1:12]\n\n## normalized counts for top 20 significant genes\ntop12_sigOE_norm &lt;- normalized_counts %&gt;%\n        filter(gene_symbol %in% top12_sigOE_genes)\n\n# Make a tidy table to plot\ntop12_counts &lt;- pivot_longer(top12_sigOE_norm, starts_with(\"mus\"), names_to = \"sample_id\", values_to = \"ncounts\" )\n\n# Add metadata\ntop12_counts_metadata &lt;- left_join(top12_counts, metadata, by = \"sample_id\")\n\n# ## Plot using ggplot2\nggplot(top12_counts_metadata, aes(x = gene_symbol, y = ncounts)) +\n        geom_boxplot(aes(fill = diet)) +\n        scale_y_log10() +\n        xlab(\"Genes\") +\n        ylab(\"log10 Normalized Counts\") +\n        ggtitle(\"Top 12 Significant DE Genes\") +\n        theme_bw() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    theme(plot.title = element_text(hjust = 0.5)) \n\n\n\n\n\n\n\n\n\nCode\n# ## Boxplots of diet per genes\nggplot(top12_counts_metadata) +\n        geom_boxplot(aes(x = diet, y = ncounts, fill = diet)) +\n        scale_y_log10() +\n        xlab(\"Diet\") +\n        ylab(\"log10 Normalized Counts\") +\n        ggtitle(\"Top 12 Significant DE Genes\") +\n        theme_bw() +\n  facet_wrap(facets=\"gene_symbol\")\n\n\n\n\n\n\n\n\n\nCode\n# ## Boxplots of gender per genes\nggplot(top12_counts_metadata) +\n        geom_boxplot(aes(x = interaction(gender, diet), y = ncounts, \n                         fill = interaction(gender, diet)), show.legend = FALSE) +\n        scale_y_log10() +\n        xlab(\"Gender.Diet\") +\n        ylab(\"log10 Normalized Counts\") +\n        ggtitle(\"Top 12 Significant DE Genes\") +\n        theme_bw() +\n        theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  facet_wrap(facets=\"gene_symbol\")\n\n\n\n\n\n\n\n\n\nCode\n## Volcanoplot \n\nas.data.frame(de_deseq$results) %&gt;%\n  rownames_to_column(var=\"gene_symbol\") -&gt; results_df\n\nggplot(results_df, aes(x=log2FoldChange, y=-log10(padj))) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nFunctional enrichment\n\nFunctional annotation\nFunctional enrichment\nBiological insights\n\n\n\n\n\nDifferential Expression Analysis with DESeq2\nIn this tutorial, we will guide you through the practical steps necessary to set up the RStudio project, load the required libraries and data, execute the DESeq2 analysis, and a final simple functional enrichment analysis.\nReference: https://nf-co.re/rnaseq/3.18.0/docs/usage/deanalysis/de_rstudio\n\n1. Setup2. Quality control3. Visualise the results4. Functional analysis\n\n\nAs with any analysis, the first step is to create a folder to store your work.\n\nChoose an appropriate location on your computer, then set up the following folder structure:\n\n  rnaseq_counts2bio_course/    \n  ├── data/    \n  ├── scripts/    \n  ├── de_results/    \n  └── sandbox/   \n\n\n\n\n\n\nTip: Set Up a Clear Folder Structure Early\n\n\n\n\n\nBefore collecting data, define a clear folder structure and file naming convention. This improves organization, avoids confusion, and supports collaboration. A consistent setup helps you and your team quickly locate and understand files.\nStart organized - your future self (and collaborators) will thank you!\nSuggested minimal structure for a data analysis project:\nproject_name_no_spaces_no_special_chars/    \n├── data/            # Raw and processed data\n│   ├── raw/         \n│   └── processed/   \n├── scripts/         # Analysis and processing code\n├── output/          # Figures, tables, results\n└── sandbox/         # Exploratory work (not for sharing)\n\n\n\n\nCreate a new RStudio project inside the rnaseq_counts2bio_course folder:\n\n2.1 Go to the File menu and select New Project;\n2.2 Select New Directory, New Project, name the project as shown below and click on Create Project;\nThe new project will be automatically opened in RStudio.\nWe can check whether we are in the correct working directory with getwd().\nNext, go to the File menu, select New File and then R Script to create a script editor in which we will save all commands required for the analysis. In the editor type:\n\n\nCode\n#### Differential expression analysis with DESeq2 ####\n\n\nand save the file as de_script.R inside the scripts folder. From now on, each command described in the course can be added to your script.\nThe analysis requires several R packages. To use them, we need to load the following libraries:\n#### Loading libraries ####\n\n# tidyverse: collection of R packages for data manipulation, visualization and modeling\n\nlibrary(\"tidyverse\")\n\n# DESeq2: package for differential gene expression analysis\n\nlibrary(\"DESeq2\")\n\n# pheatmap: package for creating heatmaps, which will be used to visualise the results\n\nlibrary(\"pheatmap\")\n\n# RColorBrewer: package for creating color palettes, which will be used to customise the heatmaps\n\nlibrary(\"RColorBrewer\")\n\n# ggrepel: package that provides geoms for ggplot2 to repel overlapping text labels in the plots\n\nlibrary(\"ggrepel\")\nand the pre-computed DESeq2 object (dds) generated by the nfcore/rnaseq pipeline. In this tutorial we will analyse the dds object generated by running the alignment with STAR and the quantification with Salmon:\n#### Import the dds obtained from nfcore/rnaseq ####\n\nload(\"/workspace/gitpod/training/results_star_salmon/star_salmon/deseq2_qc/deseq2.dds.RData\")\nAlternatively, a user could choose to analyse the the dds object generated by running only Salmon for both lightweight alignment and quantification.\nIn DESEq2, the dds object is a central data structure that contains the following components:\n\ncountData: a matrix of raw count data, where each row represents a gene and each column represents a sample;\ncolData: a data frame containing information about the samples, such as the experimental design, treatment and other relevant metadata;\ndesign: a formula specifying the experimental design utilised to estimate the dispersion and the log2 fold change.\n\nAll these components can be checked with specific commands:\n#### dds inspection ####\n\nhead(counts(dds)) # to check the raw counts\n\ncolData(dds) # to check the sample info\n\ndesign(dds) # to check the design formula\nThe colData and the design are the ones created within the nfcore/rnaseq pipeline and must be reorganised prior to the analysis. With the following commands we will create our metadata starting from the info stored in the dds. We will rename the column of the colData, we will ensure that the rownames of the metadata are present in the same order as the column names and finally we will update the colData of the dds object with our newly created metadata.\n#### Creation of metadata starting from the dds colData ####\n\nmetadata &lt;- DataFrame(\n    sample = colData(dds)$sample,\n    condition = colData(dds)$Group1,\n    replica = colData(dds)$Group2\n)\n\n# Assign names to rows of metadata\n\nrownames(metadata) &lt;- colnames(counts(dds))\n\n# Fill the dds colData with the generated metadata\n\ncolData(dds) &lt;- metadata\nTo avoid errors in DESeq2 is essential to check that sample names match between the colData and the countData, and that the sample are in the correct order:\n#### Check that sample names match in both files ####\n\nall(colnames(dds$counts) %in% rownames(metadata)) # Must be TRUE\n\nall(colnames(dds$counts) == rownames(metadata)) # Must be TRUE\nNow that everything is setted up, we can proceed to generate a new DESeq2 object with the corrected metadata and the right design:\n#### Creation of a new dds ####\n\ndds_new  &lt;- DESeqDataSet(dds, design = ~ condition)\n\n# dds inspection\n\nhead(counts(dds_new)) # to check the raw counts\n\ncolData(dds_new) # to check the sample info\n\ndesign(dds_new) # to check the design formula\nBefore running the different steps of the analysis, a good practice consists in pre-filtering the genes to remove those with very low counts. This is useful to improve computational efficiency and enhance interpretability. In general, it is reasonable to keep only genes with a sum counts of at least 10 for a minimal number of 3 samples:\n#### Pre-filtering ####\n\n# Select a minimal number of samples = 3\n\nsmallestGroupSize &lt;- 3\n\n# Select genes with a sum counts of at least 10 in 3 samples\n\nkeep &lt;- rowSums(counts(dds_new) &gt;= 10) &gt;= smallestGroupSize\n\n# Keep only the genes that pass the threshold\n\ndds_filtered &lt;- dds_new[keep,]\nNow, it is time to run the differential expression analysis with the DESeq() function:\n#### Run the DESeq2 analysis ####\n\ndds_final &lt;- DESeq(dds_filtered)\nThe DESeq() function is a high-level wrapper that simplifies the process of differential expression analysis by combining multiple steps into a single function call. This makes the workflow more user-friendly and ensures that all necessary pre-processing and statistical steps are executed in the correct order. The key functions that DESeq2 calls include:\n\nestimateSizeFactors: to normalise the count data;\nestimateDispersion: to estimate the dispersion;\nnbinomWaldTest: to perform differential expression test.\n\nThe individual functions can be carried out also singularly as shown below:\n#### Differential expression analysis step-by-step ####\n\ndds_final &lt;- estimateSizeFactors(dds_filtered)\n\ndds_final &lt;- estimateDispersions(dds_final)\n\ndds_final &lt;- nbinomWaldTest(dds_final)\n\n\nThe next step in the DESeq2 workflow is to perform quality control (QC) analysis on our data. This analysis is crucial for identifying potential issues ensuring that the data are suitable for downstream analysis. For QC analysis, it is useful to work with transformed versions of the count data, variance-stabilised (vst) or regularised log-transformed (rlog) counts. While, the rlog is more robust to outliers and extreme values, vst is computationally faster and so preferred for larger dataset.\n\n\n\n\n\n\nWarning\n\n\n\nThese transformations are used for visualisation purposes, while DESeq2 requires raw counts (non- normalized (i.e., not TPMs, RPKMs, FPKMs, …)) for differential expression analysis.\n\n\n#### Transform normalised counts for data visualisation ####\n# A user can choose among vst and rlog. In this tutorial we will work with rlog transformed data.\n\nrld &lt;- rlog(dds_final, blind = TRUE)\nThe rlog and the vst transformations have an argument, blind that can be set to:\n\nTRUE (default): useful for QC analysis because it re-estimates the dispersion, allowing for comparison of samples in an unbiased manner with respect to experimental conditions;\nFALSE: the function utilizes the already estimated dispersion, generally applied when differences in counts are expected to be due to the experimental design.\n\nNext, we perform Principal Component Analysis (PCA) to explore the data. DESeq2 provides a built-in function, plotPCA(), which uses ggplot2 for visualisation, taking the rld (or the vst) object as input. Since the treatment is the principal condition of interest in our metadata, we will use the condition information from our metadata to plot the PCA:\n#### Plot PCA ####\n\npca_plot &lt;- plotPCA(rld, intgroup = \"condition\")\n\n# Save the plot\n\nggsave(\"de_results/pca_plot.png\", plot = pca_plot, width = 6, height = 5, dpi = 300)\nThe second essential step in QC analysis is Hierarchical Clustering. Although DESeq2 does not have a built-in function for this analysis, we can use the pheatmap() function from the pheatmap package. We will extract the matrix of rlog-transformed counts from the rld object (pheatmap input), compute pairwise correlations and plot the heatmap:\n#### Plot sample to sample distance (hierarchical clustering) ####\n\n# Extract the matrix of rlog-transformed counts from the rld object\n\nsampleDists &lt;- dist(t(assay(rld)))  # Calculate pairwise distances between samples using the dist() function with Euclidean distance as the default method. By transposing the matrix with t(), we ensure that samples become rows and genes become columns, so that the dist function computes pairwise distances between samples.\n\n# Convert distances to a matrix\n\nsampleDistMatrix &lt;- as.matrix(sampleDists)\n\n# Set the row and column names of the distance matrix\n\nrownames(sampleDistMatrix) &lt;- paste(rld$condition, rld$replica, sep = \"_\")\n\ncolnames(sampleDistMatrix) &lt;- paste(rld$condition, rld$replica, sep = \"_\")\n\n# Define a color palette for the heatmap\n\ncolors &lt;- colorRampPalette(rev(brewer.pal(9, \"Greens\")))(255) # function from RColorBrewer package\n\n# Create the heatmap\n\nclustering_plot &lt;- pheatmap(sampleDistMatrix,\n                            clustering_distance_rows = sampleDists,\n                            clustering_distance_cols = sampleDists,\n                            col = colors,\n                            fontsize_col = 8,\n                            fontsize_row = 8)\n\n# Save the plot\n\nggsave(\"de_results/clustering_plot.png\", plot = clustering_plot, width = 6, height = 5, dpi = 300)\nThe normalised counts stored in the dds can be inspected with the counts() function and saved in our de_results folder:\n#### Inspect the normalised counts ####\n\n# Display the first few rows of the normalised counts to inspect the data\n\nhead(counts(dds_final, normalized = TRUE))\n\n# Display the first few rows of the raw counts (not normalised) to compare with the normalised counts\n\nhead(counts(dds_final))\n\n# Convert the normalised counts from the DESeq2 object to a tibble\n\nnormalised_counts &lt;- as_tibble(counts(dds_final, normalized = TRUE))\n\n# Add a column for gene names to the normalised counts tibble\n\nnormalised_counts$gene &lt;- rownames(counts(dds_final))\n\n# Relocate the gene column to the first position\n\nnormalised_counts &lt;- normalised_counts %&gt;%\n  relocate(gene, .before = control_rep1)\n\n# Save the normalised counts\n\nwrite.csv(normalised_counts, file = \"de_results/normalised_counts.csv\")\nThe results() function in DESeq2 is used to extract the results of the DE analysis. This function takes the dds object as input and returns a DataFrame containing the results of the analysis:\n\nbaseMean: the average expression level of the gene across all samples;\nlog2FoldChange: the log2 fold change of the gene between the condition of interest and the reference level;\nlfcSE: the standard error of the log2 fold change;\nstat: the Wald statistic, which is used to calculate the p-value;\npvalue: the p-value from the Wald test indicates the probability of observing the measured difference in gene expression (log2 fold change) by chance, assuming no true difference exists (null hypothesis). A low p-value suggests that the observed expression change between samples is unlikely due to random chance, so we can reject the null hypothesis –&gt; the gene is differentially expressed;\npadj: the adjusted p-value, which takes into account multiple testing corrections, (Benjamini-Hochberg method default) to control the false discovery rate;\n\nThe results() function returns the results for all genes in the analysis with an adjusted p-value below a specific FDR cutoff, set by default to 0.1. This threshold can be modified with the parameter alpha. The results() function can also be customised to filter the results based on certain criteria (log2 fold change or padj) or to set a specific contrast (specific comparison between two or more levels).\n\n\n\n\n\n\nImportant\n\n\n\nThe order of the contrast names determines the direction of the fold change that is reported in the results. Specifically, the first level of the contrast is the condition of interest and the second level is the reference level.\n\n\nNotice that in this tutorial the contrast is already correctly specified.\n#### Extract results table from the dds object ####\n\nres &lt;- results(dds_final)\n\n# Visualise the results\n\nhead(res)\n\n# Summarise the results showing the number of tested genes (genes with non-zero total read count), the genes up- and down-regulated at the selected threshold (alpha) and the number of genes excluded by the multiple testing due to a low mean count\n\nsummary(res)\n\n# DESeq2 function to extract the name of the contrast\n\nresultsNames(dds_final)\n\n# res &lt;- results(dds, contrast = c(\"design_formula\", \"condition_of_interest\", \"reference_condition\"))\n# Command to set the contrast, if necessary\n\n# Store the res object inside another variable because the original res file will be required for other functions\n\nres_viz &lt;- res\n\n# Add gene names as a new column to the results table\n\nres_viz$gene &lt;- rownames(res)\n\n# Convert the results to a tibble for easier manipulation and relocate the gene column to the first position\n\nres_viz &lt;- as_tibble(res_viz) %&gt;%\n  relocate(gene, .before = baseMean)\n\n# Save the results table\n\nwrite.csv(res_viz, file = \"de_results/de_result_table.csv\")\nIn the Experimental Design section, we emphasised the importance of estimating the log2 fold change threshold using a statistical power calculation, rather than selecting it arbitrarily. This approach ensures that the chosen threshold is statistically appropriate and tailored to the specifics of the experiment. However, since we are working with simulated data for demonstration purposes, we will use a padj threshold of 0.05 and consider genes with a log2 fold change of at least 1 or -1 as differentially expressed.\n#### Extract significant DE genes from the results ####\n\n# Filter the results to include only significantly DE genes with a padj less than 0.05 and a log2FoldChange of at least 1 or -1\n\nresSig &lt;- subset(res_viz, padj &lt; 0.05 & abs(log2FoldChange) &gt; 1)\n\n# Convert the results to a tibble for easier manipulation and relocate the gene column to the first position\n\nresSig &lt;- as_tibble(resSig) %&gt;%\n  relocate(gene, .before = baseMean)\n\n# Order the significant genes by their adjusted p-value (padj) in ascending order\n\nresSig &lt;- resSig[order(resSig$padj),]\n\n# Display the final results table of significant genes\n\nresSig\n\n# Save the significant DE genes\n\nwrite.csv(resSig, file = \"de_results/sig_de_genes.csv\")\n\n\nNow that we have obtained the results of the differential expression analysis, it’s time to visualise the data to gain a deeper understanding of the biological processes that are affected by the experimental conditions. Visualisation is a crucial step in RNA-seq analysis, as it allows us to identify patterns and trends in the data that may not be immediately apparent from the numerical results.\nIn the following sections, we will explore different types of plots that are commonly used to visualise the results of RNA-seq analysis, including:\n\nMA plot: scatter plot commonly utilised to visualise the results of the DE analysis for all the samples. The plot displays the mean of the normalised counts on the x-axis and the log2 fold change on the y-axis. This allows the visualisation of the relationship between the magnitude of the fold change and the mean expression level of the genes. Genes that are differentially expressed will appear farthest from the horizontal line, while genes with low expression levels will appear closer to the line.\n\n#### MA plot ####\n\n# The MA plot is not a ggplot, so we have to save it in a different way\n\n# Open a graphics device to save the plot as a PNG file\n\npng(\"MA_plot.png\", width = 1500, height = 100, res = 300)\n\n# Generate the MA plot (it will be saved to the file instead of displayed on screen)\n\nplotMA(res, ylim = c(-2, 2))\n\n# Close the device to save the file\n\ndev.off()\n\ncounts plot: plot of the normalised counts for a single gene across the different conditions in your experiment. It’s particularly useful for visualising the expression levels of specific genes of interest and comparing them across sample groups.\n\n#### Plot a specific gene in this case ENSG00000142192, a DE gene ####\n\npng(\"de_results/plotCounts.png\", width = 1000, height = 1200, res = 300)\n\nplotCounts(dds_final, gene = \"ENSG00000142192\")\n\ndev.off()\nheatmap: plot of the normalised counts for all the significant genes obtained with the pheatmap() function. The heatmap provides insights into genes and sample relationships that may not be apparent from individual gene plots alone.\n#### Heatmap ####\n\n# Extract only the first column (gene names) from the result object containing the significant genes\n\nsignificant_genes &lt;- resSig[, 1]\n\n# Extract normalised counts for significant genes from the normalised counts matrix and convert the gene column to row names\n\nsignificant_counts &lt;- inner_join(normalised_counts, significant_genes, by = \"gene\") %&gt;%\n  column_to_rownames(\"gene\")\n\n# Create the heatmap using pheatmap\n\nheatmap &lt;- pheatmap(significant_counts,\n                    cluster_rows = TRUE,\n                    fontsize = 8,\n                    scale = \"row\",\n                    fontsize_row = 8,\n                    height = 10)\n\n# Save the plot\n\nggsave(\"de_results/heatmap.png\", plot = heatmap, width = 6, height = 5, dpi = 300)\n\nvolcano plot: scatter plot that displays the log2 fold change on the x-axis and the log transformed padj on the y-axis. This allows for the visualisation of both the magnitude and significance of the changes in gene expression between two conditions. Genes that are differentially expressed (i.e., have a large log2 fold change) and are statistically significant (i.e., have a low padj) will appear in the left (downregulated genes) or in the right (upregulated genes) corners of the plot making easier their identification.\n\n#### Volcano plot ####\n\n# Convert the results to a tibble and add a column indicating differential expression status\n\nres_tb &lt;- as_tibble(res) %&gt;%\n  mutate(diffexpressed = case_when(\n    log2FoldChange &gt; 1 & padj &lt; 0.05 ~ 'upregulated',\n    log2FoldChange &lt; -1 & padj &lt; 0.05 ~ 'downregulated',\n    TRUE ~ 'not_de'))\n\n# Add a new column with gene names\n\nres_tb$gene &lt;- rownames(res)\n\n# Relocate the gene column to the first position\n\nres_tb &lt;-  res_tb %&gt;%\n  relocate(gene, .before = baseMean)\n\n# Order the table by padj and add a new column for gene labels\n\nres_tb &lt;- res_tb %&gt;% arrange(padj) %&gt;%\n  mutate(genelabels = \"\")\n\n# Label the top 5 most significant genes\n\nres_tb$genelabels[1:5] &lt;- res_tb$gene[1:5]\n\n# Create a volcano plot using ggplot2\n\nvolcano_plot &lt;- ggplot(data = res_tb, aes(x = log2FoldChange, y = -log10(padj), col = diffexpressed)) +\n  geom_point(size = 0.6) +\n  geom_text_repel(aes(label = genelabels), size = 2.5, max.overlaps = Inf) +\n  ggtitle(\"DE genes treatment versus control\") +\n  geom_vline(xintercept = c(-1, 1), col = \"black\", linetype = 'dashed', linewidth = 0.2) +\n  geom_hline(yintercept = -log10(0.05), col = \"black\", linetype = 'dashed', linewidth = 0.2) +\n  theme(plot.title = element_text(size = rel(1.25), hjust = 0.5),\n        axis.title = element_text(size = rel(1))) +\n  scale_color_manual(values = c(\"upregulated\" = \"red\",\n                                \"downregulated\" = \"blue\",\n                                \"not_de\" = \"grey\")) +\n  labs(color = 'DE genes') +\n  xlim(-3,5)\n\n# Save the plot\n\nggsave(\"de_results/volcano_plot.png\", plot = volcano_plot, width = 6, height = 5, dpi = 300)\n\n\nThe output of the differential expression analysis is a list of significant DE genes. To uncover the underlying biological mechanisms, various downstream analyses can be performed, such as functional enrichment analysis (identify overrepresented biological processes, molecular functions, cellular components or pathways), and network analysis (group genes based on similar expression patterns to identify novel interactions). To facilitate the interpretation of the resulting list of DE genes, a range of freely available web- and R-based tools can be employed.\nIn this tutorial, we will explore an enrichment analysis technique known as Over-Representation Analysis (ORA), a powerful tool for identifying biological pathways or processes significantly enriched within the list of DE genes. The underlying statistic behind ORA is the hypergeometric test, which considers three key components:\n\nUniverse: the background list of genes (for example the genes annotated in a genome);\nGeneSet: a collection of genes annotated by a reference database (such as Gene Ontology), and known to be involved in a particular biological pathway or process;\nGene List: the differentially expressed genes.\n\nThe hypergeometric test calculates the probability of observing a certain number of genes from the gene set (pathway or process) within the gene list (DE genes) by chance. An important aspect of this analysis is the concept of membership. It defines the relationship between DE genes and genes from the analysed gene set. By knowing which genes belong to which pathway/process, we can determine whether the observed overlap between DE genes and the particular pathway/process is greater than what would be expected by random chance.\n#### Enrichment analysis (ORA) ####\n\n# Loading libraries\n\n# clusterProfiler: package for enrichment analysis\n\nlibrary(clusterProfiler)\n\n# org.Hs.eg.db: package for the human gene annotation database\n\nlibrary(org.Hs.eg.db)\n\n# cowplot: package for combining multiple plots\n\ninstall.packages(\"cowplot\") # To install the package missing in the current RStudio env\n\nlibrary(cowplot)\n\n# Prepare gene list\n# Extract the log2 fold change values from the results data frame\n\ngene_list &lt;- res$log2FoldChange\n\n# Name the vector with the corresponding gene identifiers\n\nnames(gene_list) &lt;- res$gene\n\n# Sort the list in decreasing order (required for clusterProfiler)\n\ngene_list &lt;- sort(gene_list, decreasing = TRUE)\n\n# Extract the significantly differentially expressed genes from the results data frame\n\nres_genes &lt;- resSig$gene\n\n# Run GO enrichment analysis using the enrichGO function\n\ngo_enrich &lt;- enrichGO(\n  gene = res_genes,                # Genes of interest\n  universe = names(gene_list),     # Background gene set\n  OrgDb = org.Hs.eg.db,            # Annotation database\n  keyType = 'ENSEMBL',             # Key type for gene identifiers\n  readable = TRUE,                 # Convert gene IDs to gene names\n  ont = \"ALL\",                     # Ontology: can be \"BP\", \"MF\", \"CC\", or \"ALL\"\n  pvalueCutoff = 0.05,             # P-value cutoff for significance\n  qvalueCutoff = 0.10              # Q-value cutoff for significance\n)\n\n# Create a bar plot of the top enriched GO terms\n\nbarplot &lt;- barplot(\n  go_enrich,\n  title = \"Enrichment analysis barplot\",\n  font.size = 8\n)\n\n# Create a dot plot of the top enriched GO terms\n\ndotplot &lt;- dotplot(\n  go_enrich,\n  title = \"Enrichment analysis dotplot\",\n  font.size = 8\n)\n\n# Combine the bar plot and dot plot into a single plot grid\n\ngo_plot &lt;- plot_grid(barplot, dotplot, col = 2)\n\n# Save the plot\n\nggsave(\"de_results/go_plot.png\", plot = go_plot, width = 13, height = 6, dpi = 300)",
    "crumbs": [
      "Spring 2025",
      "RNA-Seq Data Analysis in R | From Counts to Biological Insights"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html",
    "href": "2025_spring/250507_taires.html",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "Last change\n\n\n\n\n\nMay 7, 2025\n\n\n\nThis workshop will provide an overview of microbiome data analysis from Illumina amplicon sequencing raw data to data processing, visualization and statistics. Participants will learn basic concepts and tools to preprocess (QIIME2) and analyse (MicrobiomeAnalyst) microbiome data, in particular bacteria associated with marine organisms.\nIn a coral associated microbiome dataset, where we have two different coral species (with one incipient speciation) and two different reproductive strategies, we want to address the following scientific questions:\n\nIs the microbiome of these cold-water corals species-specific?\nIs the microbiome related to the reproductive strategy?\nOr is the microbiome shaped by both factors?\n\n\n\nThis tutorial requires you to be able to run QIIME2. We provide you with access to redi server, where it is preinstalled. However if you want to set it up yourself, the full instructions are here. Beware that below is UNIX specific setup. We did these parts:\n\n\n\nDownload installer from here\nYour downloaded file is executable (.exe on Win, .pkg on Mac and .sh on Linux). Run the install\n\n\n\n\n# update your conda\nconda update conda\n\n# create conda environment for amplicon qiime2\nconda env create -n qiime2-amplicon-2024.10 --file https://data.qiime2.org/distro/amplicon/qiime2-amplicon-2024.10-py310-linux-conda.yml\n\n\n\n\nIn the repository, you can find almost all the necessary input files, including pdf version of Tania’s introductory presentation.\nTo facilitate download, follow this link for bulk download. This link will expire on 06/06/2025, after that you need to download files to your local mahcine one by one from Github. Or better, setup yourself a Gihub account and you can clone the whole biohap repository.\n\n\nTo get the data tables from the server to your PC or opposite, ssh protocol exists.\n\nLinux: should be part of the core installation\nPutty on Windows, or ssh client\nMac: part of the install\n\nOnce you have ssh setup, do ssh &lt;USER&gt;@10.36.5.158, and then enter your password. You are the user connecting(@) to the server (10.36.5.158, redi).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you are not at the University internet (eduroam or fixed), first you will need to connect to it via UAlg VPN service.\n\n\n\nIn your /home/&lt;USER&gt; directory (you can see the location using pwd command), create a new folder\nmkdir workshop_qiime2\n\n# and navigate to it\ncd workshop_qiime2\nTo copy the files to the server, instead of standard cp we need to use secure scp\n# scp from to, -r means with the subfolder structure (recursive)\nscp -r \"&lt;path-to-your-downloads-perhaps&gt;\\Samples\"  &lt;USER&gt;@10.36.5.158:/home/&lt;USER&gt;/workshop_qiime2/\nOther files can be transferred in the similar manner.\n\n\n\nFirst look at your manifest file, which specifies sample ids, and reads name files, in this case we use paired ends, two file_paths per line are present.\ncat Manifest_file_Species.txt\nThe path needs to be changed to conform with your file system location. You see weird ^M characters, let’s remove those\n\nEither use dos2unix Manifest_file_Species.txt\nOr replace them using sed\n\n# \\r/ refers to Windows line ending\nsed -i 's/\\r//g' Manifest_file_Species.txt\nRun cat command again to confirm successful substitution. Select part of the path you want to replace next.\n\nSelect with the cursor the path (this automatically enters into your clipboard), or right mouse click and select copy\nprobably something like /Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/\n\nNow we substitute the string with correct path\n\n\n\n\n\n\nTip\n\n\n\n\nMiddle mouse button click pastes your mouse selection, ie clipboard on the command line\nAlternatively also right click and select paste.\nYou can get your correct path using pwd to print current folder path (assuming you are where your Samples folder is)\n\n\n\nsed -i 's|/Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/|&lt;YOUR_PATH_TO_FOLDER_Samples&gt;|g' Manifest_file_Species.txt\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that since our strings contained / character we used | to separate the parts of the sed command. That was not necessary in the case above s/\\r//g where / plays a role of the separator\nsed command s: substitute our flag g means replace all occurance on the line.\ns/pattern/subtitution/flags\n\n\n\nConfirm again using cat that your manifest file is correct.\n\n\n\n\nQIIME2 is installed in a sort of container (conda environment), which allows you to work on different project which need different dependencies in parallel. First confirm that the correct conda environment is activated\nconda activate qiime2-amplicon-2024.10\nYour command line should look similar to (qiime2-amplicon-2024.10) [XXXX@redi ~]$ now.\n\n\nqiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path /opt/shared/Manifest_file_Species.txt \\\n    --output-path demux.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\ntype describes that we have paired ends with Q scores (fastq) inputs. input-path is our manifest file, describing where all the files are. demux.qza will be our output-path file and input-format describes your sequencing technology, PairedEndFastq means that your data was sequenced with Forward and Reverse primers and you have two fastq files per sample that should be joined. Phred33 refers to the type of Phred scores in your fastq file. This is the lastest version and used for most recent Illumina sequencers. V2 is the version of the manifest file format you are using.\n\n\n\nqiime demux summarize --i-data demux.qza --o-visualization demux.qzv\n\n--i-data: input, which is output of importing data previously\n--o-visualization: output filename\n\n\n\n\nEvery time you create .qzv table, it is to be visualized at QIIME2 webpage for quality control and taking decisions for next steps. Open QIIME2 webpage and drag the qzv files in there. Use the ssh as described above to get the data to your PC.\nscp &lt;USER&gt;@10.36.5.158:/home/&lt;USER&gt;/workshop_qiime2/demux.qzv \"absolute-path-on-your-PC-to-folder-you-want-your-file\"\n\n\n\ndada2 is an R package, which will be used here. This step can take substantial time and resource.\nqiime dada2 denoise-paired \\\n    --i-demultiplexed-seqs demux.qza \\\n    --p-trim-left-f x \\\n    --p-trim-left-r y \\\n    --p-trunc-len-f z \\\n    --p-trunc-len-r n \\\n    --p-n-threads 0 \\\n    --o-table table.qza \\\n    --o-representative-sequences rep-seqs.qza \\\n    --o-denoising-stats denoising-stats.qza \\\n    --verbose\nTODO: explain the input params\n\n--p-trim-left-f: xxx (see demux.qzv file)\n--p-trim-left-r: xxx (see demux.qzv file)\n--p-trunc-len-f: xxx (see demux.qzv file)\n--p-trunc-len-r: xxx (see demux.qzv file)\n--p-n-threads 0: xxx (the number of CPUs threads the tool should use during processing, zero means “use all the threads available”)\n--o-table: output table filename\n--o-representative-sequences: representative sequences file name\n--o-denoising-stats: statistics filename specification\n--verbose: write extensive progress on the screen (since this script does several things it is advisable to run it so we can follow the flow)\n\n\n\n\n\n\n\nSample output on the screen\n\n\n\n\n\nR version 4.3.3 (2024-02-29) Loading required package: Rcpp DADA2: 1.30.0 / Rcpp: 1.0.13.1 / RcppParallel: 5.1.9 2) Filtering …………………… 3) Learning Error Rates 208608800 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 219039240 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 3) Denoise samples …………………… …………………… 5) Remove chimeras (method = consensus) 6) Report read numbers through the pipeline 7) Write output Saved FeatureTable[Frequency] to: table.qza Saved FeatureData[Sequence] to: rep-seqs.qza Saved SampleData[DADA2Stats] to: denoising-stats.qza\n\n\n\n\n\n\nqiime metadata tabulate \\\n    --m-input-file denoising-stats.qza \\\n    --o-visualization denoising-stats.qzv\nArguments specify input and output files. Now we summarize …?\nqiime feature-table summarize \\\n    --i-table table.qza \\\n    --o-visualization table.qzv\nAnd finally tabulate the sequences.\nqiime feature-table tabulate-seqs \\\n    --i-data rep-seqs.qza \\\n    --o-visualization rep-seqs.qzv\nUse your scp skills again to transfer denoising-stats.qzv, rep-seqs.qzv and table.qzv to your PC. Hint, it is better to do it not from redi but from your PC’s command line.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nscp &lt;USER&gt;@10.36.5.158:/home/&lt;USER&gt;/workshop_qiime2/*.qzv \"path-to-destination-folder\"\n\n\n\n\n\n\nThe database is now in the repo because of its size, download it. If you work on redi, the database is located at &lt;fill in path DP&gt;. This is also a time-consuming step\nThe classifier needs to match our scikit-learn package version used by the QIIME2 version. At redi, the version is 1.4.2, and we have already put in the shared location path /opt/shared/silva-138-99-nb-classifier.qza.\nIn other use case, download the classfier here. If you use your own classifier, the below specifications of the classifier might change.\nqiime feature-classifier classify-sklearn \\\n    --i-classifier /opt/shared/silva-138-99-nb-classifier.qza \\\n    --i-reads rep-seqs.qza \\\n    --o-classification taxonomy.qza\n\n\n\nqiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n\n\n\nqiime tools export \\\n    --input-path table.qza \\\n    --output-path feature-table\n\n\n\nqiime tools export \\\n    --input-path taxonomy.qza \\\n    --output-path taxonomy\nTo move further with the analysis, open the taxonomy file and change the header. When you open it, you’ll see the header looks like this:\nFeature ID Taxon   Confidence\nYou need to change it to this:\n#otu-id    taxonomy    Confidence\n\n\n\n\n\n\nImportant\n\n\n\nALL spaces are tabs\n\n\nCan you figure out the sed command do do that?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nsed -i '1s/.*/#otu-id\\ttaxonomy\\tConfidence/' ~/the_name_of_the_folder_where_you_are_working/taxonomy/taxonomy.tsv\n\n\n\n\n\n\nbiom add-metadata \\\n    --input-fp feature-table/feature-table.biom \\\n    --observation-metadata-fp taxonomy/taxonomy.tsv \\\n    --output-fp biom-with-taxonomy.biom\n\n\n\nbiom convert \\\n    --input-fp biom-with-taxonomy.biom \\\n    --output-fp ASV_table.tsv \\\n    --to-tsv --header-key taxonomy\n\n\n\n\nUnfortunately the ASV table is not directly compatible with the MA. We need to split it into separate abundance and taxonomy tables. In the GH repo, there are two scripts to do that without going into the details. Both have hardcoded input ASV table name ASV_table.tsv which you need to conform too. Outputs will be ASV_table_MA.txt and taxonomy_MA.txt, respectively.\n\n\n\n\n\n\nPermissions\n\n\n\n\n\nRunning the shell script requires that executable (x) permissions for the file, which probably after scp is not the case. To enable execution, run:\nchmod +x convert_ASV_abundances.sh\nchmod +x convert_ASV_taxonomy.sh\nwith ls, the executable files should appear in green. The scripts accept one and one argument only which points to the ASV_table.tsv generated by QIIME2. Below it assumes your in the directory where the ASV table resides, otherwise include absolute or relative path.\n\n\n\n./convert_ASV_abundances.sh ASV_table.tsv\n./convert_ASV_taxonomy.sh  ASV_table.tsv\nTogether with the provided metadata_MA.txt, which assigns each sample to a coral species, those are the three files you need to scp to your local machine for MA web interface upload. MA can do too many things for you, the subset we will use is described in Tania’s presentation available on GH too.\nThe fun starts here, Click here to start the session and select Marker data profiling.\n\n\n\nMA inputs\n\n\nYou need to select the first three input files (OTU/ASV table, Metadata file, Taxonomy table), and from the dropdown, choose Taxonomy labels to be QIIME.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#setup",
    "href": "2025_spring/250507_taires.html#setup",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "This tutorial requires you to be able to run QIIME2. We provide you with access to redi server, where it is preinstalled. However if you want to set it up yourself, the full instructions are here. Beware that below is UNIX specific setup. We did these parts:\n\n\n\nDownload installer from here\nYour downloaded file is executable (.exe on Win, .pkg on Mac and .sh on Linux). Run the install\n\n\n\n\n# update your conda\nconda update conda\n\n# create conda environment for amplicon qiime2\nconda env create -n qiime2-amplicon-2024.10 --file https://data.qiime2.org/distro/amplicon/qiime2-amplicon-2024.10-py310-linux-conda.yml",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#input-files",
    "href": "2025_spring/250507_taires.html#input-files",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "In the repository, you can find almost all the necessary input files, including pdf version of Tania’s introductory presentation.\nTo facilitate download, follow this link for bulk download. This link will expire on 06/06/2025, after that you need to download files to your local mahcine one by one from Github. Or better, setup yourself a Gihub account and you can clone the whole biohap repository.\n\n\nTo get the data tables from the server to your PC or opposite, ssh protocol exists.\n\nLinux: should be part of the core installation\nPutty on Windows, or ssh client\nMac: part of the install\n\nOnce you have ssh setup, do ssh &lt;USER&gt;@10.36.5.158, and then enter your password. You are the user connecting(@) to the server (10.36.5.158, redi).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you are not at the University internet (eduroam or fixed), first you will need to connect to it via UAlg VPN service.\n\n\n\nIn your /home/&lt;USER&gt; directory (you can see the location using pwd command), create a new folder\nmkdir workshop_qiime2\n\n# and navigate to it\ncd workshop_qiime2\nTo copy the files to the server, instead of standard cp we need to use secure scp\n# scp from to, -r means with the subfolder structure (recursive)\nscp -r \"&lt;path-to-your-downloads-perhaps&gt;\\Samples\"  &lt;USER&gt;@10.36.5.158:/home/&lt;USER&gt;/workshop_qiime2/\nOther files can be transferred in the similar manner.\n\n\n\nFirst look at your manifest file, which specifies sample ids, and reads name files, in this case we use paired ends, two file_paths per line are present.\ncat Manifest_file_Species.txt\nThe path needs to be changed to conform with your file system location. You see weird ^M characters, let’s remove those\n\nEither use dos2unix Manifest_file_Species.txt\nOr replace them using sed\n\n# \\r/ refers to Windows line ending\nsed -i 's/\\r//g' Manifest_file_Species.txt\nRun cat command again to confirm successful substitution. Select part of the path you want to replace next.\n\nSelect with the cursor the path (this automatically enters into your clipboard), or right mouse click and select copy\nprobably something like /Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/\n\nNow we substitute the string with correct path\n\n\n\n\n\n\nTip\n\n\n\n\nMiddle mouse button click pastes your mouse selection, ie clipboard on the command line\nAlternatively also right click and select paste.\nYou can get your correct path using pwd to print current folder path (assuming you are where your Samples folder is)\n\n\n\nsed -i 's|/Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/|&lt;YOUR_PATH_TO_FOLDER_Samples&gt;|g' Manifest_file_Species.txt\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that since our strings contained / character we used | to separate the parts of the sed command. That was not necessary in the case above s/\\r//g where / plays a role of the separator\nsed command s: substitute our flag g means replace all occurance on the line.\ns/pattern/subtitution/flags\n\n\n\nConfirm again using cat that your manifest file is correct.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#qiime2",
    "href": "2025_spring/250507_taires.html#qiime2",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "QIIME2 is installed in a sort of container (conda environment), which allows you to work on different project which need different dependencies in parallel. First confirm that the correct conda environment is activated\nconda activate qiime2-amplicon-2024.10\nYour command line should look similar to (qiime2-amplicon-2024.10) [XXXX@redi ~]$ now.\n\n\nqiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path /opt/shared/Manifest_file_Species.txt \\\n    --output-path demux.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\ntype describes that we have paired ends with Q scores (fastq) inputs. input-path is our manifest file, describing where all the files are. demux.qza will be our output-path file and input-format describes your sequencing technology, PairedEndFastq means that your data was sequenced with Forward and Reverse primers and you have two fastq files per sample that should be joined. Phred33 refers to the type of Phred scores in your fastq file. This is the lastest version and used for most recent Illumina sequencers. V2 is the version of the manifest file format you are using.\n\n\n\nqiime demux summarize --i-data demux.qza --o-visualization demux.qzv\n\n--i-data: input, which is output of importing data previously\n--o-visualization: output filename\n\n\n\n\nEvery time you create .qzv table, it is to be visualized at QIIME2 webpage for quality control and taking decisions for next steps. Open QIIME2 webpage and drag the qzv files in there. Use the ssh as described above to get the data to your PC.\nscp &lt;USER&gt;@10.36.5.158:/home/&lt;USER&gt;/workshop_qiime2/demux.qzv \"absolute-path-on-your-PC-to-folder-you-want-your-file\"\n\n\n\ndada2 is an R package, which will be used here. This step can take substantial time and resource.\nqiime dada2 denoise-paired \\\n    --i-demultiplexed-seqs demux.qza \\\n    --p-trim-left-f x \\\n    --p-trim-left-r y \\\n    --p-trunc-len-f z \\\n    --p-trunc-len-r n \\\n    --p-n-threads 0 \\\n    --o-table table.qza \\\n    --o-representative-sequences rep-seqs.qza \\\n    --o-denoising-stats denoising-stats.qza \\\n    --verbose\nTODO: explain the input params\n\n--p-trim-left-f: xxx (see demux.qzv file)\n--p-trim-left-r: xxx (see demux.qzv file)\n--p-trunc-len-f: xxx (see demux.qzv file)\n--p-trunc-len-r: xxx (see demux.qzv file)\n--p-n-threads 0: xxx (the number of CPUs threads the tool should use during processing, zero means “use all the threads available”)\n--o-table: output table filename\n--o-representative-sequences: representative sequences file name\n--o-denoising-stats: statistics filename specification\n--verbose: write extensive progress on the screen (since this script does several things it is advisable to run it so we can follow the flow)\n\n\n\n\n\n\n\nSample output on the screen\n\n\n\n\n\nR version 4.3.3 (2024-02-29) Loading required package: Rcpp DADA2: 1.30.0 / Rcpp: 1.0.13.1 / RcppParallel: 5.1.9 2) Filtering …………………… 3) Learning Error Rates 208608800 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 219039240 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 3) Denoise samples …………………… …………………… 5) Remove chimeras (method = consensus) 6) Report read numbers through the pipeline 7) Write output Saved FeatureTable[Frequency] to: table.qza Saved FeatureData[Sequence] to: rep-seqs.qza Saved SampleData[DADA2Stats] to: denoising-stats.qza\n\n\n\n\n\n\nqiime metadata tabulate \\\n    --m-input-file denoising-stats.qza \\\n    --o-visualization denoising-stats.qzv\nArguments specify input and output files. Now we summarize …?\nqiime feature-table summarize \\\n    --i-table table.qza \\\n    --o-visualization table.qzv\nAnd finally tabulate the sequences.\nqiime feature-table tabulate-seqs \\\n    --i-data rep-seqs.qza \\\n    --o-visualization rep-seqs.qzv\nUse your scp skills again to transfer denoising-stats.qzv, rep-seqs.qzv and table.qzv to your PC. Hint, it is better to do it not from redi but from your PC’s command line.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nscp &lt;USER&gt;@10.36.5.158:/home/&lt;USER&gt;/workshop_qiime2/*.qzv \"path-to-destination-folder\"\n\n\n\n\n\n\nThe database is now in the repo because of its size, download it. If you work on redi, the database is located at &lt;fill in path DP&gt;. This is also a time-consuming step\nThe classifier needs to match our scikit-learn package version used by the QIIME2 version. At redi, the version is 1.4.2, and we have already put in the shared location path /opt/shared/silva-138-99-nb-classifier.qza.\nIn other use case, download the classfier here. If you use your own classifier, the below specifications of the classifier might change.\nqiime feature-classifier classify-sklearn \\\n    --i-classifier /opt/shared/silva-138-99-nb-classifier.qza \\\n    --i-reads rep-seqs.qza \\\n    --o-classification taxonomy.qza\n\n\n\nqiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n\n\n\nqiime tools export \\\n    --input-path table.qza \\\n    --output-path feature-table\n\n\n\nqiime tools export \\\n    --input-path taxonomy.qza \\\n    --output-path taxonomy\nTo move further with the analysis, open the taxonomy file and change the header. When you open it, you’ll see the header looks like this:\nFeature ID Taxon   Confidence\nYou need to change it to this:\n#otu-id    taxonomy    Confidence\n\n\n\n\n\n\nImportant\n\n\n\nALL spaces are tabs\n\n\nCan you figure out the sed command do do that?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nsed -i '1s/.*/#otu-id\\ttaxonomy\\tConfidence/' ~/the_name_of_the_folder_where_you_are_working/taxonomy/taxonomy.tsv\n\n\n\n\n\n\nbiom add-metadata \\\n    --input-fp feature-table/feature-table.biom \\\n    --observation-metadata-fp taxonomy/taxonomy.tsv \\\n    --output-fp biom-with-taxonomy.biom\n\n\n\nbiom convert \\\n    --input-fp biom-with-taxonomy.biom \\\n    --output-fp ASV_table.tsv \\\n    --to-tsv --header-key taxonomy",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#microbiomeanalyst-ma",
    "href": "2025_spring/250507_taires.html#microbiomeanalyst-ma",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "Unfortunately the ASV table is not directly compatible with the MA. We need to split it into separate abundance and taxonomy tables. In the GH repo, there are two scripts to do that without going into the details. Both have hardcoded input ASV table name ASV_table.tsv which you need to conform too. Outputs will be ASV_table_MA.txt and taxonomy_MA.txt, respectively.\n\n\n\n\n\n\nPermissions\n\n\n\n\n\nRunning the shell script requires that executable (x) permissions for the file, which probably after scp is not the case. To enable execution, run:\nchmod +x convert_ASV_abundances.sh\nchmod +x convert_ASV_taxonomy.sh\nwith ls, the executable files should appear in green. The scripts accept one and one argument only which points to the ASV_table.tsv generated by QIIME2. Below it assumes your in the directory where the ASV table resides, otherwise include absolute or relative path.\n\n\n\n./convert_ASV_abundances.sh ASV_table.tsv\n./convert_ASV_taxonomy.sh  ASV_table.tsv\nTogether with the provided metadata_MA.txt, which assigns each sample to a coral species, those are the three files you need to scp to your local machine for MA web interface upload. MA can do too many things for you, the subset we will use is described in Tania’s presentation available on GH too.\nThe fun starts here, Click here to start the session and select Marker data profiling.\n\n\n\nMA inputs\n\n\nYou need to select the first three input files (OTU/ASV table, Metadata file, Taxonomy table), and from the dropdown, choose Taxonomy labels to be QIIME.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  }
]