[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOinformatics: Handy and Practical",
    "section": "",
    "text": "BIOinformatics: Handy and Practical\nThis is a compilation of bioinformatics knowledge at CCMAR-Algarve. First series of hands-on workshop happened in Spring 2025.\nUntil this day, three hands-on workshops designed to equip researchers with essential data analysis skills for bioinformatics have been announced. Every month, you will get served a workshop which cover a key aspect of biological data analysis, from RNA-seq to microbiome analysis, helping you turn raw sequence files into meaningful biological insights.\nTo visit the source files for the workshops and the webpage, visit our Github. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Introduction",
      "BIOinformatics: Handy and Practical"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html",
    "href": "2025_spring/250507_taires.html",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "This workshop will provide an overview of microbiome data analysis from Illumina amplicon sequencing raw data to data processing, visualization and statistics. Participants will learn basic concepts and tools to preprocess (QIIME2) and analyse (MicrobiomeAnalyst) microbiome data, in particular bacteria associated with marine organisms.\n\n\nThis tutorial requires you to be able to run QIIME2. We provide you with access to redi server, where it is preinstalled. However if you want to set it up yourself, the full instructions are here. Beware that below is UNIX specific setup. We did these parts:\n\n\n\nDownload installer from here\nYour downloaded file is executable (.exe on Win, .pkg on Mac and .sh on Linux). Run the install\n\n\n\n\n# update your conda\nconda update conda\n\n# create conda environment for amplicon qiime2\nconda env create -n qiime2-amplicon-2024.10 --file https://data.qiime2.org/distro/amplicon/qiime2-amplicon-2024.10-py310-linux-conda.yml\n\n\n\n\nIn the repository, you can find almost all the necessary input files.\nFirst look at your manifest file, which specifies sample ids, and reads name files, in this case we use paired ends, to two file_paths per line are present. The path needs to be changed to conform with your file system location\n\n\nTo get the data tables from the server to your PC or opposite, use ssh protocol.\n\nLinux: should be part of the core installation\nPutty on Windows, or ssh client\nMac: part of the install\n\nOnce you have ssh setup, do ssh &lt;USER&gt;@10.36.5.158, and then enter your password. You are the user connecting(@) to the server (10.36.5.158, redi).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you are not at the University internet (eduroam or fixed), first you will need to connect to it via UAlg VPN service.\n\n\n\n\n\n\n# close vi editor with Esc, then :q\nvi Manifest_file_Species.txt\nYou see weird ^M characters, let’s remove those\n\nEither use dos2unix Manifest_file_Species.txt\nOr replace them using sed\n\n# \\r/ refers to Windows line ending\nsed -i 's/\\r//g' Manifest_file_Species.txt\nRun vi again to confirm successful replacement and also to select copy part of the path you want to replace next.\n\nSelect with the cursor the path (this automatically enter into your clipboard)\nprobably something like /Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/\nexit the vi editor again Esc and :q\n\nReplace the string with correct path\n\n\n\n\n\n\nTip\n\n\n\n\nMiddle mouse button click pastes your mouse selection, ie clipboard on the command line\nYou can get your correct path using pwd to print current folder path (assuming you are where you Samples folder is)\n\n\n\nsed -i 's|/Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/|&lt;YOUR_PATH_TO_FOLDER_Samples&gt;|g' Manifest_file_Species.txt\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that since our strings contained / character we used | to separate the parts of the sed command. That was not necessary in the case above s/\\r//g where / plays a role of the separator\nsed command s: substitute our flag g means replace all occurance on the line.\ns/pattern/replacement/flags\n\n\n\nConfirm again in the vi editor the correct manifest file.\n\n\n\n\n\n\nqiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path Manifest_file_Species.txt \\\n    --output-path demux.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\ntype describes that we have paired ends with Q scores (fastq) inputs. input-path is our manifest file, describing where all the files are. demux.qza will be our output-path file and input-format describes your sequencing technology, PairedEndFastq means that your data was sequenced with Forward and Reverse primers and you have two fastq files per sample that should be joined. Phred33 refers to the type of Phred scores in your fastq file. This is the lastest version and used for most recent Illumina sequencers. V2 is the version of the manifest file format you are using.\n\n\n\nqiime demux summarize --i-data demux.qza --o-visualization demux.qzv\n\n--i-data: input, which is output of importing data previously\n--o-visualization: output filename\n\n\n\n\nEvery time you create .qzv table, it is to be visualized at QIIME2 webpage for quality control and taking decisions for next steps. Open QIIME2 webpage and drag the qzv files in there. Use the ssh as described above to get the data to your PC.\n\n\n\ndada2 is an R package, which will be used here. This step can take substantial time and resource.\nqiime dada2 denoise-paired \\\n    --i-demultiplexed-seqs demux.qza \\\n    --p-trim-left-f x \\\n    --p-trim-left-r y \\\n    --p-trunc-len-f z \\\n    --p-trunc-len-r n \\\n    --p-n-threads 0 \\\n    --o-table table.qza \\\n    --o-representative-sequences rep-seqs.qza \\\n    --o-denoising-stats denoising-stats.qza \\\n    --verbose\nTODO: explain the input params\n\n--p-trim-left-f: xxx (see demux.qzv file)\n--p-trim-left-r: xxx (see demux.qzv file)\n--p-trunc-len-f: xxx (see demux.qzv file)\n--p-trunc-len-r: xxx (see demux.qzv file)\n--p-n-threads 0: xxx (the number of CPUs threads the tool should use during processing, zero means “use all the threads available”)\n--o-table: output table filename\n--o-representative-sequences: representative sequences file name\n--o-denoising-stats: statistics filename specification\n--verbose: write extensive progress on the screen (since this script does several things it is advisable to run it so we can follow the flow)\n\n\n\n\n\n\n\nSample output on the screen\n\n\n\n\n\nR version 4.3.3 (2024-02-29) Loading required package: Rcpp DADA2: 1.30.0 / Rcpp: 1.0.13.1 / RcppParallel: 5.1.9 2) Filtering …………………… 3) Learning Error Rates 208608800 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 219039240 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 3) Denoise samples …………………… …………………… 5) Remove chimeras (method = consensus) 6) Report read numbers through the pipeline 7) Write output Saved FeatureTable[Frequency] to: table.qza Saved FeatureData[Sequence] to: rep-seqs.qza Saved SampleData[DADA2Stats] to: denoising-stats.qza\n\n\n\n\n\n\nqiime metadata tabulate \\\n    --m-input-file denoising-stats.qza \\\n    --o-visualization denoising-stats.qzv\nArguments specify input and output files. Now we summarize …?\nqiime feature-table summarize \\\n    --i-table table.qza \\\n    --o-visualization table.qzv\nAnd finally tabulate the sequences.\nqiime feature-table tabulate-seqs \\\n    --i-data rep-seqs.qza \\\n    --o-visualization rep-seqs.qzv\n\n\n\nThe database is now in the repo because of its size, download it. If you work on redi, the database is located at &lt;fill in path DP&gt;. This is also a time-consuming step\nThe classifier needs to match our scikit-learn package version used by the QIIME2 version. Here the version is 1.4.2, download the classfier here\nqiime feature-classifier classify-sklearn \\\n    --i-classifier silva-138-99-nb-classifier.qza \\\n    --i-reads rep-seqs.qza \\\n    --o-classification taxonomy.qza\n\n\n\nqiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n\n\n\nqiime tools export \\\n    --input-path table.qza \\\n    --output-path feature-table\n\n\n\nqiime tools export \\\n    --input-path taxonomy.qza \\\n    --output-path taxonomy\nTo move further with the analysis, open the taxonomy file and change the header. When you open it, you’ll see the header looks like this:\nFeature ID Taxon   Confidence\nYou need to change it to this:\n#otu-id    taxonomy    Confidence\n\n\n\n\n\n\nImportant\n\n\n\nALL spaces are tabs\n\n\nCan you figure out the sed command do do that?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nsed -i '0,/^[^\\t]*\\t[^\\t]*/s/#otu-id\\ttaxonomy\\tConfidence/' taxonomy.tsv\n\n\n\n\n\n\nbiom add-metadata \\\n    --input-fp feature-table/feature-table.biom \\\n    --observation-metadata-fp taxonomy/taxonomy.tsv \\\n    --output-fp biom-with-taxonomy.biom\n\n\n\nbiom convert \\\n    --input-fp biom-with-taxonomy.biom \\\n    --output-fp ASV_table.tsv \\\n    --to-tsv --header-key taxonomy\n\n\n\n\nUnfortunately the ASV table is not directly compatible with the MA. We need to split it into separate abundance and taxonomy tables. In the GH repo are two scripts to do that without going into the details. Both have hardcoded input ASV table name ASV_table_original.txt which you need to conform too. Outputs will be ASV_table.tsv and taxonomy.tsv, respectively.\n./convert_ASV_abundances.sh\n./convert_ASV_taxonomy.sh\nMA can do too many things to cover here, the subset we will use is described in Tania’s presentation available on GH too.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#setup",
    "href": "2025_spring/250507_taires.html#setup",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "This tutorial requires you to be able to run QIIME2. We provide you with access to redi server, where it is preinstalled. However if you want to set it up yourself, the full instructions are here. Beware that below is UNIX specific setup. We did these parts:\n\n\n\nDownload installer from here\nYour downloaded file is executable (.exe on Win, .pkg on Mac and .sh on Linux). Run the install\n\n\n\n\n# update your conda\nconda update conda\n\n# create conda environment for amplicon qiime2\nconda env create -n qiime2-amplicon-2024.10 --file https://data.qiime2.org/distro/amplicon/qiime2-amplicon-2024.10-py310-linux-conda.yml",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#input-files",
    "href": "2025_spring/250507_taires.html#input-files",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "In the repository, you can find almost all the necessary input files.\nFirst look at your manifest file, which specifies sample ids, and reads name files, in this case we use paired ends, to two file_paths per line are present. The path needs to be changed to conform with your file system location\n\n\nTo get the data tables from the server to your PC or opposite, use ssh protocol.\n\nLinux: should be part of the core installation\nPutty on Windows, or ssh client\nMac: part of the install\n\nOnce you have ssh setup, do ssh &lt;USER&gt;@10.36.5.158, and then enter your password. You are the user connecting(@) to the server (10.36.5.158, redi).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you are not at the University internet (eduroam or fixed), first you will need to connect to it via UAlg VPN service.\n\n\n\n\n\n\n# close vi editor with Esc, then :q\nvi Manifest_file_Species.txt\nYou see weird ^M characters, let’s remove those\n\nEither use dos2unix Manifest_file_Species.txt\nOr replace them using sed\n\n# \\r/ refers to Windows line ending\nsed -i 's/\\r//g' Manifest_file_Species.txt\nRun vi again to confirm successful replacement and also to select copy part of the path you want to replace next.\n\nSelect with the cursor the path (this automatically enter into your clipboard)\nprobably something like /Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/\nexit the vi editor again Esc and :q\n\nReplace the string with correct path\n\n\n\n\n\n\nTip\n\n\n\n\nMiddle mouse button click pastes your mouse selection, ie clipboard on the command line\nYou can get your correct path using pwd to print current folder path (assuming you are where you Samples folder is)\n\n\n\nsed -i 's|/Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/|&lt;YOUR_PATH_TO_FOLDER_Samples&gt;|g' Manifest_file_Species.txt\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that since our strings contained / character we used | to separate the parts of the sed command. That was not necessary in the case above s/\\r//g where / plays a role of the separator\nsed command s: substitute our flag g means replace all occurance on the line.\ns/pattern/replacement/flags\n\n\n\nConfirm again in the vi editor the correct manifest file.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#qiime2",
    "href": "2025_spring/250507_taires.html#qiime2",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "qiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path Manifest_file_Species.txt \\\n    --output-path demux.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\ntype describes that we have paired ends with Q scores (fastq) inputs. input-path is our manifest file, describing where all the files are. demux.qza will be our output-path file and input-format describes your sequencing technology, PairedEndFastq means that your data was sequenced with Forward and Reverse primers and you have two fastq files per sample that should be joined. Phred33 refers to the type of Phred scores in your fastq file. This is the lastest version and used for most recent Illumina sequencers. V2 is the version of the manifest file format you are using.\n\n\n\nqiime demux summarize --i-data demux.qza --o-visualization demux.qzv\n\n--i-data: input, which is output of importing data previously\n--o-visualization: output filename\n\n\n\n\nEvery time you create .qzv table, it is to be visualized at QIIME2 webpage for quality control and taking decisions for next steps. Open QIIME2 webpage and drag the qzv files in there. Use the ssh as described above to get the data to your PC.\n\n\n\ndada2 is an R package, which will be used here. This step can take substantial time and resource.\nqiime dada2 denoise-paired \\\n    --i-demultiplexed-seqs demux.qza \\\n    --p-trim-left-f x \\\n    --p-trim-left-r y \\\n    --p-trunc-len-f z \\\n    --p-trunc-len-r n \\\n    --p-n-threads 0 \\\n    --o-table table.qza \\\n    --o-representative-sequences rep-seqs.qza \\\n    --o-denoising-stats denoising-stats.qza \\\n    --verbose\nTODO: explain the input params\n\n--p-trim-left-f: xxx (see demux.qzv file)\n--p-trim-left-r: xxx (see demux.qzv file)\n--p-trunc-len-f: xxx (see demux.qzv file)\n--p-trunc-len-r: xxx (see demux.qzv file)\n--p-n-threads 0: xxx (the number of CPUs threads the tool should use during processing, zero means “use all the threads available”)\n--o-table: output table filename\n--o-representative-sequences: representative sequences file name\n--o-denoising-stats: statistics filename specification\n--verbose: write extensive progress on the screen (since this script does several things it is advisable to run it so we can follow the flow)\n\n\n\n\n\n\n\nSample output on the screen\n\n\n\n\n\nR version 4.3.3 (2024-02-29) Loading required package: Rcpp DADA2: 1.30.0 / Rcpp: 1.0.13.1 / RcppParallel: 5.1.9 2) Filtering …………………… 3) Learning Error Rates 208608800 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 219039240 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 3) Denoise samples …………………… …………………… 5) Remove chimeras (method = consensus) 6) Report read numbers through the pipeline 7) Write output Saved FeatureTable[Frequency] to: table.qza Saved FeatureData[Sequence] to: rep-seqs.qza Saved SampleData[DADA2Stats] to: denoising-stats.qza\n\n\n\n\n\n\nqiime metadata tabulate \\\n    --m-input-file denoising-stats.qza \\\n    --o-visualization denoising-stats.qzv\nArguments specify input and output files. Now we summarize …?\nqiime feature-table summarize \\\n    --i-table table.qza \\\n    --o-visualization table.qzv\nAnd finally tabulate the sequences.\nqiime feature-table tabulate-seqs \\\n    --i-data rep-seqs.qza \\\n    --o-visualization rep-seqs.qzv\n\n\n\nThe database is now in the repo because of its size, download it. If you work on redi, the database is located at &lt;fill in path DP&gt;. This is also a time-consuming step\nThe classifier needs to match our scikit-learn package version used by the QIIME2 version. Here the version is 1.4.2, download the classfier here\nqiime feature-classifier classify-sklearn \\\n    --i-classifier silva-138-99-nb-classifier.qza \\\n    --i-reads rep-seqs.qza \\\n    --o-classification taxonomy.qza\n\n\n\nqiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n\n\n\nqiime tools export \\\n    --input-path table.qza \\\n    --output-path feature-table\n\n\n\nqiime tools export \\\n    --input-path taxonomy.qza \\\n    --output-path taxonomy\nTo move further with the analysis, open the taxonomy file and change the header. When you open it, you’ll see the header looks like this:\nFeature ID Taxon   Confidence\nYou need to change it to this:\n#otu-id    taxonomy    Confidence\n\n\n\n\n\n\nImportant\n\n\n\nALL spaces are tabs\n\n\nCan you figure out the sed command do do that?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nsed -i '0,/^[^\\t]*\\t[^\\t]*/s/#otu-id\\ttaxonomy\\tConfidence/' taxonomy.tsv\n\n\n\n\n\n\nbiom add-metadata \\\n    --input-fp feature-table/feature-table.biom \\\n    --observation-metadata-fp taxonomy/taxonomy.tsv \\\n    --output-fp biom-with-taxonomy.biom\n\n\n\nbiom convert \\\n    --input-fp biom-with-taxonomy.biom \\\n    --output-fp ASV_table.tsv \\\n    --to-tsv --header-key taxonomy",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#microbiomeanalyst-ma",
    "href": "2025_spring/250507_taires.html#microbiomeanalyst-ma",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "Unfortunately the ASV table is not directly compatible with the MA. We need to split it into separate abundance and taxonomy tables. In the GH repo are two scripts to do that without going into the details. Both have hardcoded input ASV table name ASV_table_original.txt which you need to conform too. Outputs will be ASV_table.tsv and taxonomy.tsv, respectively.\n./convert_ASV_abundances.sh\n./convert_ASV_taxonomy.sh\nMA can do too many things to cover here, the subset we will use is described in Tania’s presentation available on GH too.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250604_iduarte.html",
    "href": "2025_spring/250604_iduarte.html",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "date: 04/06/2025, Work in progress",
    "crumbs": [
      "Spring 2025",
      "RNA-Seq Data Analysis in R - From Counts to Biological Insights"
    ]
  },
  {
    "objectID": "2025_spring/250604_iduarte.html#rna-seq-data-analysis-in-r---from-counts-to-biological-insights",
    "href": "2025_spring/250604_iduarte.html#rna-seq-data-analysis-in-r---from-counts-to-biological-insights",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "date: 04/06/2025, Work in progress",
    "crumbs": [
      "Spring 2025",
      "RNA-Seq Data Analysis in R - From Counts to Biological Insights"
    ]
  },
  {
    "objectID": "2025_spring/250604_iduarte.html#author-isabel-duarte-iduarteualg.pt",
    "href": "2025_spring/250604_iduarte.html#author-isabel-duarte-iduarteualg.pt",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "author: Isabel Duarte iduarte@ualg.pt",
    "text": "author: Isabel Duarte iduarte@ualg.pt\nIntroduction to RNA-seq data analysis using R, focusing on differential expression analysis and learning how to choose the right Generalized linear model (GLM) for your question and your data. Participants will learn essential concepts and basic workflow, from preprocessing raw count data to identifying differentially expressed genes.",
    "crumbs": [
      "Spring 2025",
      "RNA-Seq Data Analysis in R - From Counts to Biological Insights"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html",
    "href": "2025_spring/250416_atkacz.html",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Large files or datasets—especially those containing genomic data—no problem. This workshop introduces essential Linux commands and simple Bash scripts to streamline data manipulation tasks. We’ll cover key operations such as searching for patterns, globally modifying content, and aligning DNA sequences, such as FASTAQ and FASTA files.\n\n\nNo extra setup needed for Linux and MacOS machines. For windows the recommended way is to install VBox. See the steps to follow:\nHere are the instructions: Installing Ubuntu on VirtualBox\n\n\n\nRun the Installer:\nAfter downloading the VirtualBox installer (.exe file for WinOS, .dmg for MacOS), double-click on it to launch the setup wizard and follow default installation instructions.\nLaunch VirtualBox.\n\n\n\n\n\nOpen VirtualBox and Create a New VM: Click on the New button.\nName and Operating System: Give your VM a name (e.g., “Ubuntu”), select “Linux” as the type, and choose Ubuntu (64-bit) as the version.\nAssign Memory: Allocate at least 2048 MB (2 GB) of RAM (or higher if your system permits) to ensure smooth operation.\nCreate a Virtual Hard Disk:\n\nChoose Create a virtual hard disk now.\nSelect the VDI (VirtualBox Disk Image) format, and set the disk size (a minimum of 20 GB is recommended).\n\n\n\n\n\n\nAttach the Ubuntu ISO after downloading:\n\nSelect your new VM, click on Settings, then navigate to the Storage section.\nUnder the “Controller: IDE” (or SATA), click on the empty optical drive.\nClick the small disk icon on the right and choose Choose a disk file. Browse to and select the Ubuntu ISO you downloaded.\n\nStart the Virtual Machine:\n\nWith the ISO attached, click Start.\n\n\n\n\n\n\nNote\n\n\n\nYou might need to restart your machine\n\n\nThe VM will boot from the ISO, and you should see the Ubuntu welcome screen.\n\nInstall Ubuntu:\n\nFollow the on-screen instructions in the Ubuntu installer:\n\nChoose your language and keyboard layout.\nSelect Install Ubuntu.\nFollow prompts to set up your timezone, create a user account, and configure your disk (use the default settings for a virtual disk).\nOnce the installation is complete, you’ll be prompted to restart the VM.\n\n\n\n\n\n\n\n\n\n\nEventually you will need bigger compute than you can get on your machine and you will be forced to work on Cloud such as High Power cluster (HPC) which allow you to parallelize and run heavy computations.\nThings can be done faster on a command line with practice.\n\n\n\n\nOpen the terminal and let’s create a file called maria_joanna_song\ntouch maria_joanna_song\nlet’s just copy in this text - open the file with double click and paste this song into the maria_joanna_song file, also found here\nE virou!\nOnde anda essa Maria?\nEu só tenho essa Maria\nEu vim do norte direto a Lisboa\nAtrás de um sonho\nQue eu nem sei se voa\nTanto quanto nós voávamos\nDebaixo dos lencóis\nA francesinha já não tem picante\nE isso faz-me lembrar os instantes\nEm que tu mordias os meus lábios\nDepois do amor\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque não estás\nPorque não estás aqui\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMinha mãe nunca me viu partir\nMas todo o filho um dia voa\nSaudade, saudade\nMamã cuida dela por mim\nDiz-me porque não estás aqui\nMaria, sem ti fico à toa\nSaudade, saudade\nMaria, eu quero-te ver\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque a saudade aperta o meu peito\nE dói demais (dói demais)\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana (Maria), Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nQuantas noites não dormi\nCada gota que eu deitei\nFez um rio que me leva a ti\nMaria Joana\nMaria Joana (quantas lágrimas chorei)\nApanha o primeiro autocarro\nVem ficar pra sempre\nDo meu lado (quantas noites não dormi)\nMaria Joana (ai, Maria)\nMaria Joana (cada gota que eu deitei)\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nTo see the full lyrics (TIP: press tab to autocomplete the file name)\ncat maria_joanna_song\nShow the first 5 lines\nhead -5 maria_joanna_song\nTo see only the last 5 lines\ntail -5 maria_joanna_song\nTo see only the last 10 to 15 lines (note the “|” sign it is a pipe and let’s you combine the commands into one, without dumping the information onto your harddrive)\nhead -15 maria_joanna_song | tail -5\nIf you want to put information into your disk you can do as follows\nhead -15 maria_joanna_song &gt; top15_maria\ntail -5 top15_maria &gt; lines10-15_maria\nTo see the output we need to either click on the file or use the terminal\ncat lines10-15_maria\n\n\n\nHow to find something in the text?\ngrep Maria maria_joanna_song\nAnd how to see only the matched pattern - commands have “options”\ngrep -o Maria maria_joanna_song\nYou can always get help ( command --help or shortcut version -h) - generally not much usefull - better to google it or ask ChatGPT\ngrep --help \nLet’s count word Maria - three different ways to count something\ngrep Maria maria_joanna_song | wc -l\ngrep Maria maria_joanna_song | wc -w\ngrep Maria maria_joanna_song | wc -c\nNow count appearances of Maria Joanna - it is important to use “” if the your search contains a space or some special character\ngrep \"Maria Joana\" maria_joanna_song | wc -l\ngrep \"Maria Joana\" maria_joanna_song | wc -w\ngrep \"Maria Joana\" maria_joanna_song | wc -c\nHow to deal with special characters\ngrep ( maria_joanna_song   #note the error\ngrep \"(\" maria_joanna_song\ngrep \\( maria_joanna_song\n\n\n\n“.” and “*” can represent any characters and therefore represent patterns rather than exact matches\ngrep Eu maria_joanna_song\ngrep Eu.* maria_joanna_song\nPlaying with multiple choices\ngrep se maria_joanna_song \ngrep se[i] maria_joanna_song \ngrep se[i,m] maria_joanna_song \nBut I am in love with Donald Trump and not with Maria Joana - how to I fix the song?\nsed 's/Maria Joana/Donald Trump/g'  maria_joanna_song | sed 's/Maria/Donald/g' &gt; donald_trump_song\nLet’s see what happened - now it doesn’t rhyme as good\ncat donald_trump_song\nmore donald_trump_song\nOk, but are we sure Donald replaced Maria?\ndiff -u *song \ndiff maria_joanna_song donald_trump_song\nvimdiff *song\n\n\n\n\n\n\nNote\n\n\n\nexit vim using Esc + :q which closes the file, alternatively Esc + :wq which saves the file.\n\n\n\n\n\nHave a look how a loop is being used here\ngrep Lisboa *song\nfor f in *song ; do grep Lisboa $f ; done\nTo show which file Lisboa is found in\nfor f in *song ; do grep -l Lisboa $f ; done\nTo show where is it found (i.e. line number)\nfor f in *song ; do grep -n Lisboa $f ; done\nI have just realised that “joanna” in portuguese is written as “joana” - how do I fix the name of my file\n\noption - fix it but keep the original\n\ncp maria_joanna_song maria_joana_song\n\n# list contents of directory\nls\n\n# other variants\nls -l\nls -la\n\noption - neh, I don’t need the previous version - it will just cause the confusion later on\n\nmv maria_joanna_song maria_joana_song\nNow let’s try previous code\ngrep Maria maria_joanna_song  # of course it doesn't work ... - what shall I do now ?\n\n\n\n\n\n\nTaken from official website:\nEDirect will run on Unix and Macintosh computers, and under the Cygwin Unix-emulation environment on Windows PCs. To install the EDirect software, open a terminal window and execute one of the following two commands:\n  sh -c \"$(curl -fsSL https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n\n  sh -c \"$(wget -q https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh -O -)\"\nThis will download a number of scripts and several precompiled programs into an “edirect” folder in the user’s home directory. It may then print an additional command for updating the PATH environment variable in the user’s configuration file. The editing instructions will look something like:\n  echo \"export PATH=\\$HOME/edirect:\\$PATH\" &gt;&gt; $HOME/.bash_profile\nAs a convenience, the installation process ends by offering to run the PATH update command for you. Answer “y” and press the Return key if you want it run. If the PATH is already set correctly, or if you prefer to make any editing changes manually, just press Return.\nOnce installation is complete, run:\n  export PATH=${HOME}/edirect:${PATH}\nto set the PATH for the current terminal session.\n\n\n\n\n\n\nWindows\n\n\n\nYou can try to install SRA-toolkit from here\n\n\n\n\n\nLet’s download some data from the Genbank - it is 16S V4 amplicon data (here the metadata)\nesearch -db sra -query PRJNA783372 | efetch -format runinfo &gt; SraRunTable.csv\nWe only need the SRRnumber, -f1 means first field.\ncut -f1 -d, SraRunTable*.csv  &gt; runs.txt\nTo save time we will work on four samples only\ntail -4 runs.txt &gt; runs4.txt\n\n\n\nOk, now we download the actual raw data\nprefetch --option-file runs4.txt\nIn case of\n\n\n\n\n\n\nNote\n\n\n\nIn case of Command 'prefetch' not found run\nsudo apt install sra-toolkit\n\n\nLet’s unpack it\nfor f in SRR* ; do fastq-dump --split-files  $f ; done\n\n\n\n\n\n\nTip\n\n\n\nTo visualize quality scores, small GUI tool fastqc could be used\n\n\n\n\n\nsudo apt install flash\nFirst merge the reads\n# merge\nflash -m 50 -M 500 -x 0.1 SRR17045222_1.fastq SRR17045222_2.fastq\n\n# rename merged file to index it\nmv out.extendedFrags.fastq file1.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045223_1.fastq SRR17045223_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045226_1.fastq SRR17045226_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045227_2.fastq SRR17045227_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nNow we filter low quality reads (here the data is kind of a fake as the fastq download needs to be adjusted wit the prefetch script)\n# if not installed, do\nsudo apt install vsearch\n\nfor f in file* ; do vsearch --fastq_filter $f --fastq_maxee 1.0 --fastqout $f\\_filtered ; done\nIt is getting messy here (delete unwanted files)\nrm *notCombined*\nrm *hist\nrm *histogram\nrm *fastq\n\n\n\n\n\n\nTip\n\n\n\nToo lazy to empty the trash yourself ? the terminal will do that for us gio trash --empty\n\n\nLet’s convert fastq to fasta\n# if not installed, do\nsudo apt install seqtk\n\nfor f in *fastq_filtered ; do seqtk seq -a $f &gt; $f.fasta ; done ; \n\n\n\n\n\n\nCaution\n\n\n\nBlast is commented out as it is unlikely to work - we could try with internal database one day\n\n\nTo save time let’s just use 10 reads from each file for taxonomic identification - we will use GenBank nt database\nfor f in *fastq_filtered.fasta ; do head -20 $f &gt; $f\\head ; done\nSimplify the naming\n\n# if not installed, do\nsudo apt install rename\n\nrename 's/.fastq_filtered.fastahead/.fasta/g' *fastq_filtered.fastahead ;\n\n\n\nLet’s try - it may not work at all, first install\nsudo apt install ncbi-blast+\nblastn -query file1.fasta -db nt -remote -outfmt 6 -max_target_seqs 1 -perc_identity 95 &gt; file1_blast.txt \n\nblastn -query file1.fasta -db nr -remote  &gt; file1_blast2.txt\n\n\nUse the web-based blast, for example on the ncbi server.\nLet’s create ASV table and visualise the data\ncat file[0-9]*.fastq_filtered.fasta &gt; all_samples.fasta\n# if not installed, do\nsudo apt install vsearch\n\nvsearch --derep_fulllength all_samples.fasta   --output dereplicated.fasta  --sizeout  --relabel ASV_\nvsearch --sortbysize dereplicated.fasta  --output dereplicated_no_singletons.fasta  --minsize 2\nvsearch --cluster_unoise dereplicated_no_singletons.fasta --minsize 2  --unoise_alpha 2   --centroids ASVs.fasta\nvsearch --usearch_global all_samples.fasta --db ASVs.fasta   --id 0.97     --otutabout ASV_table.txt\nWe will use MicrobiomeAnalyst - every tool needs the input in some specific format - here we need to change the table as well\nsed 's/#OTU ID/#NAME/g' ASV_table.txt &gt; ASV_table_MA.txt\nWe need some metadata (all invented here)\nhead -1 ASV_table_MA.txt | fmt -1 | sed 's/NAME/NAME, sample/g'  | sed 's/11$/11,fish/g' | sed 's/13$/13,fish/g' | sed 's/21$/21,fish/g' | sed 's/22$/22,fish/g' |sed 's/23$/23,not_a_fish/g' | sed 's/24$/24,not_a_fish/g' |sed 's/25$/25,not_a_fish/g' | sed 's/27$/27,not_a_fish/g' &gt; metadata.csv\n\n\n\n\n\nFollowing this link you can interrogate the same table (ASV_table_MA.txt) in the interactive jupyter notebook.\nTo compute Braycurtis distances \nTo visualize the PCoA \n\n\n\nIf you have python installed you can run scripts which are in this folder.\npython append_braycurtis3.py ASV_table_MA.txt &&\npython pcoa_with_metadata_quick_legend.py\n\n\n\nThe power of command line lies in the fact, that once you test your pipeline command by command, you can copy all the commands into a so-called bash script and rerun the whole thing at once.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nYou may want to create a separate new virtual environment (example uses conda) and install necessary dependencies first\n# create virtual environment\nconda create -n gen-test python\n\n# activate it\nconda activate gen-test\n\n# install deps (pip or conda install)\npip install dask\npip install --upgrade --force-reinstall scipy dask\npip install pyarrow\n\npython3 append_braycurtis3.py ASV_table_MA.txt\n\n# dependencies for the second script\npip install numpy==1.24.3\npip install seaborn\npip install panel\npip install scikit-bio\n\npython3 pcoa_with_metadata_quick_work.py \n\n\n\n# make a new folder for a test\nmkdir pipeline_test\n\n# copy python scripts too, if you want to visualize the results.\ncp *.py ./pipeline_test\n\n# take the commands from the command list file\ngrep -A 1000 \"genomics part\" course_linux_commands | sed '/fastqc/d' | sed '/\\*\\*\\*/d' &gt; ./pipeline_test/my_first_pipeline.sh \nRun the pipeline with\nbash my_fist_pipeline.sh\n\n\n\n\n\n\nCaution\n\n\n\nYou might need to make the script executable\nchmod +x my_fist_pipeline.sh\n\n# then you can run directly\n./my_fist_pipeline.sh",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#setup",
    "href": "2025_spring/250416_atkacz.html#setup",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "No extra setup needed for Linux and MacOS machines. For windows the recommended way is to install VBox. See the steps to follow:\nHere are the instructions: Installing Ubuntu on VirtualBox\n\n\n\nRun the Installer:\nAfter downloading the VirtualBox installer (.exe file for WinOS, .dmg for MacOS), double-click on it to launch the setup wizard and follow default installation instructions.\nLaunch VirtualBox.\n\n\n\n\n\nOpen VirtualBox and Create a New VM: Click on the New button.\nName and Operating System: Give your VM a name (e.g., “Ubuntu”), select “Linux” as the type, and choose Ubuntu (64-bit) as the version.\nAssign Memory: Allocate at least 2048 MB (2 GB) of RAM (or higher if your system permits) to ensure smooth operation.\nCreate a Virtual Hard Disk:\n\nChoose Create a virtual hard disk now.\nSelect the VDI (VirtualBox Disk Image) format, and set the disk size (a minimum of 20 GB is recommended).\n\n\n\n\n\n\nAttach the Ubuntu ISO after downloading:\n\nSelect your new VM, click on Settings, then navigate to the Storage section.\nUnder the “Controller: IDE” (or SATA), click on the empty optical drive.\nClick the small disk icon on the right and choose Choose a disk file. Browse to and select the Ubuntu ISO you downloaded.\n\nStart the Virtual Machine:\n\nWith the ISO attached, click Start.\n\n\n\n\n\n\nNote\n\n\n\nYou might need to restart your machine\n\n\nThe VM will boot from the ISO, and you should see the Ubuntu welcome screen.\n\nInstall Ubuntu:\n\nFollow the on-screen instructions in the Ubuntu installer:\n\nChoose your language and keyboard layout.\nSelect Install Ubuntu.\nFollow prompts to set up your timezone, create a user account, and configure your disk (use the default settings for a virtual disk).\nOnce the installation is complete, you’ll be prompted to restart the VM.",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#command-line",
    "href": "2025_spring/250416_atkacz.html#command-line",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Eventually you will need bigger compute than you can get on your machine and you will be forced to work on Cloud such as High Power cluster (HPC) which allow you to parallelize and run heavy computations.\nThings can be done faster on a command line with practice.\n\n\n\n\nOpen the terminal and let’s create a file called maria_joanna_song\ntouch maria_joanna_song\nlet’s just copy in this text - open the file with double click and paste this song into the maria_joanna_song file, also found here\nE virou!\nOnde anda essa Maria?\nEu só tenho essa Maria\nEu vim do norte direto a Lisboa\nAtrás de um sonho\nQue eu nem sei se voa\nTanto quanto nós voávamos\nDebaixo dos lencóis\nA francesinha já não tem picante\nE isso faz-me lembrar os instantes\nEm que tu mordias os meus lábios\nDepois do amor\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque não estás\nPorque não estás aqui\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMinha mãe nunca me viu partir\nMas todo o filho um dia voa\nSaudade, saudade\nMamã cuida dela por mim\nDiz-me porque não estás aqui\nMaria, sem ti fico à toa\nSaudade, saudade\nMaria, eu quero-te ver\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque a saudade aperta o meu peito\nE dói demais (dói demais)\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana (Maria), Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nQuantas noites não dormi\nCada gota que eu deitei\nFez um rio que me leva a ti\nMaria Joana\nMaria Joana (quantas lágrimas chorei)\nApanha o primeiro autocarro\nVem ficar pra sempre\nDo meu lado (quantas noites não dormi)\nMaria Joana (ai, Maria)\nMaria Joana (cada gota que eu deitei)\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nTo see the full lyrics (TIP: press tab to autocomplete the file name)\ncat maria_joanna_song\nShow the first 5 lines\nhead -5 maria_joanna_song\nTo see only the last 5 lines\ntail -5 maria_joanna_song\nTo see only the last 10 to 15 lines (note the “|” sign it is a pipe and let’s you combine the commands into one, without dumping the information onto your harddrive)\nhead -15 maria_joanna_song | tail -5\nIf you want to put information into your disk you can do as follows\nhead -15 maria_joanna_song &gt; top15_maria\ntail -5 top15_maria &gt; lines10-15_maria\nTo see the output we need to either click on the file or use the terminal\ncat lines10-15_maria\n\n\n\nHow to find something in the text?\ngrep Maria maria_joanna_song\nAnd how to see only the matched pattern - commands have “options”\ngrep -o Maria maria_joanna_song\nYou can always get help ( command --help or shortcut version -h) - generally not much usefull - better to google it or ask ChatGPT\ngrep --help \nLet’s count word Maria - three different ways to count something\ngrep Maria maria_joanna_song | wc -l\ngrep Maria maria_joanna_song | wc -w\ngrep Maria maria_joanna_song | wc -c\nNow count appearances of Maria Joanna - it is important to use “” if the your search contains a space or some special character\ngrep \"Maria Joana\" maria_joanna_song | wc -l\ngrep \"Maria Joana\" maria_joanna_song | wc -w\ngrep \"Maria Joana\" maria_joanna_song | wc -c\nHow to deal with special characters\ngrep ( maria_joanna_song   #note the error\ngrep \"(\" maria_joanna_song\ngrep \\( maria_joanna_song\n\n\n\n“.” and “*” can represent any characters and therefore represent patterns rather than exact matches\ngrep Eu maria_joanna_song\ngrep Eu.* maria_joanna_song\nPlaying with multiple choices\ngrep se maria_joanna_song \ngrep se[i] maria_joanna_song \ngrep se[i,m] maria_joanna_song \nBut I am in love with Donald Trump and not with Maria Joana - how to I fix the song?\nsed 's/Maria Joana/Donald Trump/g'  maria_joanna_song | sed 's/Maria/Donald/g' &gt; donald_trump_song\nLet’s see what happened - now it doesn’t rhyme as good\ncat donald_trump_song\nmore donald_trump_song\nOk, but are we sure Donald replaced Maria?\ndiff -u *song \ndiff maria_joanna_song donald_trump_song\nvimdiff *song\n\n\n\n\n\n\nNote\n\n\n\nexit vim using Esc + :q which closes the file, alternatively Esc + :wq which saves the file.\n\n\n\n\n\nHave a look how a loop is being used here\ngrep Lisboa *song\nfor f in *song ; do grep Lisboa $f ; done\nTo show which file Lisboa is found in\nfor f in *song ; do grep -l Lisboa $f ; done\nTo show where is it found (i.e. line number)\nfor f in *song ; do grep -n Lisboa $f ; done\nI have just realised that “joanna” in portuguese is written as “joana” - how do I fix the name of my file\n\noption - fix it but keep the original\n\ncp maria_joanna_song maria_joana_song\n\n# list contents of directory\nls\n\n# other variants\nls -l\nls -la\n\noption - neh, I don’t need the previous version - it will just cause the confusion later on\n\nmv maria_joanna_song maria_joana_song\nNow let’s try previous code\ngrep Maria maria_joanna_song  # of course it doesn't work ... - what shall I do now ?",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#genomics",
    "href": "2025_spring/250416_atkacz.html#genomics",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Taken from official website:\nEDirect will run on Unix and Macintosh computers, and under the Cygwin Unix-emulation environment on Windows PCs. To install the EDirect software, open a terminal window and execute one of the following two commands:\n  sh -c \"$(curl -fsSL https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n\n  sh -c \"$(wget -q https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh -O -)\"\nThis will download a number of scripts and several precompiled programs into an “edirect” folder in the user’s home directory. It may then print an additional command for updating the PATH environment variable in the user’s configuration file. The editing instructions will look something like:\n  echo \"export PATH=\\$HOME/edirect:\\$PATH\" &gt;&gt; $HOME/.bash_profile\nAs a convenience, the installation process ends by offering to run the PATH update command for you. Answer “y” and press the Return key if you want it run. If the PATH is already set correctly, or if you prefer to make any editing changes manually, just press Return.\nOnce installation is complete, run:\n  export PATH=${HOME}/edirect:${PATH}\nto set the PATH for the current terminal session.\n\n\n\n\n\n\nWindows\n\n\n\nYou can try to install SRA-toolkit from here\n\n\n\n\n\nLet’s download some data from the Genbank - it is 16S V4 amplicon data (here the metadata)\nesearch -db sra -query PRJNA783372 | efetch -format runinfo &gt; SraRunTable.csv\nWe only need the SRRnumber, -f1 means first field.\ncut -f1 -d, SraRunTable*.csv  &gt; runs.txt\nTo save time we will work on four samples only\ntail -4 runs.txt &gt; runs4.txt\n\n\n\nOk, now we download the actual raw data\nprefetch --option-file runs4.txt\nIn case of\n\n\n\n\n\n\nNote\n\n\n\nIn case of Command 'prefetch' not found run\nsudo apt install sra-toolkit\n\n\nLet’s unpack it\nfor f in SRR* ; do fastq-dump --split-files  $f ; done\n\n\n\n\n\n\nTip\n\n\n\nTo visualize quality scores, small GUI tool fastqc could be used\n\n\n\n\n\nsudo apt install flash\nFirst merge the reads\n# merge\nflash -m 50 -M 500 -x 0.1 SRR17045222_1.fastq SRR17045222_2.fastq\n\n# rename merged file to index it\nmv out.extendedFrags.fastq file1.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045223_1.fastq SRR17045223_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045226_1.fastq SRR17045226_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045227_2.fastq SRR17045227_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nNow we filter low quality reads (here the data is kind of a fake as the fastq download needs to be adjusted wit the prefetch script)\n# if not installed, do\nsudo apt install vsearch\n\nfor f in file* ; do vsearch --fastq_filter $f --fastq_maxee 1.0 --fastqout $f\\_filtered ; done\nIt is getting messy here (delete unwanted files)\nrm *notCombined*\nrm *hist\nrm *histogram\nrm *fastq\n\n\n\n\n\n\nTip\n\n\n\nToo lazy to empty the trash yourself ? the terminal will do that for us gio trash --empty\n\n\nLet’s convert fastq to fasta\n# if not installed, do\nsudo apt install seqtk\n\nfor f in *fastq_filtered ; do seqtk seq -a $f &gt; $f.fasta ; done ; \n\n\n\n\n\n\nCaution\n\n\n\nBlast is commented out as it is unlikely to work - we could try with internal database one day\n\n\nTo save time let’s just use 10 reads from each file for taxonomic identification - we will use GenBank nt database\nfor f in *fastq_filtered.fasta ; do head -20 $f &gt; $f\\head ; done\nSimplify the naming\n\n# if not installed, do\nsudo apt install rename\n\nrename 's/.fastq_filtered.fastahead/.fasta/g' *fastq_filtered.fastahead ;\n\n\n\nLet’s try - it may not work at all, first install\nsudo apt install ncbi-blast+\nblastn -query file1.fasta -db nt -remote -outfmt 6 -max_target_seqs 1 -perc_identity 95 &gt; file1_blast.txt \n\nblastn -query file1.fasta -db nr -remote  &gt; file1_blast2.txt\n\n\nUse the web-based blast, for example on the ncbi server.\nLet’s create ASV table and visualise the data\ncat file[0-9]*.fastq_filtered.fasta &gt; all_samples.fasta\n# if not installed, do\nsudo apt install vsearch\n\nvsearch --derep_fulllength all_samples.fasta   --output dereplicated.fasta  --sizeout  --relabel ASV_\nvsearch --sortbysize dereplicated.fasta  --output dereplicated_no_singletons.fasta  --minsize 2\nvsearch --cluster_unoise dereplicated_no_singletons.fasta --minsize 2  --unoise_alpha 2   --centroids ASVs.fasta\nvsearch --usearch_global all_samples.fasta --db ASVs.fasta   --id 0.97     --otutabout ASV_table.txt\nWe will use MicrobiomeAnalyst - every tool needs the input in some specific format - here we need to change the table as well\nsed 's/#OTU ID/#NAME/g' ASV_table.txt &gt; ASV_table_MA.txt\nWe need some metadata (all invented here)\nhead -1 ASV_table_MA.txt | fmt -1 | sed 's/NAME/NAME, sample/g'  | sed 's/11$/11,fish/g' | sed 's/13$/13,fish/g' | sed 's/21$/21,fish/g' | sed 's/22$/22,fish/g' |sed 's/23$/23,not_a_fish/g' | sed 's/24$/24,not_a_fish/g' |sed 's/25$/25,not_a_fish/g' | sed 's/27$/27,not_a_fish/g' &gt; metadata.csv",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#visualise-the-data-with-python",
    "href": "2025_spring/250416_atkacz.html#visualise-the-data-with-python",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Following this link you can interrogate the same table (ASV_table_MA.txt) in the interactive jupyter notebook.\nTo compute Braycurtis distances \nTo visualize the PCoA",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#local-python-scripts",
    "href": "2025_spring/250416_atkacz.html#local-python-scripts",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "If you have python installed you can run scripts which are in this folder.\npython append_braycurtis3.py ASV_table_MA.txt &&\npython pcoa_with_metadata_quick_legend.py",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#finally",
    "href": "2025_spring/250416_atkacz.html#finally",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "The power of command line lies in the fact, that once you test your pipeline command by command, you can copy all the commands into a so-called bash script and rerun the whole thing at once.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nYou may want to create a separate new virtual environment (example uses conda) and install necessary dependencies first\n# create virtual environment\nconda create -n gen-test python\n\n# activate it\nconda activate gen-test\n\n# install deps (pip or conda install)\npip install dask\npip install --upgrade --force-reinstall scipy dask\npip install pyarrow\n\npython3 append_braycurtis3.py ASV_table_MA.txt\n\n# dependencies for the second script\npip install numpy==1.24.3\npip install seaborn\npip install panel\npip install scikit-bio\n\npython3 pcoa_with_metadata_quick_work.py \n\n\n\n# make a new folder for a test\nmkdir pipeline_test\n\n# copy python scripts too, if you want to visualize the results.\ncp *.py ./pipeline_test\n\n# take the commands from the command list file\ngrep -A 1000 \"genomics part\" course_linux_commands | sed '/fastqc/d' | sed '/\\*\\*\\*/d' &gt; ./pipeline_test/my_first_pipeline.sh \nRun the pipeline with\nbash my_fist_pipeline.sh\n\n\n\n\n\n\nCaution\n\n\n\nYou might need to make the script executable\nchmod +x my_fist_pipeline.sh\n\n# then you can run directly\n./my_fist_pipeline.sh",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_fall/call.html",
    "href": "2025_fall/call.html",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "Since the world is spinning, the next edition is uncertain. Therefore, we call for your ideas and contributions already now for the Autumn 2025.",
    "crumbs": [
      "Fall 2025",
      "Call for contributions"
    ]
  },
  {
    "objectID": "2025_fall/call.html#call-for-contributions",
    "href": "2025_fall/call.html#call-for-contributions",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "Since the world is spinning, the next edition is uncertain. Therefore, we call for your ideas and contributions already now for the Autumn 2025.",
    "crumbs": [
      "Fall 2025",
      "Call for contributions"
    ]
  }
]