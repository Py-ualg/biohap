[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIOinformatics: Handy and Practical",
    "section": "",
    "text": "BIOinformatics: Handy and Practical\nThis is a compilation of bioinformatics knowledge at CCMAR-Algarve. First series of hands-on workshop happened in Spring 2025.\nUntil this day, three hands-on workshops designed to equip researchers with essential data analysis skills for bioinformatics have been announced. Every month, you will get served a workshop which cover a key aspect of biological data analysis, from RNA-seq to microbiome analysis, helping you turn raw sequence files into meaningful biological insights.\nTo visit the source files for the workshops and the webpage, visit our Github. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Introduction",
      "BIOinformatics: Handy and Practical"
    ]
  },
  {
    "objectID": "basics/abundance_tables.html",
    "href": "basics/abundance_tables.html",
    "title": "Abundance table preprocessing",
    "section": "",
    "text": "Work in progress\nThis is a personal commitment to understand the effect (variance) of abundance table normalization and scaling methods on downstream tasks, which may be differential analysis etc.\nMost well-known packages have the normalization methods implemented so raw data tables can be supplied to them, such as QIIME2 or refseq. For EMO-BON analysis, I do not use those (might be a mistake, because of bug risks), so I need to understand them properly.\nLoad SSU combined taxonomy from 181 EMO-BON samplings.\nCode\nimport pandas as pd\nimport numpy as np\nfrom skbio.diversity import beta_diversity\n\n#| code-fold: false\n# read the data from github\nssu_url = \"https://github.com/emo-bon/momics-demos/raw/refs/heads/main/data/parquet_files/metagoflow_analyses.SSU.parquet\"\n\nssu = pd.read_parquet(ssu_url)\n\n# change abundance to int\nssu['abundance'] = ssu['abundance'].astype(int)\nssu.head()\n\n\n\n\n\n\n\n\n\nref_code\nncbi_tax_id\nabundance\nsuperkingdom\nkingdom\nphylum\nclass\norder\nfamily\ngenus\nspecies\n\n\n\n\n0\nEMOBON00084\n2157\n7\nArchaea\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n1\nEMOBON00084\n1801616\n1\nArchaea\n\nCandidatus_Woesearchaeota\nNone\nNone\nNone\nNone\nNone\n\n\n2\nEMOBON00084\n28890\n1\nArchaea\n\nEuryarchaeota\nNone\nNone\nNone\nNone\nNone\n\n\n3\nEMOBON00084\n183968\n1\nArchaea\n\nEuryarchaeota\nThermococci\nNone\nNone\nNone\nNone\n\n\n4\nEMOBON00084\n192989\n3\nArchaea\n\nNanoarchaeota\nNone\nNone\nNone\nNone\nNone\nLet’s order them by abundance\nCode\nssu.sort_values(by='abundance', inplace=True, ascending=False)\n\nssu\n\n\n\n\n\n\n\n\n\nref_code\nncbi_tax_id\nabundance\nsuperkingdom\nkingdom\nphylum\nclass\norder\nfamily\ngenus\nspecies\n\n\n\n\n9377\nEMOBON00009\n1236\n26938\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n\n\n9014\nEMOBON00010\n1236\n26431\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n\n\n10691\nEMOBON00008\n1236\n14402\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n\n\n68804\nEMOBON00125\n54526\n13108\nBacteria\n\nProteobacteria\nAlphaproteobacteria\nPelagibacterales\nNone\nNone\nNone\n\n\n17321\nEMOBON00003\n72037\n12545\nEukaryota\nMetazoa\nArthropoda\nHexanauplia\nNone\nNone\nNone\nNone\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76710\nEMOBON00228\n12916\n1\nBacteria\n\nProteobacteria\nBetaproteobacteria\nBurkholderiales\nComamonadaceae\nAcidovorax\nNone\n\n\n76708\nEMOBON00228\n48736\n1\nBacteria\n\nProteobacteria\nBetaproteobacteria\nBurkholderiales\nBurkholderiaceae\nRalstonia\nNone\n\n\n76702\nEMOBON00228\n1381133\n1\nBacteria\n\nProteobacteria\nBetaproteobacteria\n\n\nCandidatus_Profftella\nNone\n\n\n76698\nEMOBON00228\n165695\n1\nBacteria\n\nProteobacteria\nAlphaproteobacteria\nSphingomonadales\nSphingomonadaceae\nSphingobium\nNone\n\n\n76694\nEMOBON00228\n418853\n1\nBacteria\n\nProteobacteria\nAlphaproteobacteria\nSneathiellales\nSneathiellaceae\nSneathiella\nSneathiella_glossodoripedis\n\n\n\n\n111320 rows × 11 columns",
    "crumbs": [
      "Basics",
      "Abundance table preprocessing"
    ]
  },
  {
    "objectID": "basics/abundance_tables.html#total-sum-scaling-tss-followed-by-square-root-transformation",
    "href": "basics/abundance_tables.html#total-sum-scaling-tss-followed-by-square-root-transformation",
    "title": "Abundance table preprocessing",
    "section": "Total Sum Scaling (TSS) followed by Square Root Transformation",
    "text": "Total Sum Scaling (TSS) followed by Square Root Transformation\n\nTSS\n\nconverts raw counts into relative abundances, alternative name Relative Abundance Normalization. Simple division by sum of abundances in each sample separately.\nPurpose: Adjusts for varying sequencing depths between samples.\nreference, McMurdie, P. J., & Holmes, S. (2014). Waste not, want not: why rarefying microbiome data is inadmissible. PLoS computational biology, 10(4), e1003531.\n\n\n\nSquare root transformation to relative abundances\n\nThis is a variance-stabilizing transformation — it reduces the effect of highly abundant taxa and improves comparability across samples.\nIt’s commonly used before distance-based analyses like Bray–Curtis dissimilarity or ordination (e.g., NMDS, PCoA).\nreference, Legendre, P., & Gallagher, E. D. (2001). Ecologically meaningful transformations for ordination of species data. Oecologia, 129(2), 271–280.\n\nHere is a function to pivot the taxonomy:\n\n\nCode\ndef pivot_taxonomic_data(df: pd.DataFrame, values_col='abundance') -&gt; pd.DataFrame:\n    \"\"\"\n    Prepares the taxonomic data (LSU and SSU tables) for analysis.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing taxonomic information.\n\n    Returns:\n        pd.DataFrame: A pivot table with taxonomic data.\n    \"\"\"\n    # Select relevant columns\n    df['taxonomic_concat'] = (\n        df['ncbi_tax_id'].astype(str) + \n        ';sk_' + df['superkingdom'].fillna('') + \n        ';k_' + df['kingdom'].fillna('') + \n        ';p_' + df['phylum'].fillna('') + \n        ';c_' + df['class'].fillna('') + \n        ';o_' + df['order'].fillna('') + \n        ';f_' + df['family'].fillna('') + \n        ';g_' + df['genus'].fillna('') + \n        ';s_' + df['species'].fillna('')\n    )\n    pivot_table = df.pivot_table(\n        index=['ncbi_tax_id','taxonomic_concat'], \n        columns='ref_code', \n        values=values_col,\n    ).fillna(0)\n    pivot_table = pivot_table.reset_index()\n    # change inex name\n    pivot_table.columns.name = None\n\n    return pivot_table\n\n\n, and methods to calculate to apply various scaling and normalization methods:\n\ndef TSS(df, sampleIds='ref_code'):\n    \"\"\" Calculate TSS\"\"\"\n    df['abundance_TSS'] = df.groupby(sampleIds)['abundance'].transform(lambda x: x / x.sum())\n    return df",
    "crumbs": [
      "Basics",
      "Abundance table preprocessing"
    ]
  },
  {
    "objectID": "basics/abundance_tables.html#now-i-want-to-systematically-transform-and-send-downstream",
    "href": "basics/abundance_tables.html#now-i-want-to-systematically-transform-and-send-downstream",
    "title": "Abundance table preprocessing",
    "section": "Now I want to systematically transform and send downstream",
    "text": "Now I want to systematically transform and send downstream\nDownstream tasks are\n\nBeta diversity\nPCoA\n???\n\n\nssu = TSS(ssu)\n\nassert ssu[ssu['ref_code'] == 'EMOBON00009']['abundance_TSS'].sum() == 1.0\nssu.head()\n\n\n\n\n\n\n\n\nref_code\nncbi_tax_id\nabundance\nsuperkingdom\nkingdom\nphylum\nclass\norder\nfamily\ngenus\nspecies\nabundance_TSS\n\n\n\n\n9377\nEMOBON00009\n1236\n26938\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n0.457989\n\n\n9014\nEMOBON00010\n1236\n26431\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n0.458879\n\n\n10691\nEMOBON00008\n1236\n14402\nBacteria\n\nProteobacteria\nGammaproteobacteria\nNone\nNone\nNone\nNone\n0.216178\n\n\n68804\nEMOBON00125\n54526\n13108\nBacteria\n\nProteobacteria\nAlphaproteobacteria\nPelagibacterales\nNone\nNone\nNone\n0.376753\n\n\n17321\nEMOBON00003\n72037\n12545\nEukaryota\nMetazoa\nArthropoda\nHexanauplia\nNone\nNone\nNone\nNone\n0.337676\n\n\n\n\n\n\n\nCalculate and plot Beta diversity\n\n\nCode\nimport seaborn as sns\n\npivot = pivot_taxonomic_data(ssu, values_col='abundance_TSS')\nmetric = 'braycurtis'\npivot.head()\n\n\n\n\n\n\n\n\n\nncbi_tax_id\ntaxonomic_concat\nEMOBON00001\nEMOBON00003\nEMOBON00004\nEMOBON00005\nEMOBON00006\nEMOBON00007\nEMOBON00008\nEMOBON00009\n...\nEMOBON00242\nEMOBON00243\nEMOBON00244\nEMOBON00245\nEMOBON00246\nEMOBON00247\nEMOBON00248\nEMOBON00249\nEMOBON00250\nEMOBON00251\n\n\n\n\n0\n2\n2;sk_Bacteria;k_;p_;c_;o_;f_;g_;s_\n0.018025\n0.017335\n0.027319\n0.047492\n0.04602\n0.069328\n0.060912\n0.00692\n...\n0.017378\n0.013971\n0.051088\n0.016756\n0.053871\n0.019248\n0.041221\n0.025231\n0.039109\n0.026800\n\n\n1\n6\n6;sk_Bacteria;k_;p_Proteobacteria;c_Alphaprote...\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n0.00000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\n10\n10;sk_Bacteria;k_;p_Proteobacteria;c_Gammaprot...\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000017\n0.000000\n0.00000\n...\n0.000000\n0.000000\n0.000201\n0.000051\n0.000145\n0.000050\n0.000123\n0.000118\n0.000088\n0.000182\n\n\n3\n16\n16;sk_Bacteria;k_;p_Proteobacteria;c_Betaprote...\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n0.00000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n4\n18\n18;sk_Bacteria;k_;p_Proteobacteria;c_Deltaprot...\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.000000\n0.000000\n0.00000\n...\n0.000097\n0.000219\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n5 rows × 183 columns\n\n\n\n\n\nCode\nbeta = beta_diversity(metric, pivot.iloc[:, 2:].T)\nsns.heatmap(beta.to_data_frame(), vmin=0, vmax=1.0, cmap=\"viridis\")\n\n\n\n\n\n\n\n\n\n\nHow do I evaluate difference between methods?",
    "crumbs": [
      "Basics",
      "Abundance table preprocessing"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html",
    "href": "2025_spring/250416_atkacz.html",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Large files or datasets—especially those containing genomic data—no problem. This workshop introduces essential Linux commands and simple Bash scripts to streamline data manipulation tasks. We’ll cover key operations such as searching for patterns, globally modifying content, and aligning DNA sequences, such as FASTAQ and FASTA files.\n\n\nNo extra setup needed for Linux and MacOS machines. For windows the recommended way is to install VBox. See the steps to follow:\nHere are the instructions: Installing Ubuntu on VirtualBox\n\n\n\nRun the Installer:\nAfter downloading the VirtualBox installer (.exe file for WinOS, .dmg for MacOS), double-click on it to launch the setup wizard and follow default installation instructions.\nLaunch VirtualBox.\n\n\n\n\n\nOpen VirtualBox and Create a New VM: Click on the New button.\nName and Operating System: Give your VM a name (e.g., “Ubuntu”), select “Linux” as the type, and choose Ubuntu (64-bit) as the version.\nAssign Memory: Allocate at least 2048 MB (2 GB) of RAM (or higher if your system permits) to ensure smooth operation.\nCreate a Virtual Hard Disk:\n\nChoose Create a virtual hard disk now.\nSelect the VDI (VirtualBox Disk Image) format, and set the disk size (a minimum of 20 GB is recommended).\n\n\n\n\n\n\nAttach the Ubuntu ISO after downloading:\n\nSelect your new VM, click on Settings, then navigate to the Storage section.\nUnder the “Controller: IDE” (or SATA), click on the empty optical drive.\nClick the small disk icon on the right and choose Choose a disk file. Browse to and select the Ubuntu ISO you downloaded.\n\nStart the Virtual Machine:\n\nWith the ISO attached, click Start.\n\n\n\n\n\n\nNote\n\n\n\nYou might need to restart your machine\n\n\nThe VM will boot from the ISO, and you should see the Ubuntu welcome screen.\n\nInstall Ubuntu:\n\nFollow the on-screen instructions in the Ubuntu installer:\n\nChoose your language and keyboard layout.\nSelect Install Ubuntu.\nFollow prompts to set up your timezone, create a user account, and configure your disk (use the default settings for a virtual disk).\nOnce the installation is complete, you’ll be prompted to restart the VM.\n\n\n\n\n\n\n\n\n\n\nEventually you will need bigger compute than you can get on your machine and you will be forced to work on Cloud such as High Power cluster (HPC) which allow you to parallelize and run heavy computations.\nThings can be done faster on a command line with practice.\n\n\n\n\nOpen the terminal and let’s create a file called maria_joanna_song\ntouch maria_joanna_song\nlet’s just copy in this text - open the file with double click and paste this song into the maria_joanna_song file, also found here\nE virou!\nOnde anda essa Maria?\nEu só tenho essa Maria\nEu vim do norte direto a Lisboa\nAtrás de um sonho\nQue eu nem sei se voa\nTanto quanto nós voávamos\nDebaixo dos lencóis\nA francesinha já não tem picante\nE isso faz-me lembrar os instantes\nEm que tu mordias os meus lábios\nDepois do amor\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque não estás\nPorque não estás aqui\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMinha mãe nunca me viu partir\nMas todo o filho um dia voa\nSaudade, saudade\nMamã cuida dela por mim\nDiz-me porque não estás aqui\nMaria, sem ti fico à toa\nSaudade, saudade\nMaria, eu quero-te ver\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque a saudade aperta o meu peito\nE dói demais (dói demais)\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana (Maria), Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nQuantas noites não dormi\nCada gota que eu deitei\nFez um rio que me leva a ti\nMaria Joana\nMaria Joana (quantas lágrimas chorei)\nApanha o primeiro autocarro\nVem ficar pra sempre\nDo meu lado (quantas noites não dormi)\nMaria Joana (ai, Maria)\nMaria Joana (cada gota que eu deitei)\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nTo see the full lyrics (TIP: press tab to autocomplete the file name)\ncat maria_joanna_song\nShow the first 5 lines\nhead -5 maria_joanna_song\nTo see only the last 5 lines\ntail -5 maria_joanna_song\nTo see only the last 10 to 15 lines (note the “|” sign it is a pipe and let’s you combine the commands into one, without dumping the information onto your harddrive)\nhead -15 maria_joanna_song | tail -5\nIf you want to put information into your disk you can do as follows\nhead -15 maria_joanna_song &gt; top15_maria\ntail -5 top15_maria &gt; lines10-15_maria\nTo see the output we need to either click on the file or use the terminal\ncat lines10-15_maria\n\n\n\nHow to find something in the text?\ngrep Maria maria_joanna_song\nAnd how to see only the matched pattern - commands have “options”\ngrep -o Maria maria_joanna_song\nYou can always get help ( command --help or shortcut version -h) - generally not much usefull - better to google it or ask ChatGPT\ngrep --help \nLet’s count word Maria - three different ways to count something\ngrep Maria maria_joanna_song | wc -l\ngrep Maria maria_joanna_song | wc -w\ngrep Maria maria_joanna_song | wc -c\nNow count appearances of Maria Joanna - it is important to use “” if the your search contains a space or some special character\ngrep \"Maria Joana\" maria_joanna_song | wc -l\ngrep \"Maria Joana\" maria_joanna_song | wc -w\ngrep \"Maria Joana\" maria_joanna_song | wc -c\nHow to deal with special characters\ngrep ( maria_joanna_song   #note the error\ngrep \"(\" maria_joanna_song\ngrep \\( maria_joanna_song\n\n\n\n“.” and “*” can represent any characters and therefore represent patterns rather than exact matches\ngrep Eu maria_joanna_song\ngrep Eu.* maria_joanna_song\nPlaying with multiple choices\ngrep se maria_joanna_song \ngrep se[i] maria_joanna_song \ngrep se[i,m] maria_joanna_song \nBut I am in love with Donald Trump and not with Maria Joana - how to I fix the song?\nsed 's/Maria Joana/Donald Trump/g'  maria_joanna_song | sed 's/Maria/Donald/g' &gt; donald_trump_song\nLet’s see what happened - now it doesn’t rhyme as good\ncat donald_trump_song\nmore donald_trump_song\nOk, but are we sure Donald replaced Maria?\ndiff -u *song \ndiff maria_joanna_song donald_trump_song\nvimdiff *song\n\n\n\n\n\n\nNote\n\n\n\nexit vim using Esc + :q which closes the file, alternatively Esc + :wq which saves the file.\n\n\n\n\n\nHave a look how a loop is being used here\ngrep Lisboa *song\nfor f in *song ; do grep Lisboa $f ; done\nTo show which file Lisboa is found in\nfor f in *song ; do grep -l Lisboa $f ; done\nTo show where is it found (i.e. line number)\nfor f in *song ; do grep -n Lisboa $f ; done\nI have just realised that “joanna” in portuguese is written as “joana” - how do I fix the name of my file\n\noption - fix it but keep the original\n\ncp maria_joanna_song maria_joana_song\n\n# list contents of directory\nls\n\n# other variants\nls -l\nls -la\n\noption - neh, I don’t need the previous version - it will just cause the confusion later on\n\nmv maria_joanna_song maria_joana_song\nNow let’s try previous code\ngrep Maria maria_joanna_song  # of course it doesn't work ... - what shall I do now ?\n\n\n\n\n\n\nTaken from official website:\nEDirect will run on Unix and Macintosh computers, and under the Cygwin Unix-emulation environment on Windows PCs. To install the EDirect software, open a terminal window and execute one of the following two commands:\n  sh -c \"$(curl -fsSL https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n\n  sh -c \"$(wget -q https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh -O -)\"\nThis will download a number of scripts and several precompiled programs into an “edirect” folder in the user’s home directory. It may then print an additional command for updating the PATH environment variable in the user’s configuration file. The editing instructions will look something like:\n  echo \"export PATH=\\$HOME/edirect:\\$PATH\" &gt;&gt; $HOME/.bash_profile\nAs a convenience, the installation process ends by offering to run the PATH update command for you. Answer “y” and press the Return key if you want it run. If the PATH is already set correctly, or if you prefer to make any editing changes manually, just press Return.\nOnce installation is complete, run:\n  export PATH=${HOME}/edirect:${PATH}\nto set the PATH for the current terminal session.\n\n\n\n\n\n\nWindows\n\n\n\nYou can try to install SRA-toolkit from here\n\n\n\n\n\nLet’s download some data from the Genbank - it is 16S V4 amplicon data (here the metadata)\nesearch -db sra -query PRJNA783372 | efetch -format runinfo &gt; SraRunTable.csv\nWe only need the SRRnumber, -f1 means first field.\ncut -f1 -d, SraRunTable*.csv  &gt; runs.txt\nTo save time we will work on four samples only\ntail -4 runs.txt &gt; runs4.txt\n\n\n\nOk, now we download the actual raw data\nprefetch --option-file runs4.txt\nIn case of\n\n\n\n\n\n\nNote\n\n\n\nIn case of Command 'prefetch' not found run\nsudo apt install sra-toolkit\n\n\nLet’s unpack it\nfor f in SRR* ; do fastq-dump --split-files  $f ; done\n\n\n\n\n\n\nTip\n\n\n\nTo visualize quality scores, small GUI tool fastqc could be used\n\n\n\n\n\nsudo apt install flash\nFirst merge the reads\n# merge\nflash -m 50 -M 500 -x 0.1 SRR17045222_1.fastq SRR17045222_2.fastq\n\n# rename merged file to index it\nmv out.extendedFrags.fastq file1.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045223_1.fastq SRR17045223_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045226_1.fastq SRR17045226_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045227_2.fastq SRR17045227_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nNow we filter low quality reads (here the data is kind of a fake as the fastq download needs to be adjusted wit the prefetch script)\n# if not installed, do\nsudo apt install vsearch\n\nfor f in file* ; do vsearch --fastq_filter $f --fastq_maxee 1.0 --fastqout $f\\_filtered ; done\nIt is getting messy here (delete unwanted files)\nrm *notCombined*\nrm *hist\nrm *histogram\nrm *fastq\n\n\n\n\n\n\nTip\n\n\n\nToo lazy to empty the trash yourself ? the terminal will do that for us gio trash --empty\n\n\nLet’s convert fastq to fasta\n# if not installed, do\nsudo apt install seqtk\n\nfor f in *fastq_filtered ; do seqtk seq -a $f &gt; $f.fasta ; done ; \n\n\n\n\n\n\nCaution\n\n\n\nBlast is commented out as it is unlikely to work - we could try with internal database one day\n\n\nTo save time let’s just use 10 reads from each file for taxonomic identification - we will use GenBank nt database\nfor f in *fastq_filtered.fasta ; do head -20 $f &gt; $f\\head ; done\nSimplify the naming\n\n# if not installed, do\nsudo apt install rename\n\nrename 's/.fastq_filtered.fastahead/.fasta/g' *fastq_filtered.fastahead ;\n\n\n\nLet’s try - it may not work at all, first install\nsudo apt install ncbi-blast+\nblastn -query file1.fasta -db nt -remote -outfmt 6 -max_target_seqs 1 -perc_identity 95 &gt; file1_blast.txt \n\nblastn -query file1.fasta -db nr -remote  &gt; file1_blast2.txt\n\n\nUse the web-based blast, for example on the ncbi server.\nLet’s create ASV table and visualise the data\ncat file[0-9]*.fastq_filtered.fasta &gt; all_samples.fasta\n# if not installed, do\nsudo apt install vsearch\n\nvsearch --derep_fulllength all_samples.fasta   --output dereplicated.fasta  --sizeout  --relabel ASV_\nvsearch --sortbysize dereplicated.fasta  --output dereplicated_no_singletons.fasta  --minsize 2\nvsearch --cluster_unoise dereplicated_no_singletons.fasta --minsize 2  --unoise_alpha 2   --centroids ASVs.fasta\nvsearch --usearch_global all_samples.fasta --db ASVs.fasta   --id 0.97     --otutabout ASV_table.txt\nWe will use MicrobiomeAnalyst - every tool needs the input in some specific format - here we need to change the table as well\nsed 's/#OTU ID/#NAME/g' ASV_table.txt &gt; ASV_table_MA.txt\nWe need some metadata (all invented here)\nhead -1 ASV_table_MA.txt | fmt -1 | sed 's/NAME/NAME, sample/g'  | sed 's/11$/11,fish/g' | sed 's/13$/13,fish/g' | sed 's/21$/21,fish/g' | sed 's/22$/22,fish/g' |sed 's/23$/23,not_a_fish/g' | sed 's/24$/24,not_a_fish/g' |sed 's/25$/25,not_a_fish/g' | sed 's/27$/27,not_a_fish/g' &gt; metadata.csv\n\n\n\n\n\nFollowing this link you can interrogate the same table (ASV_table_MA.txt) in the interactive jupyter notebook.\nTo compute Braycurtis distances \nTo visualize the PCoA \n\n\n\nIf you have python installed you can run scripts which are in this folder.\npython append_braycurtis3.py ASV_table_MA.txt &&\npython pcoa_with_metadata_quick_legend.py\n\n\n\nThe power of command line lies in the fact, that once you test your pipeline command by command, you can copy all the commands into a so-called bash script and rerun the whole thing at once.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nYou may want to create a separate new virtual environment (example uses conda) and install necessary dependencies first\n# create virtual environment\nconda create -n gen-test python\n\n# activate it\nconda activate gen-test\n\n# install deps (pip or conda install)\npip install dask\npip install --upgrade --force-reinstall scipy dask\npip install pyarrow\n\npython3 append_braycurtis3.py ASV_table_MA.txt\n\n# dependencies for the second script\npip install numpy==1.24.3\npip install seaborn\npip install panel\npip install scikit-bio\n\npython3 pcoa_with_metadata_quick_work.py \n\n\n\n# make a new folder for a test\nmkdir pipeline_test\n\n# copy python scripts too, if you want to visualize the results.\ncp *.py ./pipeline_test\n\n# take the commands from the command list file\ngrep -A 1000 \"genomics part\" course_linux_commands | sed '/fastqc/d' | sed '/\\*\\*\\*/d' &gt; ./pipeline_test/my_first_pipeline.sh \nRun the pipeline with\nbash my_fist_pipeline.sh\n\n\n\n\n\n\nCaution\n\n\n\nYou might need to make the script executable\nchmod +x my_fist_pipeline.sh\n\n# then you can run directly\n./my_fist_pipeline.sh",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#setup",
    "href": "2025_spring/250416_atkacz.html#setup",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "No extra setup needed for Linux and MacOS machines. For windows the recommended way is to install VBox. See the steps to follow:\nHere are the instructions: Installing Ubuntu on VirtualBox\n\n\n\nRun the Installer:\nAfter downloading the VirtualBox installer (.exe file for WinOS, .dmg for MacOS), double-click on it to launch the setup wizard and follow default installation instructions.\nLaunch VirtualBox.\n\n\n\n\n\nOpen VirtualBox and Create a New VM: Click on the New button.\nName and Operating System: Give your VM a name (e.g., “Ubuntu”), select “Linux” as the type, and choose Ubuntu (64-bit) as the version.\nAssign Memory: Allocate at least 2048 MB (2 GB) of RAM (or higher if your system permits) to ensure smooth operation.\nCreate a Virtual Hard Disk:\n\nChoose Create a virtual hard disk now.\nSelect the VDI (VirtualBox Disk Image) format, and set the disk size (a minimum of 20 GB is recommended).\n\n\n\n\n\n\nAttach the Ubuntu ISO after downloading:\n\nSelect your new VM, click on Settings, then navigate to the Storage section.\nUnder the “Controller: IDE” (or SATA), click on the empty optical drive.\nClick the small disk icon on the right and choose Choose a disk file. Browse to and select the Ubuntu ISO you downloaded.\n\nStart the Virtual Machine:\n\nWith the ISO attached, click Start.\n\n\n\n\n\n\nNote\n\n\n\nYou might need to restart your machine\n\n\nThe VM will boot from the ISO, and you should see the Ubuntu welcome screen.\n\nInstall Ubuntu:\n\nFollow the on-screen instructions in the Ubuntu installer:\n\nChoose your language and keyboard layout.\nSelect Install Ubuntu.\nFollow prompts to set up your timezone, create a user account, and configure your disk (use the default settings for a virtual disk).\nOnce the installation is complete, you’ll be prompted to restart the VM.",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#command-line",
    "href": "2025_spring/250416_atkacz.html#command-line",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Eventually you will need bigger compute than you can get on your machine and you will be forced to work on Cloud such as High Power cluster (HPC) which allow you to parallelize and run heavy computations.\nThings can be done faster on a command line with practice.\n\n\n\n\nOpen the terminal and let’s create a file called maria_joanna_song\ntouch maria_joanna_song\nlet’s just copy in this text - open the file with double click and paste this song into the maria_joanna_song file, also found here\nE virou!\nOnde anda essa Maria?\nEu só tenho essa Maria\nEu vim do norte direto a Lisboa\nAtrás de um sonho\nQue eu nem sei se voa\nTanto quanto nós voávamos\nDebaixo dos lencóis\nA francesinha já não tem picante\nE isso faz-me lembrar os instantes\nEm que tu mordias os meus lábios\nDepois do amor\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque não estás\nPorque não estás aqui\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMinha mãe nunca me viu partir\nMas todo o filho um dia voa\nSaudade, saudade\nMamã cuida dela por mim\nDiz-me porque não estás aqui\nMaria, sem ti fico à toa\nSaudade, saudade\nMaria, eu quero-te ver\nNão sei se vou aguentar\nO tempo que vou cá ficar\nPorque a saudade aperta o meu peito\nE dói demais (dói demais)\nAs lágrimas vão secar\nMas a saudade vai continuar\nNo meu peito a chamar\nMaria Joana, Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria Joana (Maria), Maria Joana\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nQuantas noites não dormi\nCada gota que eu deitei\nFez um rio que me leva a ti\nMaria Joana\nMaria Joana (quantas lágrimas chorei)\nApanha o primeiro autocarro\nVem ficar pra sempre\nDo meu lado (quantas noites não dormi)\nMaria Joana (ai, Maria)\nMaria Joana (cada gota que eu deitei)\nApanha o primeiro autocarro\nVem ficar pra sempre do meu lado\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nMaria, Maria, Maria\nMaria, Maria, Maria\n(Maria, Maria, Maria, Maria)\nQuantas lágrimas chorei\nTo see the full lyrics (TIP: press tab to autocomplete the file name)\ncat maria_joanna_song\nShow the first 5 lines\nhead -5 maria_joanna_song\nTo see only the last 5 lines\ntail -5 maria_joanna_song\nTo see only the last 10 to 15 lines (note the “|” sign it is a pipe and let’s you combine the commands into one, without dumping the information onto your harddrive)\nhead -15 maria_joanna_song | tail -5\nIf you want to put information into your disk you can do as follows\nhead -15 maria_joanna_song &gt; top15_maria\ntail -5 top15_maria &gt; lines10-15_maria\nTo see the output we need to either click on the file or use the terminal\ncat lines10-15_maria\n\n\n\nHow to find something in the text?\ngrep Maria maria_joanna_song\nAnd how to see only the matched pattern - commands have “options”\ngrep -o Maria maria_joanna_song\nYou can always get help ( command --help or shortcut version -h) - generally not much usefull - better to google it or ask ChatGPT\ngrep --help \nLet’s count word Maria - three different ways to count something\ngrep Maria maria_joanna_song | wc -l\ngrep Maria maria_joanna_song | wc -w\ngrep Maria maria_joanna_song | wc -c\nNow count appearances of Maria Joanna - it is important to use “” if the your search contains a space or some special character\ngrep \"Maria Joana\" maria_joanna_song | wc -l\ngrep \"Maria Joana\" maria_joanna_song | wc -w\ngrep \"Maria Joana\" maria_joanna_song | wc -c\nHow to deal with special characters\ngrep ( maria_joanna_song   #note the error\ngrep \"(\" maria_joanna_song\ngrep \\( maria_joanna_song\n\n\n\n“.” and “*” can represent any characters and therefore represent patterns rather than exact matches\ngrep Eu maria_joanna_song\ngrep Eu.* maria_joanna_song\nPlaying with multiple choices\ngrep se maria_joanna_song \ngrep se[i] maria_joanna_song \ngrep se[i,m] maria_joanna_song \nBut I am in love with Donald Trump and not with Maria Joana - how to I fix the song?\nsed 's/Maria Joana/Donald Trump/g'  maria_joanna_song | sed 's/Maria/Donald/g' &gt; donald_trump_song\nLet’s see what happened - now it doesn’t rhyme as good\ncat donald_trump_song\nmore donald_trump_song\nOk, but are we sure Donald replaced Maria?\ndiff -u *song \ndiff maria_joanna_song donald_trump_song\nvimdiff *song\n\n\n\n\n\n\nNote\n\n\n\nexit vim using Esc + :q which closes the file, alternatively Esc + :wq which saves the file.\n\n\n\n\n\nHave a look how a loop is being used here\ngrep Lisboa *song\nfor f in *song ; do grep Lisboa $f ; done\nTo show which file Lisboa is found in\nfor f in *song ; do grep -l Lisboa $f ; done\nTo show where is it found (i.e. line number)\nfor f in *song ; do grep -n Lisboa $f ; done\nI have just realised that “joanna” in portuguese is written as “joana” - how do I fix the name of my file\n\noption - fix it but keep the original\n\ncp maria_joanna_song maria_joana_song\n\n# list contents of directory\nls\n\n# other variants\nls -l\nls -la\n\noption - neh, I don’t need the previous version - it will just cause the confusion later on\n\nmv maria_joanna_song maria_joana_song\nNow let’s try previous code\ngrep Maria maria_joanna_song  # of course it doesn't work ... - what shall I do now ?",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#genomics",
    "href": "2025_spring/250416_atkacz.html#genomics",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Taken from official website:\nEDirect will run on Unix and Macintosh computers, and under the Cygwin Unix-emulation environment on Windows PCs. To install the EDirect software, open a terminal window and execute one of the following two commands:\n  sh -c \"$(curl -fsSL https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n\n  sh -c \"$(wget -q https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh -O -)\"\nThis will download a number of scripts and several precompiled programs into an “edirect” folder in the user’s home directory. It may then print an additional command for updating the PATH environment variable in the user’s configuration file. The editing instructions will look something like:\n  echo \"export PATH=\\$HOME/edirect:\\$PATH\" &gt;&gt; $HOME/.bash_profile\nAs a convenience, the installation process ends by offering to run the PATH update command for you. Answer “y” and press the Return key if you want it run. If the PATH is already set correctly, or if you prefer to make any editing changes manually, just press Return.\nOnce installation is complete, run:\n  export PATH=${HOME}/edirect:${PATH}\nto set the PATH for the current terminal session.\n\n\n\n\n\n\nWindows\n\n\n\nYou can try to install SRA-toolkit from here\n\n\n\n\n\nLet’s download some data from the Genbank - it is 16S V4 amplicon data (here the metadata)\nesearch -db sra -query PRJNA783372 | efetch -format runinfo &gt; SraRunTable.csv\nWe only need the SRRnumber, -f1 means first field.\ncut -f1 -d, SraRunTable*.csv  &gt; runs.txt\nTo save time we will work on four samples only\ntail -4 runs.txt &gt; runs4.txt\n\n\n\nOk, now we download the actual raw data\nprefetch --option-file runs4.txt\nIn case of\n\n\n\n\n\n\nNote\n\n\n\nIn case of Command 'prefetch' not found run\nsudo apt install sra-toolkit\n\n\nLet’s unpack it\nfor f in SRR* ; do fastq-dump --split-files  $f ; done\n\n\n\n\n\n\nTip\n\n\n\nTo visualize quality scores, small GUI tool fastqc could be used\n\n\n\n\n\nsudo apt install flash\nFirst merge the reads\n# merge\nflash -m 50 -M 500 -x 0.1 SRR17045222_1.fastq SRR17045222_2.fastq\n\n# rename merged file to index it\nmv out.extendedFrags.fastq file1.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045223_1.fastq SRR17045223_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045226_1.fastq SRR17045226_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nflash -m 50 -M 500 -x 0.1 SRR17045227_2.fastq SRR17045227_2.fastq\nmv out.extendedFrags.fastq file2.fastq\nNow we filter low quality reads (here the data is kind of a fake as the fastq download needs to be adjusted wit the prefetch script)\n# if not installed, do\nsudo apt install vsearch\n\nfor f in file* ; do vsearch --fastq_filter $f --fastq_maxee 1.0 --fastqout $f\\_filtered ; done\nIt is getting messy here (delete unwanted files)\nrm *notCombined*\nrm *hist\nrm *histogram\nrm *fastq\n\n\n\n\n\n\nTip\n\n\n\nToo lazy to empty the trash yourself ? the terminal will do that for us gio trash --empty\n\n\nLet’s convert fastq to fasta\n# if not installed, do\nsudo apt install seqtk\n\nfor f in *fastq_filtered ; do seqtk seq -a $f &gt; $f.fasta ; done ; \n\n\n\n\n\n\nCaution\n\n\n\nBlast is commented out as it is unlikely to work - we could try with internal database one day\n\n\nTo save time let’s just use 10 reads from each file for taxonomic identification - we will use GenBank nt database\nfor f in *fastq_filtered.fasta ; do head -20 $f &gt; $f\\head ; done\nSimplify the naming\n\n# if not installed, do\nsudo apt install rename\n\nrename 's/.fastq_filtered.fastahead/.fasta/g' *fastq_filtered.fastahead ;\n\n\n\nLet’s try - it may not work at all, first install\nsudo apt install ncbi-blast+\nblastn -query file1.fasta -db nt -remote -outfmt 6 -max_target_seqs 1 -perc_identity 95 &gt; file1_blast.txt \n\nblastn -query file1.fasta -db nr -remote  &gt; file1_blast2.txt\n\n\nUse the web-based blast, for example on the ncbi server.\nLet’s create ASV table and visualise the data\ncat file[0-9]*.fastq_filtered.fasta &gt; all_samples.fasta\n# if not installed, do\nsudo apt install vsearch\n\nvsearch --derep_fulllength all_samples.fasta   --output dereplicated.fasta  --sizeout  --relabel ASV_\nvsearch --sortbysize dereplicated.fasta  --output dereplicated_no_singletons.fasta  --minsize 2\nvsearch --cluster_unoise dereplicated_no_singletons.fasta --minsize 2  --unoise_alpha 2   --centroids ASVs.fasta\nvsearch --usearch_global all_samples.fasta --db ASVs.fasta   --id 0.97     --otutabout ASV_table.txt\nWe will use MicrobiomeAnalyst - every tool needs the input in some specific format - here we need to change the table as well\nsed 's/#OTU ID/#NAME/g' ASV_table.txt &gt; ASV_table_MA.txt\nWe need some metadata (all invented here)\nhead -1 ASV_table_MA.txt | fmt -1 | sed 's/NAME/NAME, sample/g'  | sed 's/11$/11,fish/g' | sed 's/13$/13,fish/g' | sed 's/21$/21,fish/g' | sed 's/22$/22,fish/g' |sed 's/23$/23,not_a_fish/g' | sed 's/24$/24,not_a_fish/g' |sed 's/25$/25,not_a_fish/g' | sed 's/27$/27,not_a_fish/g' &gt; metadata.csv",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#visualise-the-data-with-python",
    "href": "2025_spring/250416_atkacz.html#visualise-the-data-with-python",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "Following this link you can interrogate the same table (ASV_table_MA.txt) in the interactive jupyter notebook.\nTo compute Braycurtis distances \nTo visualize the PCoA",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#local-python-scripts",
    "href": "2025_spring/250416_atkacz.html#local-python-scripts",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "If you have python installed you can run scripts which are in this folder.\npython append_braycurtis3.py ASV_table_MA.txt &&\npython pcoa_with_metadata_quick_legend.py",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250416_atkacz.html#finally",
    "href": "2025_spring/250416_atkacz.html#finally",
    "title": "Simple Linux – An Introduction to Genomic Analysis on Linux",
    "section": "",
    "text": "The power of command line lies in the fact, that once you test your pipeline command by command, you can copy all the commands into a so-called bash script and rerun the whole thing at once.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nYou may want to create a separate new virtual environment (example uses conda) and install necessary dependencies first\n# create virtual environment\nconda create -n gen-test python\n\n# activate it\nconda activate gen-test\n\n# install deps (pip or conda install)\npip install dask\npip install --upgrade --force-reinstall scipy dask\npip install pyarrow\n\npython3 append_braycurtis3.py ASV_table_MA.txt\n\n# dependencies for the second script\npip install numpy==1.24.3\npip install seaborn\npip install panel\npip install scikit-bio\n\npython3 pcoa_with_metadata_quick_work.py \n\n\n\n# make a new folder for a test\nmkdir pipeline_test\n\n# copy python scripts too, if you want to visualize the results.\ncp *.py ./pipeline_test\n\n# take the commands from the command list file\ngrep -A 1000 \"genomics part\" course_linux_commands | sed '/fastqc/d' | sed '/\\*\\*\\*/d' &gt; ./pipeline_test/my_first_pipeline.sh \nRun the pipeline with\nbash my_fist_pipeline.sh\n\n\n\n\n\n\nCaution\n\n\n\nYou might need to make the script executable\nchmod +x my_fist_pipeline.sh\n\n# then you can run directly\n./my_fist_pipeline.sh",
    "crumbs": [
      "Spring 2025",
      "Simple Linux – An Introduction to Genomic Analysis on Linux"
    ]
  },
  {
    "objectID": "2025_spring/250604_iduarte/250604_iduarte.html",
    "href": "2025_spring/250604_iduarte/250604_iduarte.html",
    "title": "RNA-Seq Data Analysis in R | From Counts to Biological Insights",
    "section": "",
    "text": "Welcome! This crash course walks you through RNA-seq data analysis in R: From raw count matrices to functional interpretation of differentially expressed genes.\n\n\n\nPerform quality control of the count data.\nIdentify the R commands needed to run a differential expression analysis using DESeq2.\nVisualize the DE results.\nPerform functional enrichment of the DE genes.\nVisualize functionally enriched categories.\n\n\n\n\nAttribution | These course materials are freely adapted from a combination between the following resources:\n\nnf-core RNA-seq tutorial: nf-core/rnaseq tutorial\nThe Carpentries Incubator guided tutorial: RNA-seq analysis with Bioconductor\nThe Harvard Chan Bioinformatics Core tutorial: Differential Gene Expression Analysis (bulk RNA-seq)\n\nNewer versions of this tutorial based on different dataset can be found here.",
    "crumbs": [
      "Spring 2025",
      "RNA-Seq Data Analysis in R | From Counts to Biological Insights"
    ]
  },
  {
    "objectID": "2025_spring/250604_iduarte/250604_iduarte.html#rna-seq-analysis-crash-course",
    "href": "2025_spring/250604_iduarte/250604_iduarte.html#rna-seq-analysis-crash-course",
    "title": "RNA-Seq Data Analysis in R | From Counts to Biological Insights",
    "section": "",
    "text": "Welcome! This crash course walks you through RNA-seq data analysis in R: From raw count matrices to functional interpretation of differentially expressed genes.\n\n\n\nPerform quality control of the count data.\nIdentify the R commands needed to run a differential expression analysis using DESeq2.\nVisualize the DE results.\nPerform functional enrichment of the DE genes.\nVisualize functionally enriched categories.\n\n\n\n\nAttribution | These course materials are freely adapted from a combination between the following resources:\n\nnf-core RNA-seq tutorial: nf-core/rnaseq tutorial\nThe Carpentries Incubator guided tutorial: RNA-seq analysis with Bioconductor\nThe Harvard Chan Bioinformatics Core tutorial: Differential Gene Expression Analysis (bulk RNA-seq)\n\nNewer versions of this tutorial based on different dataset can be found here.",
    "crumbs": [
      "Spring 2025",
      "RNA-Seq Data Analysis in R | From Counts to Biological Insights"
    ]
  },
  {
    "objectID": "2025_spring/250604_iduarte/250604_iduarte.html#hands-on-tutorial",
    "href": "2025_spring/250604_iduarte/250604_iduarte.html#hands-on-tutorial",
    "title": "RNA-Seq Data Analysis in R | From Counts to Biological Insights",
    "section": "Hands-on tutorial",
    "text": "Hands-on tutorial\n\nIntroductionTutorial\n\n\nRNA-seq is a leading method for quantifying RNA levels in biological samples, leveraging next-generation sequencing (NGS) technologies. The process begins with RNA extraction and conversion to cDNA, followed by sequencing to produce reads representing the RNA present in a sample.\n\nRNA-seq overviewDifferential expressionFunctional analysis\n\n\nLab Protocol Overview\n\n\n\nRNA-seq data (i.e. reads) are processed through a standard workflow with three main stages:\n\nData pre-processing – improves read quality by removing contaminants and adapters.\nAlignment and quantification – maps reads to a reference genome and estimates gene expression, either through traditional or faster lightweight methods.\nDifferential expression analysis – identifies and visualizes genes with significant expression differences.\n\nAdditional downstream analyses (e.g., functional enrichment, co-expression, or multi-omics integration) are popular ways to derive biological insights from these analyses.\n\n\n\nAdapted from: https://nf-co.re/rnaseq/dev/docs/usage/differential_expression_analysis/theory\n\n\nNote | As shown in the above scheme, this course will not cover the first two steps. It will begin with a gene count matrix and proceed with differential expression analysis, visualization, and a brief overview of functional enrichment.\n\n\nDifferential expression (DE) analysis compares gene expression levels across conditions (e.g., disease vs. healthy) to identify genes with statistically significant changes. This is typically done using tools like DESeq2, a robust R package designed for analyzing RNA-seq count data.\n\nInput Requirements:\n\nA count matrix (genes × samples).\nA metadata table describing sample attributes.\n\nQuality Control:\n\nUse PCA and hierarchical clustering to explore variation and detect outliers.\nTransform counts using variance stabilizing transformation (vst) or regularized log (rlog) to ensure comparable variance across genes, improving downstream analysis.\n\nFiltering:\n\nRemove genes with low or zero counts to improve sensitivity and reduce false positives.\n\nDesign Formula:\n\nSpecifies how gene counts depend on experimental factors.\nCan include main conditions and covariates (e.g., gender, batch, stage).\nExample:\ndesign = ~ condition\ndesign = ~ gender + developmental_stage + condition\nThe main factor of interest is usually placed last for clarity.\n\n\n\nDE with DESeq2\nDESeq2 is a widely used R package for identifying differentially expressed (DE) genes from RNA-seq count data. RNA-seq data typically exhibit many low-count genes and a long-tailed distribution due to highly expressed genes, requiring specialized statistical modeling. The major steps in DESeq2 are the following:\n\nNormalization\n\nAdjusts for sequencing depth and RNA composition using size factors calculated via the median ratio method.\nNormalized counts are used for visualization but raw counts must be used for DESeq2 modeling.\n\nDispersion Estimation\n\nRNA-seq data show overdispersion (variance &gt; mean).\nDESeq2 models count data using the negative binomial distribution.\nDispersion is estimated:\n\nGlobally (common dispersion),\nPer gene (gene-wise dispersion),\nThen refined through shrinkage toward a fitted mean-dispersion curve to improve stability, especially with small sample sizes.\n\nGenes with extreme variability are not shrunk to avoid false positives.\n\nModel Fitting and Hypothesis Testing\n\nA generalized linear model (GLM) is fit to each gene’s normalized counts.\nDESeq2 tests whether gene expression differs significantly between groups:\n\nWald test for simple comparisons (e.g., treated vs. control),\nLikelihood Ratio Test (LRT) for more complex designs with multiple variables.\n\nEach test returns a log2 fold change and a p-value.\n\nMultiple Testing Correction\n\nTo control for false positives from testing thousands of genes, DESeq2 adjusts p-values using Benjamini-Hochberg FDR correction.\nAn FDR cutoff of &lt;0.05 means that 5% of DE genes may be false positives.\n\n\n\n\n\nAfter identifying differentially expressed (DE) genes, functional analysis helps interpret their biological relevance by uncovering the pathways, processes, or interactions they may be involved in. This includes:\n\nFunctional enrichment analysis – identifies overrepresented biological processes, molecular functions, cellular components, or pathways.\nNetwork analysis – groups genes with similar expression patterns to reveal potential interactions.\n\nThis course focuses on Over-Representation Analysis (ORA), a common enrichment method that uses the hypergeometric test to assess whether certain biological pathways or gene sets are statistically enriched in the DE gene list.\nKey components of ORA:\n\nUniverse – the full set of genes considered (e.g., all genes in the genome).\nGene Set – a group of genes annotated to a particular function or pathway (e.g., from Gene Ontology).\nGene List – the list of DE genes identified in the analysis.\n\nThe test evaluates whether the overlap between the DE gene list and a gene set exceeds what would be expected by chance, pointing to potentially meaningful biological mechanisms.\nTools commonly used for functional enrichment include Gene Ontology, KEGG, Reactome, clusterProfiler, and g:Profiler. These support the biological interpretation of DE results and help uncover pathways affected by the experimental condition.\n\n\n\n\n\n\n\nDifferential Expression Analysis with DESeq2\nIn this tutorial, we will guide you through the practical steps necessary to set up the RStudio project, load the required packages and data, execute the DESeq2 analysis, and derive biological insights from the DE results.\n\n\n\n0. RStudio Proj1. Setup2. Quality control3. Differential Expression4. Visualise DE results5. Functional analysis\n\n\nAs with any analysis, the first step is to create a folder to store your work.\n\nChoose an appropriate location on your computer, then set up the following folder structure:\n\n  rnaseq_counts2bio_course/    \n  ├── data/    \n  ├── de_results/    \n  ├── scripts/    \n  └── sandbox/   \n\n\n\n\n\n\nTip: Set Up a Clear Folder Structure Early\n\n\n\n\n\nBefore collecting data, define a clear folder structure and file naming convention. This improves organization, avoids confusion, and supports collaboration. A consistent setup helps you and your team quickly locate and understand files.\nStart organized - your future self (and collaborators) will thank you!\nSuggested minimal structure for a data analysis project:\nproject_name_no_spaces_no_special_chars/    \n├── data/            # Raw and processed data\n│   ├── processed/         \n│   └── raw/   \n├── output/          # Figures, tables \n├── results/         # Analysis results, with appropriate sub-folders\n├── scripts/         # Analysis and processing code\n└── sandbox/         # Exploratory work (not for sharing)\n\n\n\n\nCreate a new RStudio project inside the rnaseq_counts2bio_course folder:\n\n\n2.1 Go to the File menu and select New Project;\n2.2 Select Existing Directory;\n2.3 Navigate to the course directory rnaseq_counts2bio_course and click on Create Project;\n2.4 The new project will be automatically opened in RStudio, and inherits the directory name.\n\n\nWe can check whether we are in the correct working directory with getwd().\n\nNext, go to the File menu, select New File and then R Markdown to create a notebook style script file, using literate programming, in which we will save all the R code required for this analysis.\n\n\n3.1 In the Title write: Differential expression analysis with DESeq2, choose HTML as Default Output Format, and insert the author name.\n3.2 Save the file as de_analysis.Rmd inside the scripts folder.\n3.3 Delete the example markdown code, except the YAML header (the first lines between ---), and the setup code chunk.\n\nFrom now on, each command described in the course will be added to this script.\n\n\n\n\nOur case study\n\n\n\n\nArtificial Gravity Attenuates the Transcriptomic Response to Spaceflight in the Optic Nerve and Retina\n\nProlonged exposure to microgravity in space poses risks to eye health. To explore a potential countermeasure, researchers exposed mice on the International Space Station to varying levels of artificial gravity (0, 0.33, 0.67, and 1G) using centrifugation. After returning the mice to Earth, RNA-seq of their optic nerve and retina revealed that microgravity triggers gene expression changes. Adding artificial gravity on board the ISS can attenuate the transcriptomic response to microgravity in a dose-dependent manner. Such attenuation may effectively mitigate spaceflight-induced detrimental effects on ocular tissue.\n\nData repository: NASA Open Science for Life in Space\nDOI: https://doi.org/10.26030/d6dj-d777\nExperimental factors: Spaceflight, Altered gravity\nOrganism: Mus musculus (mouse)\nAssay: transcription profiling\n\nTechnology: RNA Sequencing (RNA-Seq)\nPlatform: Illumina\n\n\n\n\n\n\n\n\nHands-on tutorial\n\n\n\n1.1 Load packages and data\nIf you are a Windows user, please install first RTools which match your exact R version.\n\n\nCode\n# # Install the required packages if not already installed\n# install.packages(c(\"pak\"))\n# \n# pak::pkg_install(c(\"BiocManager\", \"remotes\", \"here\", \"tidyverse\",        \n# \"DESeq2\", \"pheatmap\", \"RColorBrewer\", \"ggrepel\", \"clusterProfiler\",\n# \"enrichplot\", \"org.Mm.eg.db\", \"patchwork\", \"ComplexHeatmap\"\n# ))\n# \n# # Install the course data package\n# pak::pak(\"patterninstitute/OSD758\")\n\n\n# Load packages\nlibrary(\"here\")            # package to find your current working directory\nlibrary(\"tidyverse\")       # packages for data manipulation and visualization\nlibrary(\"DESeq2\")          # differential expression analysis\nlibrary(\"pheatmap\")        # heatmaps\nlibrary(\"RColorBrewer\")    # color palettes\nlibrary(\"ggrepel\")         # repel overlapping text labels in ggplot2 plots\nlibrary(\"clusterProfiler\") # for enrichment analysis\nlibrary(\"enrichplot\")      # to draw functional enrichment results\nlibrary(\"org.Mm.eg.db\")    # mouse gene annotation database\nlibrary(\"patchwork\")         # combining multiple plots\nlibrary(\"ComplexHeatmap\")  # to draw heatmaps\n\n# Install and load package containing the data\nlibrary(OSD758)\n\n# Gene expression in Counts\nraw_counts &lt;- OSD758::gene_expression(format = \"wide\", only_expressed_genes = TRUE) \n# View(raw_counts)\n\n# Samples metadata\nsamples &lt;- OSD758::samples()\n# View(samples)\n\n\n\n\n\n\n\nThe first step in any data analysis pipeline is quality control (QC) to check for data issues, and ensure the data is suitable for downstream analyses.\n\n\n2.1 Variance stabilization data transformation\nFor QC analysis, it is useful to work with transformed versions of the count data, variance-stabilised (vst) or regularised log-transformed (rlog) counts. While, the rlog is more robust to outliers and extreme values, vst is computationally faster and so preferred for larger datasets.\n\n\n\n\n\n\nInfo: The rlog() and the vst() functions from DESeq2\n\n\n\n\n\nThe rlog and the vst transformations have an argument, blind that can be set to:\n\nTRUE (default): useful for QC analysis because it re-estimates the dispersion, allowing for comparison of samples in an unbiased manner with respect to experimental conditions;\nFALSE: the function utilizes the already estimated dispersion, generally applied when differences in counts are expected to be due to the experimental design.\n\n\n\n\n\n\n\n\n\n\nAttention: vst and rlog Transformations are Used for Visualization Only\n\n\n\n\n\nVariance stabilization transformations are used for visualisation purposes only. Differential expression analysis using DESeq2 requires raw, unnormalized counts (not TPMs, RPKMs, or FPKMs).\n\n\n\n\n\nCode\n# Create a list to save the QC results\nqc &lt;- list()\n\n# You can choose between vst() and rlog() - this tutorial uses vst.\nqc$vst &lt;- DESeq2::vst(raw_counts, blind = TRUE)\n\n\n\n\n2.2 Principal Component Analysis\nCheck which variables from the experimental conditions are the major source of variation.\n\n\nCode\n# Run PCA\nqc$pca_vst &lt;- prcomp(t(qc$vst)) \n\n# Extract the components\nqc$components &lt;- qc$pca_vst[[\"x\"]]\nqc$components &lt;- tibble::as_tibble(qc$components, rownames = \"sample_id\")\n\n# Add sample annotations to components for plot coloring\nqc$components_annot &lt;-\n  dplyr::left_join(qc$components, as.data.frame(samples[, c(1,5,6,8)]), by = \"sample_id\") |&gt;\n  dplyr::relocate(spacecraft, acceleration_source, gravity_class, .after = sample_id)\n\n# Calculate the % variance per component\nqc$pca_percent_var &lt;- round(qc$pca_vst$sdev^2/sum(qc$pca_vst$sdev^2)*100)\n\n#\n# 2D PCA | Using ggplot2\n#\n\n# Color by gravity_class\nqc$pca_gravity &lt;-\n  ggplot(qc$components_annot, aes(x = PC1, y = PC2, color = gravity_class)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"PCA gene expression | Colored by gravity_class\",\n    x = paste0(\"PC1 (\", qc$pca_percent_var[1], \"% variance)\"),\n    y = paste0(\"PC2 (\", qc$pca_percent_var[2], \"% variance)\")\n  ) +\n  theme_minimal()\n\n# Color by accelaration_source\nqc$pca_acceleration &lt;-\n  ggplot(qc$components_annot, aes(x = PC1, y = PC2, color = acceleration_source)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"PCA gene expression | Colored by acceleration_source\",\n    x = paste0(\"PC1 (\", qc$pca_percent_var[1], \"% variance)\"),\n    y = paste0(\"PC2 (\", qc$pca_percent_var[2], \"% variance)\")\n  ) +\n  theme_minimal()\n\n# Color by gravity_class and shape by acceleration_source\nqc$pca_gravity_acceleration &lt;-\n  ggplot(qc$components_annot, aes(x = PC1, y = PC2, \n                               color = gravity_class,\n                               shape = acceleration_source)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"PCA gene expression | Colored by gravity_class | Shape acceleration_source\",\n    x = paste0(\"PC1 (\", qc$pca_percent_var[1], \"% variance)\"),\n    y = paste0(\"PC2 (\", qc$pca_percent_var[2], \"% variance)\")\n  ) +\n  theme_minimal()\n\n\n# Assemble pca plots\nqc$pca_gravity_acceleration /\n(qc$pca_gravity | qc$pca_acceleration)\n\n\n\n\n\n\n\n\n\n\n\n2.3 Hierarchical clustering\nCheck how similar the replicates are to each other.\n\nDistance between samples (in gene expression space) - Euclidean distance.\n\n\n\nCode\n# Plot sample to sample distance for hierarchical clustering\n\n# Calculate Euclidean distances between samples (rows) by transposing the matrix with t().\nqc$sample_dist_matrix &lt;- as.matrix(dist(t(qc$vst), method = \"euclidean\"))\n\n\n# Define a color palette for the heatmap\nqc$colors &lt;- colorRampPalette(rev(brewer.pal(9, \"Greens\")))(255) # function from RColorBrewer package\n\n# Create the heatmap\nqc$dist_clustering &lt;- pheatmap::pheatmap(\n  qc$sample_dist_matrix,\n  cluster_rows = TRUE,\n  cluster_cols = TRUE,\n  col = qc$colors,\n  fontsize_col = 8,\n  fontsize_row = 5\n)\n\n\n\n\n\n\n\n\n\n\nCorrelation between samples.\n\n\n\nCode\n### Compute pairwise correlation values\nqc$sample_corr &lt;- cor(qc$vst)\n\n### Plot heatmap using the correlation matrix\nqc$corr_clustering &lt;-\n  pheatmap::pheatmap(\n    qc$sample_corr,\n    cluster_rows = TRUE,\n    cluster_cols = TRUE,\n    fontsize_row = 5,\n    fontsize_col = 8\n  )\n\n\n\n\n\n\n\n\n\n\n\n3.1 Check if the data and metadata sample ids match\nTo avoid errors in DESeq2 is essential to check that sample names match between the colData and the countData, and that the samples are in the exact same order.\n\n\nCode\n# Create list to save the analysis objects\nde_deseq &lt;- list()\n\n# Check that sample ids match between raw_counts and samples \n# Ensure same content\nstopifnot(setequal(colnames(raw_counts), samples$sample_id))\n\n# Reorder columns to match sample order\nraw_counts &lt;- raw_counts[, samples$sample_id]\n\n\n\n\n3.2 Differential Expression with DESeq2\nThe calculation of the differential expression using DESeq2 requires raw (unnormalized) integer counts, a sample metadata table with experimental conditions, and a design formula specifying the variables for model fitting.\nThis will generate a dds object.\n\n\nCode\n# Make sure the factor levels are ordered so that the desired baseline comes first.\n# DESeq2 uses the first level from factors as the baseline.\nsamples &lt;-\n  samples |&gt;\n  dplyr::mutate(gravity_class = factor(\n    gravity_class,\n    levels = c(\"1.00_G\", \"0.33_G\", \"0.66_G\", \"micro_G\")\n  ))\n\n\n# DE Step 1: Create a DESeqDataSet object (dds)\nde_deseq$dds &lt;- DESeq2::DESeqDataSetFromMatrix(\n  countData = raw_counts,\n  colData = samples,\n  design = ~ gravity_class\n)\n\n# DE Step 2: Run the DESeq function to perform the analysis\nde_deseq$dds &lt;- DESeq(de_deseq$dds)\n\n\n\n\n\n\n\n\nOptional: DESeq() function can be separated into individual steps\n\n\n\n\n\nThe DESeq() function is a high-level wrapper that simplifies the process of differential expression analysis by combining multiple steps into a single function call. This makes the workflow more user-friendly and ensures that all necessary pre-processing and statistical steps are executed in the correct order. The key functions that DESeq2 calls include:\n\nestimateSizeFactors: to normalise the count data;\nestimateDispersion: to estimate the dispersion;\nnbinomWaldTest: to perform differential expression test.\n\nThe individual functions can be carried out also singularly as shown below:\n\n# Differential expression analysis step-by-step\nde_deseq$dds_stepwise &lt;- DESeq2::estimateSizeFactors(de_deseq$dds)\n\nde_deseq$dds_stepwise &lt;- DESeq2::estimateDispersions(de_deseq$dds_stepwise)\n\nde_deseq$dds_stepwise &lt;- DESeq2::nbinomWaldTest(de_deseq$dds_stepwise)\n\n\n\n\n\n\n\n\n\nOptional: Pre-filtering of low count genes is sometimes performed\n\n\n\n\n\nBefore running the different steps of the analysis, sometimes its is advisable to pre-filter the genes to remove those with very low counts. This is useful to improve computational efficiency and enhance interpretability. In general, it is reasonable to keep only genes with sum counts of at least 10 for a minimal number of 3 samples. Here is the optional code.\n# Pre-filtering\n\n# Select a minimal number of samples = 3\nsmallestGroupSize &lt;- 3\n\n# Select genes with sum counts of at least 10 in 3 samples\nkeep &lt;- rowSums(counts(de_deseq$dds) &gt;= 10) &gt;= smallestGroupSize\n\n# Keep only the genes that pass the threshold\nde_deseq$dds_filtered &lt;- de_deseq$dds[keep,]\n\n\n\n\n\n3.3 Inspect the dds object\n\n\n\n\n\n\nInfo: dds object from DESeq2\n\n\n\n\n\nIn DESEq2, the dds object is a central data structure that contains the following components:\n\ncountData: a matrix of raw count data, where each row represents a gene and each column represents a sample;\ncolData: a data frame containing information about the samples, such as the experimental design, treatment and other relevant metadata;\ndesign: a formula specifying the experimental design used to estimate the dispersion and the log2 fold change.\n\n\n\n\n\n\nCode\n# Check the design formula\nDESeq2::design(de_deseq$dds) \n\n# Check the sample info\nSummarizedExperiment::colData(de_deseq$dds) \n\n# Display the first rows of the raw counts\nhead(DESeq2::counts(de_deseq$dds))\n\n# Display the first rows of the normalised counts to compare with raw counts \nhead(DESeq2::counts(de_deseq$dds, normalized = TRUE))\n\n# Convert the normalised counts from the DESeq2 object to a tibble\nnormalised_counts &lt;- tibble::as_tibble(DESeq2::counts(de_deseq$dds, normalized = TRUE),\n                                       rownames = \"ensembl_gen_id\")\nhead(normalised_counts)\n\n\n\n\n3.4 Extract the Differential Expression results\n\n\n\n\n\n\nInfo: The results() function from DESeq2\n\n\n\n\n\nThe results() function in DESeq2 is used to extract the results of the DE analysis. This function takes the dds object as input and returns a DataFrame containing the results of the analysis:\n\nbaseMean: the average expression level of the gene across all samples;\nlog2FoldChange: the log2 fold change of the gene between the condition of interest and the reference level;\nlfcSE: the standard error of the log2 fold change;\nstat: the Wald statistic, which is used to calculate the p-value;\npvalue: the p-value from the Wald test indicates the probability of observing the measured difference in gene expression (log2 fold change) by chance, assuming no true difference exists (null hypothesis). A low p-value suggests that the observed expression change between samples is unlikely due to random chance, so we can reject the null hypothesis –&gt; the gene is differentially expressed;\npadj: the adjusted p-value, which takes into account multiple testing corrections, (Benjamini-Hochberg method default) to control the false discovery rate.\n\nThe results() function returns the results for all genes in the analysis with an adjusted p-value below a specific FDR cutoff, set by default to 0.1. This threshold can be modified with the parameter alpha. The results() function can also be customised to filter the results based on certain criteria (log2 fold change or padj) or to set a specific contrast (specific comparison between two or more levels).\n\n\n\n\n\nCode\n# Find the names of the estimated effects (coefficients) of the model\nDESeq2::resultsNames(de_deseq$dds)\n\n# Extract DE results for each gravity condition vs 1.00 G\n    # The results function by default applies the Benjamini-Hochberg method to control FDR\nde_deseq$res_033_vs_1G &lt;- DESeq2::results(de_deseq$dds, name = \"gravity_class_0.33_G_vs_1.00_G\")\nde_deseq$res_066_vs_1G &lt;- DESeq2::results(de_deseq$dds, name = \"gravity_class_0.66_G_vs_1.00_G\")\nde_deseq$res_micro_vs_1G &lt;- DESeq2::results(de_deseq$dds, name = \"gravity_class_micro_G_vs_1.00_G\")\n\n\n# Summarise the results:\n  # Shows the number of tested genes, the number up- and down-regulated (at alpha),\n  # and how many were excluded by multiple testing due to low counts.\nDESeq2::summary(de_deseq$res_033_vs_1G)\nDESeq2::summary(de_deseq$res_066_vs_1G)\nDESeq2::summary(de_deseq$res_micro_vs_1G)\n\n\n\n\n\n\n\n\nImportant: Extracting results from contrasts\n\n\n\n\n\nWhen more than one variable is used in the design formula, and you want to manually specify the comparison of interest, you should run the following command:\nmy_results &lt;- DESeq2::results(dds, contrast = c(\"variable_name\", \"condition_of_interest\", \"reference_condition\"))\nExample:\nIf the design is ~ tissue + condition, and you want to compare the levels \"treated\" vs \"control\" within the variable \"condition\": my_results &lt;- DESeq2::results(dds, contrast = c(\"condition\", \"treated\", \"control\"))\nThis will extract the log2 fold change of \"treated\" relative to \"control\", controlling for the effect of the other variables in the design.\n\n\n\n\n\n3.5 Select significant DE results\n\n\n\n\n\n\nInfo: The Order of the Contrasts Determines the Fold Change Signal\n\n\n\n\n\nThe order of the contrast names determines the direction of the fold change that is reported in the results. Specifically, the first level of the contrast is the condition of interest and the second level is the reference level.\n\n\n\n\n\nCode\n# Extract significant results (padj &lt; 0.05) and convert to tibble\nde_deseq$sig_033_vs_1G &lt;-\n  de_deseq$res_033_vs_1G |&gt;\n  tibble::as_tibble(rownames = \"ensembl_gen_id\") |&gt;\n  dplyr::filter(!is.na(padj), padj &lt; 0.05) |&gt;\n  dplyr::arrange(padj, log2FoldChange)\n\nde_deseq$sig_066_vs_1G &lt;-\n  de_deseq$res_066_vs_1G |&gt;\n  tibble::as_tibble(rownames = \"ensembl_gen_id\") |&gt;\n  dplyr::filter(!is.na(padj), padj &lt; 0.05) |&gt;\n  dplyr::arrange(padj, log2FoldChange)\n\nde_deseq$sig_micro_vs_1G &lt;-\n  de_deseq$res_micro_vs_1G |&gt;\n  tibble::as_tibble(rownames = \"ensembl_gen_id\") |&gt;\n  dplyr::filter(!is.na(padj), padj &lt; 0.05) |&gt;\n  dplyr::arrange(padj, log2FoldChange)\n\n# Look at the top results\nhead(de_deseq$sig_033_vs_1G)\nhead(de_deseq$sig_066_vs_1G)\nhead(de_deseq$sig_micro_vs_1G)\n\n\n\n\nAfter differential expression analysis, the next step is to visualize the data. This helps reveal patterns not obvious from the raw numbers.\nThe next sections show common plots used in RNA-seq analysis.\nDue to time constraints, we will focus on the differential expression between microgravity and Earth’s gravity (1G).\n\n\n\n\n\n\nOptional: MA plots and counts plots\n\n\n\n\n\n\nMA plot: scatter plot commonly utilised to visualise the results of the DE analysis for all the samples. The plot displays the mean of the normalised counts on the x-axis and the log2 fold change on the y-axis. This allows the visualisation of the relationship between the magnitude of the fold change and the mean expression level of the genes. Genes that are differentially expressed will appear farthest from the horizontal line, while genes with low expression levels will appear closer to the line.\ncounts plot: plot of the normalised counts for a single gene across the different conditions in your experiment. It’s particularly useful for visualising the expression levels of specific genes of interest and comparing them across sample groups.\n\n# Generate the MA plot\nplotMA(res, ylim = c(-2, 2))\n\n# Plot a specific gene in this case Xist, a DE gene\n# plotCounts(de_deseq$dds, gene = \"Xist\")\n\n\n\n\n\n4.1 Heatmap\n\n\n\n\n\n\nInfo: Heatmaps\n\n\n\n\n\nHeatmaps plot of the normalised counts for all the significant genes. The heatmap provides insights into genes and sample relationships that may not be apparent from individual gene plots alone.\n\n\n\n\n\nCode\n# List to save all the visualization plots\nde_plots &lt;- list()\n\n# Extract only gene ids from the significant results\nsig_gene_ids &lt;- de_deseq$sig_micro_vs_1G$ensembl_gen_id\n\n# Map between ENSEMBL gene ids and gene symbol\nensembl2symbol &lt;- OSD758::gene_expression(\"long\") |&gt;\n  dplyr::select(ensembl_gen_id, gene_symbol) |&gt;\n  dplyr::distinct()\n  \n# Get normalised counts for significant genes \nsig_normalised_counts &lt;- normalised_counts |&gt;\n  dplyr::filter(ensembl_gen_id %in% sig_gene_ids) |&gt;\n  dplyr::left_join(ensembl2symbol, by = \"ensembl_gen_id\") |&gt;\n  dplyr::select(-ensembl_gen_id) |&gt;\n  tibble::column_to_rownames(\"gene_symbol\") |&gt;\n  as.matrix()\n\n\n# Scale each row: subtract mean and divide by SD.\n# The 2 transpositions are required because, by default, scale applies to the columns.\nsig_normalised_counts_scaled &lt;- t(scale(t(sig_normalised_counts)))   # scale rows, not columns\n\n# Find min and max values to get meaningful colors in heatmaps\nrange(sig_normalised_counts_scaled)\n\n\n[1] -3.324583  5.724152\n\n\nCode\n# Complex heatmap\nde_plots$ht &lt;- ComplexHeatmap::Heatmap(sig_normalised_counts_scaled[1:200, ],\n                        name = \"Exprs (z-score)\",\n                        column_title = \"Microgravity vs Earth's Gravity (1G) | Top 200 DE genes\",\n                        cluster_columns = TRUE,\n                        cluster_rows = TRUE,\n                        # number of clusters in K-means to split rows\n                        row_km = 2,\n                        # add cluster names\n                        row_title = c(\"A\", \"B\"),\n                        row_title_rot = 90,\n                        row_gap = unit(2, \"mm\"),\n                        # number of clusters in K-means to split columns\n                        column_km = 3,\n                        column_gap = unit(2, \"mm\"),\n                        border = \"grey\",\n                        na_col = \"white\",\n                        # Color range (min and max values from sig_normalised_counts_scaled)\n                        col = circlize::colorRamp2(c(-4, 0, 6), c(\"skyblue3\", \"white\", \"forestgreen\")),\n                        column_names_gp = grid::gpar(fontsize = 9),\n                        row_names_gp = grid::gpar(fontsize = 5),\n                        rect_gp = grid::gpar(col = \"grey\", lwd = 0.5))\n\n# Print the plot\nComplexHeatmap::draw(de_plots$ht, heatmap_legend_side = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n4.2 Volcano plot\n\n\n\n\n\n\nInfo: Volcano plots\n\n\n\n\n\nVolcano plots scatter plot that displays the log2 fold change on the x-axis and the log transformed padj on the y-axis. This allows for the visualisation of both the magnitude and significance of the changes in gene expression between two conditions. Genes that are differentially expressed (i.e., have a large log2 fold change) and are statistically significant (i.e., have a low padj) will appear in the left (downregulated genes) or in the right (upregulated genes) corners of the plot making easier their identification.\n\n\n\n\n\nCode\n# Add a column with differential expression status and add gene symbol to the results\nsig_res_annot &lt;- \n  de_deseq$sig_micro_vs_1G |&gt;\n  dplyr::mutate(diffexpressed = case_when(\n    log2FoldChange &gt; 1 & padj &lt; 0.05 ~ 'upregulated',\n    log2FoldChange &lt; -1 & padj &lt; 0.05 ~ 'downregulated',\n    TRUE ~ 'not_de')) |&gt;\n  dplyr::left_join(ensembl2symbol, by = \"ensembl_gen_id\") |&gt;\n  dplyr::select(-ensembl_gen_id) |&gt;\n  # add gene symbols\n  dplyr::relocate(gene_symbol, .before =  1L) |&gt;\n  dplyr::arrange(padj, log2FoldChange)\n\n\n# Create a volcano plot using ggplot2\nde_plots$volcano_plot &lt;-\n  ggplot(data = sig_res_annot, aes(\n    x = log2FoldChange,\n    y = -log10(padj),\n    col = diffexpressed))+\n  geom_point(size = 0.6) +\n  geom_text_repel(data = filter(sig_res_annot, \n                                ((abs(log2FoldChange) &gt; log2(8)) & (padj &lt; -log10(0.05)))), \n                  aes(label = gene_symbol), size = 2.5, max.overlaps = Inf) +\n  ggtitle(\"DE genes micro gravity versus Earth's gravity\") +\n  geom_vline(xintercept = c(-1, 1), col = \"black\", linetype = 'dashed', linewidth = 0.2) +\n  geom_hline(yintercept = -log10(0.05), col = \"black\", linetype = 'dashed', linewidth = 0.2) +\n  theme(plot.title = element_text(size = rel(1.25), hjust = 0.5),\n        axis.title = element_text(size = rel(1))) +\n  scale_color_manual(values = c(\"upregulated\" = \"red\",\n                                \"downregulated\" = \"blue\",\n                                \"not_de\" = \"grey\")) +\n  labs(color = 'DE genes') +\n  xlim(-5, 5) +   # Caution: This hides some genes\n  ylim(0, 7.5) +  # Caution: This hides some genes\n  theme_light()\n\n# Print the volcano plot\nde_plots$volcano_plot\n\n\n\n\n\n\n\n\n\n\n\nDifferential expression analysis yields a list of significant DE genes, which can be explored further through downstream analyses like functional enrichment and network analysis to uncover biological mechanisms.\nThis tutorial focuses on Over-Representation Analysis (ORA), a method for identifying enriched pathways or processes among DE genes.\n\n\n\n\n\n\nInfo: The hypergeometric test in ORA\n\n\n\n\n\nThe underlying statistic behind ORA is the hypergeometric test, which considers three key components:\n\nUniverse: the background list of genes (for example the genes annotated in a genome);\nGeneSet: a collection of genes annotated by a reference database (such as Gene Ontology), and known to be involved in a particular biological pathway or process;\nGene List: the differentially expressed genes.\n\nThe hypergeometric test calculates the probability of observing a certain number of genes from the gene set (pathway or process) within the gene list (DE genes) by chance. An important aspect of this analysis is the concept of membership. It defines the relationship between DE genes and genes from the analysed gene set. By knowing which genes belong to which pathway/process, we can determine whether the observed overlap between DE genes and the particular pathway/process is greater than what would be expected by random chance.\n\n\n\n\n\n5.1 Enrichment analysis\n\n\nCode\n# Enrichment analysis (ORA)\n\n# Create a list to save the enrichment analysis results\nfun_enrich &lt;- list()\n\n# Prepare list of significant DE genes in descending Log2FoldChange\nfun_enrich$de_genes_fc &lt;-\n  de_deseq$sig_micro_vs_1G |&gt;\n  dplyr::select(ensembl_gen_id, log2FoldChange) |&gt;\n  dplyr::arrange(dplyr::desc(log2FoldChange))\n\n# Run GO enrichment analysis using the enrichGO function\nfun_enrich$ego &lt;- clusterProfiler::enrichGO(\n  gene = fun_enrich$de_genes_fc$ensembl_gen_id, # Genes of interest\n  universe = ensembl2symbol$ensembl_gen_id,     # Background gene set\n  OrgDb = org.Mm.eg.db,                         # Annotation database\n  keyType = 'ENSEMBL',                          # Key type for gene identifiers\n  readable = TRUE,                              # Convert gene IDs to gene names\n  ont = \"BP\",                                   # Ontology: can be \"BP\", \"MF\", \"CC\", or \"ALL\"\n  pvalueCutoff = 0.05,                          # P-value cutoff for significance\n  qvalueCutoff = 0.10                           # Q-value cutoff for significance\n)\n\n\n# Visualize the enriched GO terms\nfun_enrich$dotplot &lt;- \n  enrichplot::dotplot(fun_enrich$ego, showCategory = 20, title = \"GO BP | Enrichment barplot\")\n\nfun_enrich$heatplot &lt;- \n  enrichplot::heatplot(fun_enrich$ego, showCategory = 10, \n                       foldChange = fun_enrich$de_genes_fc$log2FoldChange) +\n  ggplot2::ggtitle(\"GO BP | Enrichment heatplot\")\n\nfun_enrich$emapplot &lt;- \n  enrichplot::emapplot(pairwise_termsim(fun_enrich$ego), showCategory = 15, layout = \"nicely\")\n\nfun_enrich$cnetplot &lt;- \n  enrichplot::cnetplot(fun_enrich$ego, categorySize = \"pvalue\", showCategory = 5, \n                                 layout = \"nicely\", foldChange = fun_enrich$de_genes_fc$log2FoldChange)\n\nfun_enrich$treeplot &lt;- \n  enrichplot::treeplot(enrichplot::pairwise_termsim(fun_enrich$ego), \n                       showCategory = 20, nCluster=5, offset = rel(2)) + \n  ggplot2::ggtitle(\"GO BP | Enrichment treeplot\") + \n  ggplot2::theme(text = element_text(size = 8))\n\n\n# Combine the enrichment plots into panels from a single figure\n(fun_enrich$dotplot) |\n(fun_enrich$emapplot / fun_enrich$cnetplot)\n\n\n\n\n\n\n\n\n\nCode\nfun_enrich$treeplot / fun_enrich$heatplot",
    "crumbs": [
      "Spring 2025",
      "RNA-Seq Data Analysis in R | From Counts to Biological Insights"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "2025_spring/250604_iduarte/250604_iduarte_intro.html",
    "href": "2025_spring/250604_iduarte/250604_iduarte_intro.html",
    "title": "RNA-Seq Data Analysis in R | From Counts to Biological Insights",
    "section": "",
    "text": "RNA-seq is a leading method for quantifying RNA levels in biological samples, leveraging next-generation sequencing (NGS) technologies. The process begins with RNA extraction and conversion to cDNA, followed by sequencing to produce reads representing the RNA present in a sample.\n\nRNA-seq overviewDifferential expressionFunctional analysis\n\n\nLab Protocol Overview\n\n\n\nRNA-seq data (i.e. reads) are processed through a standard workflow with three main stages:\n\nData pre-processing – improves read quality by removing contaminants and adapters.\nAlignment and quantification – maps reads to a reference genome and estimates gene expression, either through traditional or faster lightweight methods.\nDifferential expression analysis – identifies and visualizes genes with significant expression differences.\n\nAdditional downstream analyses (e.g., functional enrichment, co-expression, or multi-omics integration) are popular ways to derive biological insights from these analyses.\n\n\n\nAdapted from: https://nf-co.re/rnaseq/dev/docs/usage/differential_expression_analysis/theory\n\n\nNote | As shown in the above scheme, this course will not cover the first two steps. It will begin with a gene count matrix and proceed with differential expression analysis, visualization, and a brief overview of functional enrichment.\n\n\nDifferential expression (DE) analysis compares gene expression levels across conditions (e.g., disease vs. healthy) to identify genes with statistically significant changes. This is typically done using tools like DESeq2, a robust R package designed for analyzing RNA-seq count data.\n\nInput Requirements:\n\nA count matrix (genes × samples).\nA metadata table describing sample attributes.\n\nQuality Control:\n\nUse PCA and hierarchical clustering to explore variation and detect outliers.\nTransform counts using variance stabilizing transformation (vst) or regularized log (rlog) to ensure comparable variance across genes, improving downstream analysis.\n\nFiltering:\n\nRemove genes with low or zero counts to improve sensitivity and reduce false positives.\n\nDesign Formula:\n\nSpecifies how gene counts depend on experimental factors.\nCan include main conditions and covariates (e.g., gender, batch, stage).\nExample:\ndesign = ~ condition\ndesign = ~ gender + developmental_stage + condition\nThe main factor of interest is usually placed last for clarity.\n\n\n\nDE with DESeq2\nDESeq2 is a widely used R package for identifying differentially expressed (DE) genes from RNA-seq count data. RNA-seq data typically exhibit many low-count genes and a long-tailed distribution due to highly expressed genes, requiring specialized statistical modeling. The major steps in DESeq2 are the following:\n\nNormalization\n\nAdjusts for sequencing depth and RNA composition using size factors calculated via the median ratio method.\nNormalized counts are used for visualization but raw counts must be used for DESeq2 modeling.\n\nDispersion Estimation\n\nRNA-seq data show overdispersion (variance &gt; mean).\nDESeq2 models count data using the negative binomial distribution.\nDispersion is estimated:\n\nGlobally (common dispersion),\nPer gene (gene-wise dispersion),\nThen refined through shrinkage toward a fitted mean-dispersion curve to improve stability, especially with small sample sizes.\n\nGenes with extreme variability are not shrunk to avoid false positives.\n\nModel Fitting and Hypothesis Testing\n\nA generalized linear model (GLM) is fit to each gene’s normalized counts.\nDESeq2 tests whether gene expression differs significantly between groups:\n\nWald test for simple comparisons (e.g., treated vs. control),\nLikelihood Ratio Test (LRT) for more complex designs with multiple variables.\n\nEach test returns a log2 fold change and a p-value.\n\nMultiple Testing Correction\n\nTo control for false positives from testing thousands of genes, DESeq2 adjusts p-values using Benjamini-Hochberg FDR correction.\nAn FDR cutoff of &lt;0.05 means that 5% of DE genes may be false positives.\n\n\n\n\n\nAfter identifying differentially expressed (DE) genes, functional analysis helps interpret their biological relevance by uncovering the pathways, processes, or interactions they may be involved in. This includes:\n\nFunctional enrichment analysis – identifies overrepresented biological processes, molecular functions, cellular components, or pathways.\nNetwork analysis – groups genes with similar expression patterns to reveal potential interactions.\n\nThis course focuses on Over-Representation Analysis (ORA), a common enrichment method that uses the hypergeometric test to assess whether certain biological pathways or gene sets are statistically enriched in the DE gene list.\nKey components of ORA:\n\nUniverse – the full set of genes considered (e.g., all genes in the genome).\nGene Set – a group of genes annotated to a particular function or pathway (e.g., from Gene Ontology).\nGene List – the list of DE genes identified in the analysis.\n\nThe test evaluates whether the overlap between the DE gene list and a gene set exceeds what would be expected by chance, pointing to potentially meaningful biological mechanisms.\nTools commonly used for functional enrichment include Gene Ontology, KEGG, Reactome, clusterProfiler, and g:Profiler. These support the biological interpretation of DE results and help uncover pathways affected by the experimental condition."
  },
  {
    "objectID": "2025_spring/250604_iduarte/250604_iduarte_nfcore.html",
    "href": "2025_spring/250604_iduarte/250604_iduarte_nfcore.html",
    "title": "RNA-Seq Data Analysis in R | From Counts to Biological Insights",
    "section": "",
    "text": "Differential Expression Analysis with DESeq2\nIn this tutorial, we will guide you through the practical steps necessary to set up the RStudio project, load the required packages and data, execute the DESeq2 analysis, and derive biological insights from the DE results.\n\n\n\n0. RStudio Proj1. Setup2. Quality control3. Differential Expression4. Visualise DE results5. Functional analysis\n\n\nAs with any analysis, the first step is to create a folder to store your work.\n\nChoose an appropriate location on your computer, then set up the following folder structure:\n\n  rnaseq_counts2bio_course/    \n  ├── data/    \n  ├── de_results/    \n  ├── scripts/    \n  └── sandbox/   \n\n\n\n\n\n\nTip: Set Up a Clear Folder Structure Early\n\n\n\n\n\nBefore collecting data, define a clear folder structure and file naming convention. This improves organization, avoids confusion, and supports collaboration. A consistent setup helps you and your team quickly locate and understand files.\nStart organized - your future self (and collaborators) will thank you!\nSuggested minimal structure for a data analysis project:\nproject_name_no_spaces_no_special_chars/    \n├── data/            # Raw and processed data\n│   ├── processed/         \n│   └── raw/   \n├── output/          # Figures, tables \n├── results/         # Analysis results, with appropriate sub-folders\n├── scripts/         # Analysis and processing code\n└── sandbox/         # Exploratory work (not for sharing)\n\n\n\n\nCreate a new RStudio project inside the rnaseq_counts2bio_course folder:\n\n\n2.1 Go to the File menu and select New Project;\n2.2 Select Existing Directory;\n2.3 Navigate to the course directory rnaseq_counts2bio_course and click on Create Project;\n2.4 The new project will be automatically opened in RStudio, and inherits the directory name.\n\n\nWe can check whether we are in the correct working directory with getwd().\n\nNext, go to the File menu, select New File and then R Markdown to create a notebook style script file, using literate programming, in which we will save all the R code required for this analysis.\n\n\n3.1 In the Title write: Differential expression analysis with DESeq2, choose HTML as Default Output Format, and insert the author name.\n3.2 Save the file as de_analysis.Rmd inside the scripts folder.\n3.3 Delete the example markdown code, except the YAML header (the first lines between ---), and the setup code chunk.\n\nFrom now on, each command described in the course will be added to this script.\n\n\n\n\nOur case study\n\n\n\n\nArtificial Gravity Attenuates the Transcriptomic Response to Spaceflight in the Optic Nerve and Retina\n\nProlonged exposure to microgravity in space poses risks to eye health. To explore a potential countermeasure, researchers exposed mice on the International Space Station to varying levels of artificial gravity (0, 0.33, 0.67, and 1G) using centrifugation. After returning the mice to Earth, RNA-seq of their optic nerve and retina revealed that microgravity triggers gene expression changes. Adding artificial gravity on board the ISS can attenuate the transcriptomic response to microgravity in a dose-dependent manner. Such attenuation may effectively mitigate spaceflight-induced detrimental effects on ocular tissue.\n\nData repository: NASA Open Science for Life in Space\nDOI: https://doi.org/10.26030/d6dj-d777\nExperimental factors: Spaceflight, Altered gravity\nOrganism: Mus musculus (mouse)\nAssay: transcription profiling\n\nTechnology: RNA Sequencing (RNA-Seq)\nPlatform: Illumina\n\n\n\n\n\n\n\n\nHands-on tutorial\n\n\n\n1.1 Load packages and data\nIf you are a Windows user, please install first RTools which match your exact R version.\n\n# # Install the required packages if not already installed\n# install.packages(c(\"pak\"))\n# \n# pak::pkg_install(c(\"BiocManager\", \"remotes\", \"here\", \"tidyverse\",        \n# \"DESeq2\", \"pheatmap\", \"RColorBrewer\", \"ggrepel\", \"clusterProfiler\",\n# \"enrichplot\", \"org.Mm.eg.db\", \"patchwork\", \"ComplexHeatmap\"\n# ))\n# \n# # Install the course data package\n# pak::pak(\"patterninstitute/OSD758\")\n\n\n# Load packages\nlibrary(\"here\")            # package to find your current working directory\nlibrary(\"tidyverse\")       # packages for data manipulation and visualization\nlibrary(\"DESeq2\")          # differential expression analysis\nlibrary(\"pheatmap\")        # heatmaps\nlibrary(\"RColorBrewer\")    # color palettes\nlibrary(\"ggrepel\")         # repel overlapping text labels in ggplot2 plots\nlibrary(\"clusterProfiler\") # for enrichment analysis\nlibrary(\"enrichplot\")      # to draw functional enrichment results\nlibrary(\"org.Mm.eg.db\")    # mouse gene annotation database\nlibrary(\"patchwork\")         # combining multiple plots\nlibrary(\"ComplexHeatmap\")  # to draw heatmaps\n\n# Install and load package containing the data\nlibrary(OSD758)\n\n# Gene expression in Counts\nraw_counts &lt;- OSD758::gene_expression(format = \"wide\", only_expressed_genes = TRUE) \n# View(raw_counts)\n\n# Samples metadata\nsamples &lt;- OSD758::samples()\n# View(samples)\n\n\n\n\n\n\nThe first step in any data analysis pipeline is quality control (QC) to check for data issues, and ensure the data is suitable for downstream analyses.\n\n\n2.1 Variance stabilization data transformation\nFor QC analysis, it is useful to work with transformed versions of the count data, variance-stabilised (vst) or regularised log-transformed (rlog) counts. While, the rlog is more robust to outliers and extreme values, vst is computationally faster and so preferred for larger datasets.\n\n\n\n\n\n\nInfo: The rlog() and the vst() functions from DESeq2\n\n\n\n\n\nThe rlog and the vst transformations have an argument, blind that can be set to:\n\nTRUE (default): useful for QC analysis because it re-estimates the dispersion, allowing for comparison of samples in an unbiased manner with respect to experimental conditions;\nFALSE: the function utilizes the already estimated dispersion, generally applied when differences in counts are expected to be due to the experimental design.\n\n\n\n\n\n\n\n\n\n\nAttention: vst and rlog Transformations are Used for Visualization Only\n\n\n\n\n\nVariance stabilization transformations are used for visualisation purposes only. Differential expression analysis using DESeq2 requires raw, unnormalized counts (not TPMs, RPKMs, or FPKMs).\n\n\n\n\n# Create a list to save the QC results\nqc &lt;- list()\n\n# You can choose between vst() and rlog() - this tutorial uses vst.\nqc$vst &lt;- DESeq2::vst(raw_counts, blind = TRUE)\n\n\n\n2.2 Principal Component Analysis\nCheck which variables from the experimental conditions are the major source of variation.\n\n# Run PCA\nqc$pca_vst &lt;- prcomp(t(qc$vst)) \n\n# Extract the components\nqc$components &lt;- qc$pca_vst[[\"x\"]]\nqc$components &lt;- tibble::as_tibble(qc$components, rownames = \"sample_id\")\n\n# Add sample annotations to components for plot coloring\nqc$components_annot &lt;-\n  dplyr::left_join(qc$components, as.data.frame(samples[, c(1,5,6,8)]), by = \"sample_id\") |&gt;\n  dplyr::relocate(spacecraft, acceleration_source, gravity_class, .after = sample_id)\n\n# Calculate the % variance per component\nqc$pca_percent_var &lt;- round(qc$pca_vst$sdev^2/sum(qc$pca_vst$sdev^2)*100)\n\n#\n# 2D PCA | Using ggplot2\n#\n\n# Color by gravity_class\nqc$pca_gravity &lt;-\n  ggplot(qc$components_annot, aes(x = PC1, y = PC2, color = gravity_class)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"PCA gene expression | Colored by gravity_class\",\n    x = paste0(\"PC1 (\", qc$pca_percent_var[1], \"% variance)\"),\n    y = paste0(\"PC2 (\", qc$pca_percent_var[2], \"% variance)\")\n  ) +\n  theme_minimal()\n\n# Color by accelaration_source\nqc$pca_acceleration &lt;-\n  ggplot(qc$components_annot, aes(x = PC1, y = PC2, color = acceleration_source)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"PCA gene expression | Colored by acceleration_source\",\n    x = paste0(\"PC1 (\", qc$pca_percent_var[1], \"% variance)\"),\n    y = paste0(\"PC2 (\", qc$pca_percent_var[2], \"% variance)\")\n  ) +\n  theme_minimal()\n\n# Color by gravity_class and shape by acceleration_source\nqc$pca_gravity_acceleration &lt;-\n  ggplot(qc$components_annot, aes(x = PC1, y = PC2, \n                               color = gravity_class,\n                               shape = acceleration_source)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"PCA gene expression | Colored by gravity_class | Shape acceleration_source\",\n    x = paste0(\"PC1 (\", qc$pca_percent_var[1], \"% variance)\"),\n    y = paste0(\"PC2 (\", qc$pca_percent_var[2], \"% variance)\")\n  ) +\n  theme_minimal()\n\n\n# Assemble pca plots\nqc$pca_gravity_acceleration /\n(qc$pca_gravity | qc$pca_acceleration)\n\n\n\n\n\n\n\n\n\n\n2.3 Hierarchical clustering\nCheck how similar the replicates are to each other.\n\nDistance between samples (in gene expression space) - Euclidean distance.\n\n\n# Plot sample to sample distance for hierarchical clustering\n\n# Calculate Euclidean distances between samples (rows) by transposing the matrix with t().\nqc$sample_dist_matrix &lt;- as.matrix(dist(t(qc$vst), method = \"euclidean\"))\n\n\n# Define a color palette for the heatmap\nqc$colors &lt;- colorRampPalette(rev(brewer.pal(9, \"Greens\")))(255) # function from RColorBrewer package\n\n# Create the heatmap\nqc$dist_clustering &lt;- pheatmap::pheatmap(\n  qc$sample_dist_matrix,\n  cluster_rows = TRUE,\n  cluster_cols = TRUE,\n  col = qc$colors,\n  fontsize_col = 8,\n  fontsize_row = 5\n)\n\n\n\n\n\n\n\n\n\nCorrelation between samples.\n\n\n### Compute pairwise correlation values\nqc$sample_corr &lt;- cor(qc$vst)\n\n### Plot heatmap using the correlation matrix\nqc$corr_clustering &lt;-\n  pheatmap::pheatmap(\n    qc$sample_corr,\n    cluster_rows = TRUE,\n    cluster_cols = TRUE,\n    fontsize_row = 5,\n    fontsize_col = 8\n  )\n\n\n\n\n\n\n\n\n\n\n3.1 Check if the data and metadata sample ids match\nTo avoid errors in DESeq2 is essential to check that sample names match between the colData and the countData, and that the samples are in the exact same order.\n\n# Create list to save the analysis objects\nde_deseq &lt;- list()\n\n# Check that sample ids match between raw_counts and samples \n# Ensure same content\nstopifnot(setequal(colnames(raw_counts), samples$sample_id))\n\n# Reorder columns to match sample order\nraw_counts &lt;- raw_counts[, samples$sample_id]\n\n\n\n3.2 Differential Expression with DESeq2\nThe calculation of the differential expression using DESeq2 requires raw (unnormalized) integer counts, a sample metadata table with experimental conditions, and a design formula specifying the variables for model fitting.\nThis will generate a dds object.\n\n# Make sure the factor levels are ordered so that the desired baseline comes first.\n# DESeq2 uses the first level from factors as the baseline.\nsamples &lt;-\n  samples |&gt;\n  dplyr::mutate(gravity_class = factor(\n    gravity_class,\n    levels = c(\"1.00_G\", \"0.33_G\", \"0.66_G\", \"micro_G\")\n  ))\n\n\n# DE Step 1: Create a DESeqDataSet object (dds)\nde_deseq$dds &lt;- DESeq2::DESeqDataSetFromMatrix(\n  countData = raw_counts,\n  colData = samples,\n  design = ~ gravity_class\n)\n\n# DE Step 2: Run the DESeq function to perform the analysis\nde_deseq$dds &lt;- DESeq(de_deseq$dds)\n\n\n\n\n\n\n\nOptional: DESeq() function can be separated into individual steps\n\n\n\n\n\nThe DESeq() function is a high-level wrapper that simplifies the process of differential expression analysis by combining multiple steps into a single function call. This makes the workflow more user-friendly and ensures that all necessary pre-processing and statistical steps are executed in the correct order. The key functions that DESeq2 calls include:\n\nestimateSizeFactors: to normalise the count data;\nestimateDispersion: to estimate the dispersion;\nnbinomWaldTest: to perform differential expression test.\n\nThe individual functions can be carried out also singularly as shown below:\n\n# Differential expression analysis step-by-step\nde_deseq$dds_stepwise &lt;- DESeq2::estimateSizeFactors(de_deseq$dds)\n\nde_deseq$dds_stepwise &lt;- DESeq2::estimateDispersions(de_deseq$dds_stepwise)\n\nde_deseq$dds_stepwise &lt;- DESeq2::nbinomWaldTest(de_deseq$dds_stepwise)\n\n\n\n\n\n\n\n\n\nOptional: Pre-filtering of low count genes is sometimes performed\n\n\n\n\n\nBefore running the different steps of the analysis, sometimes its is advisable to pre-filter the genes to remove those with very low counts. This is useful to improve computational efficiency and enhance interpretability. In general, it is reasonable to keep only genes with sum counts of at least 10 for a minimal number of 3 samples. Here is the optional code.\n# Pre-filtering\n\n# Select a minimal number of samples = 3\nsmallestGroupSize &lt;- 3\n\n# Select genes with sum counts of at least 10 in 3 samples\nkeep &lt;- rowSums(counts(de_deseq$dds) &gt;= 10) &gt;= smallestGroupSize\n\n# Keep only the genes that pass the threshold\nde_deseq$dds_filtered &lt;- de_deseq$dds[keep,]\n\n\n\n\n\n3.3 Inspect the dds object\n\n\n\n\n\n\nInfo: dds object from DESeq2\n\n\n\n\n\nIn DESEq2, the dds object is a central data structure that contains the following components:\n\ncountData: a matrix of raw count data, where each row represents a gene and each column represents a sample;\ncolData: a data frame containing information about the samples, such as the experimental design, treatment and other relevant metadata;\ndesign: a formula specifying the experimental design used to estimate the dispersion and the log2 fold change.\n\n\n\n\n\n# Check the design formula\nDESeq2::design(de_deseq$dds) \n\n# Check the sample info\nSummarizedExperiment::colData(de_deseq$dds) \n\n# Display the first rows of the raw counts\nhead(DESeq2::counts(de_deseq$dds))\n\n# Display the first rows of the normalised counts to compare with raw counts \nhead(DESeq2::counts(de_deseq$dds, normalized = TRUE))\n\n# Convert the normalised counts from the DESeq2 object to a tibble\nnormalised_counts &lt;- tibble::as_tibble(DESeq2::counts(de_deseq$dds, normalized = TRUE),\n                                       rownames = \"ensembl_gen_id\")\nhead(normalised_counts)\n\n\n\n3.4 Extract the Differential Expression results\n\n\n\n\n\n\nInfo: The results() function from DESeq2\n\n\n\n\n\nThe results() function in DESeq2 is used to extract the results of the DE analysis. This function takes the dds object as input and returns a DataFrame containing the results of the analysis:\n\nbaseMean: the average expression level of the gene across all samples;\nlog2FoldChange: the log2 fold change of the gene between the condition of interest and the reference level;\nlfcSE: the standard error of the log2 fold change;\nstat: the Wald statistic, which is used to calculate the p-value;\npvalue: the p-value from the Wald test indicates the probability of observing the measured difference in gene expression (log2 fold change) by chance, assuming no true difference exists (null hypothesis). A low p-value suggests that the observed expression change between samples is unlikely due to random chance, so we can reject the null hypothesis –&gt; the gene is differentially expressed;\npadj: the adjusted p-value, which takes into account multiple testing corrections, (Benjamini-Hochberg method default) to control the false discovery rate.\n\nThe results() function returns the results for all genes in the analysis with an adjusted p-value below a specific FDR cutoff, set by default to 0.1. This threshold can be modified with the parameter alpha. The results() function can also be customised to filter the results based on certain criteria (log2 fold change or padj) or to set a specific contrast (specific comparison between two or more levels).\n\n\n\n\n# Find the names of the estimated effects (coefficients) of the model\nDESeq2::resultsNames(de_deseq$dds)\n\n# Extract DE results for each gravity condition vs 1.00 G\n    # The results function by default applies the Benjamini-Hochberg method to control FDR\nde_deseq$res_033_vs_1G &lt;- DESeq2::results(de_deseq$dds, name = \"gravity_class_0.33_G_vs_1.00_G\")\nde_deseq$res_066_vs_1G &lt;- DESeq2::results(de_deseq$dds, name = \"gravity_class_0.66_G_vs_1.00_G\")\nde_deseq$res_micro_vs_1G &lt;- DESeq2::results(de_deseq$dds, name = \"gravity_class_micro_G_vs_1.00_G\")\n\n\n# Summarise the results:\n  # Shows the number of tested genes, the number up- and down-regulated (at alpha),\n  # and how many were excluded by multiple testing due to low counts.\nDESeq2::summary(de_deseq$res_033_vs_1G)\nDESeq2::summary(de_deseq$res_066_vs_1G)\nDESeq2::summary(de_deseq$res_micro_vs_1G)\n\n\n\n\n\n\n\nImportant: Extracting results from contrasts\n\n\n\n\n\nWhen more than one variable is used in the design formula, and you want to manually specify the comparison of interest, you should run the following command:\nmy_results &lt;- DESeq2::results(dds, contrast = c(\"variable_name\", \"condition_of_interest\", \"reference_condition\"))\nExample:\nIf the design is ~ tissue + condition, and you want to compare the levels \"treated\" vs \"control\" within the variable \"condition\": my_results &lt;- DESeq2::results(dds, contrast = c(\"condition\", \"treated\", \"control\"))\nThis will extract the log2 fold change of \"treated\" relative to \"control\", controlling for the effect of the other variables in the design.\n\n\n\n\n\n3.5 Select significant DE results\n\n\n\n\n\n\nInfo: The Order of the Contrasts Determines the Fold Change Signal\n\n\n\n\n\nThe order of the contrast names determines the direction of the fold change that is reported in the results. Specifically, the first level of the contrast is the condition of interest and the second level is the reference level.\n\n\n\n\n# Extract significant results (padj &lt; 0.05) and convert to tibble\nde_deseq$sig_033_vs_1G &lt;-\n  de_deseq$res_033_vs_1G |&gt;\n  tibble::as_tibble(rownames = \"ensembl_gen_id\") |&gt;\n  dplyr::filter(!is.na(padj), padj &lt; 0.05) |&gt;\n  dplyr::arrange(padj, log2FoldChange)\n\nde_deseq$sig_066_vs_1G &lt;-\n  de_deseq$res_066_vs_1G |&gt;\n  tibble::as_tibble(rownames = \"ensembl_gen_id\") |&gt;\n  dplyr::filter(!is.na(padj), padj &lt; 0.05) |&gt;\n  dplyr::arrange(padj, log2FoldChange)\n\nde_deseq$sig_micro_vs_1G &lt;-\n  de_deseq$res_micro_vs_1G |&gt;\n  tibble::as_tibble(rownames = \"ensembl_gen_id\") |&gt;\n  dplyr::filter(!is.na(padj), padj &lt; 0.05) |&gt;\n  dplyr::arrange(padj, log2FoldChange)\n\n# Look at the top results\nhead(de_deseq$sig_033_vs_1G)\nhead(de_deseq$sig_066_vs_1G)\nhead(de_deseq$sig_micro_vs_1G)\n\n\n\nAfter differential expression analysis, the next step is to visualize the data. This helps reveal patterns not obvious from the raw numbers.\nThe next sections show common plots used in RNA-seq analysis.\nDue to time constraints, we will focus on the differential expression between microgravity and Earth’s gravity (1G).\n\n\n\n\n\n\nOptional: MA plots and counts plots\n\n\n\n\n\n\nMA plot: scatter plot commonly utilised to visualise the results of the DE analysis for all the samples. The plot displays the mean of the normalised counts on the x-axis and the log2 fold change on the y-axis. This allows the visualisation of the relationship between the magnitude of the fold change and the mean expression level of the genes. Genes that are differentially expressed will appear farthest from the horizontal line, while genes with low expression levels will appear closer to the line.\ncounts plot: plot of the normalised counts for a single gene across the different conditions in your experiment. It’s particularly useful for visualising the expression levels of specific genes of interest and comparing them across sample groups.\n\n# Generate the MA plot\nplotMA(res, ylim = c(-2, 2))\n\n# Plot a specific gene in this case Xist, a DE gene\n# plotCounts(de_deseq$dds, gene = \"Xist\")\n\n\n\n\n\n4.1 Heatmap\n\n\n\n\n\n\nInfo: Heatmaps\n\n\n\n\n\nHeatmaps plot of the normalised counts for all the significant genes. The heatmap provides insights into genes and sample relationships that may not be apparent from individual gene plots alone.\n\n\n\n\n# List to save all the visualization plots\nde_plots &lt;- list()\n\n# Extract only gene ids from the significant results\nsig_gene_ids &lt;- de_deseq$sig_micro_vs_1G$ensembl_gen_id\n\n# Map between ENSEMBL gene ids and gene symbol\nensembl2symbol &lt;- OSD758::gene_expression(\"long\") |&gt;\n  dplyr::select(ensembl_gen_id, gene_symbol) |&gt;\n  dplyr::distinct()\n  \n# Get normalised counts for significant genes \nsig_normalised_counts &lt;- normalised_counts |&gt;\n  dplyr::filter(ensembl_gen_id %in% sig_gene_ids) |&gt;\n  dplyr::left_join(ensembl2symbol, by = \"ensembl_gen_id\") |&gt;\n  dplyr::select(-ensembl_gen_id) |&gt;\n  tibble::column_to_rownames(\"gene_symbol\") |&gt;\n  as.matrix()\n\n\n# Scale each row: subtract mean and divide by SD.\n# The 2 transpositions are required because, by default, scale applies to the columns.\nsig_normalised_counts_scaled &lt;- t(scale(t(sig_normalised_counts)))   # scale rows, not columns\n\n# Find min and max values to get meaningful colors in heatmaps\nrange(sig_normalised_counts_scaled)\n\n[1] -3.324583  5.724152\n\n# Complex heatmap\nde_plots$ht &lt;- ComplexHeatmap::Heatmap(sig_normalised_counts_scaled[1:200, ],\n                        name = \"Exprs (z-score)\",\n                        column_title = \"Microgravity vs Earth's Gravity (1G) | Top 200 DE genes\",\n                        cluster_columns = TRUE,\n                        cluster_rows = TRUE,\n                        # number of clusters in K-means to split rows\n                        row_km = 2,\n                        # add cluster names\n                        row_title = c(\"A\", \"B\"),\n                        row_title_rot = 90,\n                        row_gap = unit(2, \"mm\"),\n                        # number of clusters in K-means to split columns\n                        column_km = 3,\n                        column_gap = unit(2, \"mm\"),\n                        border = \"grey\",\n                        na_col = \"white\",\n                        # Color range (min and max values from sig_normalised_counts_scaled)\n                        col = circlize::colorRamp2(c(-4, 0, 6), c(\"skyblue3\", \"white\", \"forestgreen\")),\n                        column_names_gp = grid::gpar(fontsize = 9),\n                        row_names_gp = grid::gpar(fontsize = 5),\n                        rect_gp = grid::gpar(col = \"grey\", lwd = 0.5))\n\n# Print the plot\nComplexHeatmap::draw(de_plots$ht, heatmap_legend_side = \"right\")\n\n\n\n\n\n\n\n\n\n\n4.2 Volcano plot\n\n\n\n\n\n\nInfo: Volcano plots\n\n\n\n\n\nVolcano plots scatter plot that displays the log2 fold change on the x-axis and the log transformed padj on the y-axis. This allows for the visualisation of both the magnitude and significance of the changes in gene expression between two conditions. Genes that are differentially expressed (i.e., have a large log2 fold change) and are statistically significant (i.e., have a low padj) will appear in the left (downregulated genes) or in the right (upregulated genes) corners of the plot making easier their identification.\n\n\n\n\n# Add a column with differential expression status and add gene symbol to the results\nsig_res_annot &lt;- \n  de_deseq$sig_micro_vs_1G |&gt;\n  dplyr::mutate(diffexpressed = case_when(\n    log2FoldChange &gt; 1 & padj &lt; 0.05 ~ 'upregulated',\n    log2FoldChange &lt; -1 & padj &lt; 0.05 ~ 'downregulated',\n    TRUE ~ 'not_de')) |&gt;\n  dplyr::left_join(ensembl2symbol, by = \"ensembl_gen_id\") |&gt;\n  dplyr::select(-ensembl_gen_id) |&gt;\n  # add gene symbols\n  dplyr::relocate(gene_symbol, .before =  1L) |&gt;\n  dplyr::arrange(padj, log2FoldChange)\n\n\n# Create a volcano plot using ggplot2\nde_plots$volcano_plot &lt;-\n  ggplot(data = sig_res_annot, aes(\n    x = log2FoldChange,\n    y = -log10(padj),\n    col = diffexpressed))+\n  geom_point(size = 0.6) +\n  geom_text_repel(data = filter(sig_res_annot, \n                                ((abs(log2FoldChange) &gt; log2(8)) & (padj &lt; -log10(0.05)))), \n                  aes(label = gene_symbol), size = 2.5, max.overlaps = Inf) +\n  ggtitle(\"DE genes micro gravity versus Earth's gravity\") +\n  geom_vline(xintercept = c(-1, 1), col = \"black\", linetype = 'dashed', linewidth = 0.2) +\n  geom_hline(yintercept = -log10(0.05), col = \"black\", linetype = 'dashed', linewidth = 0.2) +\n  theme(plot.title = element_text(size = rel(1.25), hjust = 0.5),\n        axis.title = element_text(size = rel(1))) +\n  scale_color_manual(values = c(\"upregulated\" = \"red\",\n                                \"downregulated\" = \"blue\",\n                                \"not_de\" = \"grey\")) +\n  labs(color = 'DE genes') +\n  xlim(-5, 5) +   # Caution: This hides some genes\n  ylim(0, 7.5) +  # Caution: This hides some genes\n  theme_light()\n\n# Print the volcano plot\nde_plots$volcano_plot\n\n\n\n\n\n\n\n\n\n\nDifferential expression analysis yields a list of significant DE genes, which can be explored further through downstream analyses like functional enrichment and network analysis to uncover biological mechanisms.\nThis tutorial focuses on Over-Representation Analysis (ORA), a method for identifying enriched pathways or processes among DE genes.\n\n\n\n\n\n\nInfo: The hypergeometric test in ORA\n\n\n\n\n\nThe underlying statistic behind ORA is the hypergeometric test, which considers three key components:\n\nUniverse: the background list of genes (for example the genes annotated in a genome);\nGeneSet: a collection of genes annotated by a reference database (such as Gene Ontology), and known to be involved in a particular biological pathway or process;\nGene List: the differentially expressed genes.\n\nThe hypergeometric test calculates the probability of observing a certain number of genes from the gene set (pathway or process) within the gene list (DE genes) by chance. An important aspect of this analysis is the concept of membership. It defines the relationship between DE genes and genes from the analysed gene set. By knowing which genes belong to which pathway/process, we can determine whether the observed overlap between DE genes and the particular pathway/process is greater than what would be expected by random chance.\n\n\n\n\n\n5.1 Enrichment analysis\n\n# Enrichment analysis (ORA)\n\n# Create a list to save the enrichment analysis results\nfun_enrich &lt;- list()\n\n# Prepare list of significant DE genes in descending Log2FoldChange\nfun_enrich$de_genes_fc &lt;-\n  de_deseq$sig_micro_vs_1G |&gt;\n  dplyr::select(ensembl_gen_id, log2FoldChange) |&gt;\n  dplyr::arrange(dplyr::desc(log2FoldChange))\n\n# Run GO enrichment analysis using the enrichGO function\nfun_enrich$ego &lt;- clusterProfiler::enrichGO(\n  gene = fun_enrich$de_genes_fc$ensembl_gen_id, # Genes of interest\n  universe = ensembl2symbol$ensembl_gen_id,     # Background gene set\n  OrgDb = org.Mm.eg.db,                         # Annotation database\n  keyType = 'ENSEMBL',                          # Key type for gene identifiers\n  readable = TRUE,                              # Convert gene IDs to gene names\n  ont = \"BP\",                                   # Ontology: can be \"BP\", \"MF\", \"CC\", or \"ALL\"\n  pvalueCutoff = 0.05,                          # P-value cutoff for significance\n  qvalueCutoff = 0.10                           # Q-value cutoff for significance\n)\n\n\n# Visualize the enriched GO terms\nfun_enrich$dotplot &lt;- \n  enrichplot::dotplot(fun_enrich$ego, showCategory = 20, title = \"GO BP | Enrichment barplot\")\n\nfun_enrich$heatplot &lt;- \n  enrichplot::heatplot(fun_enrich$ego, showCategory = 10, \n                       foldChange = fun_enrich$de_genes_fc$log2FoldChange) +\n  ggplot2::ggtitle(\"GO BP | Enrichment heatplot\")\n\nfun_enrich$emapplot &lt;- \n  enrichplot::emapplot(pairwise_termsim(fun_enrich$ego), showCategory = 15, layout = \"nicely\")\n\nfun_enrich$cnetplot &lt;- \n  enrichplot::cnetplot(fun_enrich$ego, categorySize = \"pvalue\", showCategory = 5, \n                                 layout = \"nicely\", foldChange = fun_enrich$de_genes_fc$log2FoldChange)\n\nfun_enrich$treeplot &lt;- \n  enrichplot::treeplot(enrichplot::pairwise_termsim(fun_enrich$ego), \n                       showCategory = 20, nCluster=5, offset = rel(2)) + \n  ggplot2::ggtitle(\"GO BP | Enrichment treeplot\") + \n  ggplot2::theme(text = element_text(size = 8))\n\n\n# Combine the enrichment plots into panels from a single figure\n(fun_enrich$dotplot) |\n(fun_enrich$emapplot / fun_enrich$cnetplot)\n\n\n\n\n\n\n\nfun_enrich$treeplot / fun_enrich$heatplot"
  },
  {
    "objectID": "2025_spring/250507_taires.html",
    "href": "2025_spring/250507_taires.html",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "Last change\n\n\n\n\n\nJuly 28, 2025\n\n\n\nThis workshop will provide an overview of microbiome data analysis from Illumina amplicon sequencing raw data to data processing, visualization and statistics. Participants will learn basic concepts and tools to preprocess (QIIME2) and analyse (MicrobiomeAnalyst) microbiome data, in particular bacteria associated with marine organisms.\nIn a coral associated microbiome dataset, where we have two different coral species (with one incipient speciation) and two different reproductive strategies, we want to address the following scientific questions:\n\nIs the microbiome of these cold-water corals species-specific?\nIs the microbiome related to the reproductive strategy?\nOr is the microbiome shaped by both factors?\n\n\n\nThis tutorial requires you to be able to run QIIME2. We provide you with access to redi server, where it is preinstalled. However if you want to set it up yourself, the full instructions areavailable at qiime2 webpage. Beware that below is UNIX specific setup. We did these parts:\n\n\n\nDownload installer from anaconda\nYour downloaded file is executable (.exe on Win, .pkg on Mac and .sh on Linux). Run the install\n\n\n\n\n# update your conda\nconda update conda\n\n# create conda environment for amplicon qiime2\nconda env create -n qiime2-amplicon-2024.10 --file https://data.qiime2.org/distro/amplicon/qiime2-amplicon-2024.10-py310-linux-conda.yml\n\n\n\n\nIn the repository, you can find almost all the necessary input files, including pdf version of Tania’s introductory presentation.\nDownload the files to your local mahcine one by one from Github. Or better, setup yourself a Gihub account and you can clone the whole biohap repository.\n\n\nTo get the data tables from the server to your PC or opposite, ssh protocol exists.\n\nLinux: should be part of the core installation\nPutty on Windows, or ssh client\nMac: part of the install\n\nOnce you have ssh setup, do ssh &lt;USERNAME&gt;@10.36.5.158, and then enter your password. You are the username connecting(@) to the server (10.36.5.158, redi).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you are not at the University internet (eduroam or fixed), first you will need to connect to it via UAlg VPN service.\n\n\n\nIn your /home/&lt;USERNAME&gt; directory (you can see the location using pwd command), create a new folder\nmkdir workshop_qiime2\n\n# and navigate to it\ncd workshop_qiime2\nTo copy the files to the server, instead of standard cp we need to use secure scp. Note that the server address is most probbably different but the structure of the command is &lt;USERNAME&gt;@&lt;SERVER IP address&gt;:&lt;ABSOLUTE-PATH&gt;\n# scp from to, -r means with the subfolder structure (recursive)\nscp -r \"&lt;path-to-your-downloads-perhaps&gt;\\Samples\"  &lt;USERNAME&gt;@10.36.5.158:/home/&lt;USERNAME&gt;/workshop_qiime2/\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote different slashes used on different operating systems. On windows, the path will be something like “C:”, macOS similar to “/Users/USERNAME/Downloads/”, unix “/home/USERNAME/Downloads”\n\n\n\nTransfer the other files in the similar manner.\n\n\n\n\n\n\nTip\n\n\n\n\n\nEvery time you are lost on the server, use pwd to print current directory you are in, ls to list contents of the directory and cd to navigate to other directories (going up the tree one level with cd ..)\n\n\n\n\n\n\nFirst look at your manifest file, which specifies sample ids, and reads filenames, in this case we use paired ends, which means two file_paths per line are present.\ncat Manifest_file_Species.txt\nThe path needs to be changed to conform with your file system location. You see weird ^M characters, let’s remove those\n\nEither use dos2unix Manifest_file_Species.txt\nOr replace them using sed\n\n# \\r/ refers to Windows line ending\nsed -i 's/\\r//g' Manifest_file_Species.txt\nRun cat command again to confirm successful substitution. Select part of the path you want to replace next.\n\nSelect with the cursor the path (this automatically enters into your clipboard), or right mouse click and select copy\nprobably something like /Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/\n\nNow we substitute the string with correct path\n\n\n\n\n\n\nTip\n\n\n\n\nMiddle mouse button click pastes your mouse selection, ie clipboard on the command line\nAlternatively also right click and select paste.\nYou can get your correct path using pwd to print current folder path (assuming you are where your Samples folder is)\n\n\n\nsed -i 's|/Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/|&lt;YOUR_PATH_TO_FOLDER_Samples&gt;|g' Manifest_file_Species.txt\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that since our strings contained / character we used | to separate the parts of the sed command. That was not necessary in the case above s/\\r//g where / plays a role of the separator\nsed command s: substitute our flag g means replace all occurance on the line.\ns/pattern/subtitution/flags\n\n\n\nConfirm again using cat that your manifest file is correct.\n\n\n\n\nQIIME2 is installed in a sort of container (conda environment), which allows you to work on different project which need different dependencies in parallel. First, activate the correct conda environment, which has QIIME2 installed.\nconda activate qiime2-amplicon-2024.10\nYour command line should look similar to (qiime2-amplicon-2024.10) [XXXX@redi ~]$ now. Make sure that you are in your workshop_qiime2 directory where you have also copied the files which serve as inputs for the below section (flags -i, --input-path or similar)\n\n\nqiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path Manifest_file_Species.txt \\\n    --output-path demux.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\ntype describes that we have paired ends with Q scores (fastq) inputs. input-path is our manifest file, describing where all the files are. demux.qza will be our output-path file and input-format describes your sequencing technology, PairedEndFastq means that your data was sequenced with Forward and Reverse primers and you have two fastq files per sample that should be joined. Phred33 refers to the type of Phred scores in your fastq file. This is the lastest version and used for most recent Illumina sequencers. V2 is the version of the manifest file formatting we conform to.\n\n\n\nqiime demux summarize --i-data demux.qza --o-visualization demux.qzv\n\n--i-data: input, which is output of importing data previously\n--o-visualization: output filename\n\n\n\n\nEvery time you create .qzv table, it is to be visualized at QIIME2 webpage for quality control and taking decisions for next steps. Open QIIME2 webpage and drag the qzv files in there. Use the ssh as described above to get the data to your PC.\nThis is easiest to do from your local machine side, for which you need to open additional powershell for Windows or terminal for mac users (NOT logged in to the server).\nscp &lt;USERNAME&gt;@10.36.5.158:/home/&lt;USERNAME&gt;/workshop_qiime2/demux.qzv \"absolute-path-on-your-PC-to-folder-you-want-your-file\"\nGo to QIIME2 and drag any .qzv file to visualize it.\n\n\n\ndada2 is an R package, which will be used here. This step can take substantial time and resource.\nqiime dada2 denoise-paired \\\n    --i-demultiplexed-seqs demux.qza \\\n    --p-trim-left-f x \\\n    --p-trim-left-r y \\\n    --p-trunc-len-f z \\\n    --p-trunc-len-r n \\\n    --p-n-threads 0 \\\n    --o-table table.qza \\\n    --o-representative-sequences rep-seqs.qza \\\n    --o-denoising-stats denoising-stats.qza \\\n    --verbose\n\n\n\n\n\n\nTrimming\n\n\n\nNote: When choosing where to trim the sequences, after looking at the demux file, besides taking into account that low quality nucleotides should be removed, we need to make sure that the forward (R1) and reverse (R2) sequences, after trimming, can still overlap. A good overlap may be between 20 and 100bp.\nLet’s say:\n\nYour amplicon is 450 bp\nYour reads are 250 bp each (paired-end)\n\nIf both reads are high-quality all the way: 250 (R1) + 250 (R2) = 500 bp, with expected overlap: ~50 bp, 🍏 But if you truncate reads too aggressively (e.g., with –p-trunc-len-f 200 –p-trunc-len-r 180), R1 becomes 200 bp, R2 becomes 180 bp. Then:\n200 + 180 = 380 &lt; 450 The sequences won’t be able to overlap,🍎\nIf, for example, all your reverse reads are very low quality (usually the quality of R2 reads are lower than R1) the analysis can be done using only R1 sequences as single-end sequences. But first, complain to your sequencing centre.\n\n\n\n--p-trim-left-f: xxx (see demux.qzv file)\n--p-trim-left-r: xxx (see demux.qzv file)\n--p-trunc-len-f: xxx (see demux.qzv file)\n--p-trunc-len-r: xxx (see demux.qzv file)\n--p-n-threads 0: xxx (the number of CPUs threads the tool should use during processing, zero means “use all the threads available”)\n--o-table: output table filename\n--o-representative-sequences: representative sequences file name\n--o-denoising-stats: statistics filename specification\n--verbose: write extensive progress on the screen (since this script does several things it is advisable to run it so we can follow the flow)\n\n\n\n\n\n\n\nSample output on the screen\n\n\n\n\n\nR version 4.3.3 (2024-02-29) Loading required package: Rcpp DADA2: 1.30.0 / Rcpp: 1.0.13.1 / RcppParallel: 5.1.9 2) Filtering …………………… 3) Learning Error Rates 208608800 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 219039240 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 3) Denoise samples …………………… …………………… 5) Remove chimeras (method = consensus) 6) Report read numbers through the pipeline 7) Write output Saved FeatureTable[Frequency] to: table.qza Saved FeatureData[Sequence] to: rep-seqs.qza Saved SampleData[DADA2Stats] to: denoising-stats.qza\n\n\n\n\n\n\nCreate .qzv files for visualizations again.\nqiime metadata tabulate \\\n    --m-input-file denoising-stats.qza \\\n    --o-visualization denoising-stats.qzv\nArguments specify input and output files.\nqiime feature-table summarize \\\n    --i-table table.qza \\\n    --o-visualization table.qzv\nAnd finally tabulate the sequences.\nqiime feature-table tabulate-seqs \\\n    --i-data rep-seqs.qza \\\n    --o-visualization rep-seqs.qzv\nUse your scp skills again to transfer denoising-stats.qzv, rep-seqs.qzv and table.qzv to your PC. Again, do it from your PC’s command line.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nscp &lt;USERNAME&gt;@10.36.5.158:/home/&lt;USERNAME&gt;/workshop_qiime2/*.qzv \"path-to-destination-folder\"\n\n\n\n\n\n\nThe database is now in the repo because of its size, download it. If you work on redi, the database is located at &lt;fill in path DP&gt;. This is also a time-consuming step\nThe classifier needs to match our scikit-learn package version used by the QIIME2 version. At redi, the version is 1.4.2, and we have already put in the shared location path /opt/shared/silva-138-99-nb-classifier.qza.\nIn other use case, download the appropriate classfier from qiime. If you use your own classifier, the below specifications of the classifier might change.\nqiime feature-classifier classify-sklearn \\\n    --i-classifier /opt/shared/silva-138-99-nb-classifier.qza \\\n    --i-reads rep-seqs.qza \\\n    --o-classification taxonomy.qza\n\n\n\nqiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n\n\n\nqiime tools export \\\n    --input-path table.qza \\\n    --output-path feature-table\n\n\n\n\n\n\nNote\n\n\n\nNote: We first create a biom table from the .qza file so we can then integrate the taxonomic assignments. Biom tables can also be used in other programs for downstream analysis if you want.\n\n\n\n\n\nqiime tools export \\\n    --input-path taxonomy.qza \\\n    --output-path taxonomy\nTo move further with the analysis, open the taxonomy file and change the header. When you open it, you’ll see the header looks like this:\nFeature ID Taxon   Confidence\nYou need to change it to this:\n#otu-id    taxonomy    Confidence\n\n\n\n\n\n\nImportant\n\n\n\nALL spaces are tabs\n\n\nCan you figure out the sed command do do that?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nsed -i '1s/.*/#otu-id\\ttaxonomy\\tConfidence/' ~/the_name_of_the_folder_where_you_are_working/taxonomy/taxonomy.tsv\n\n\n\nConfirm if the header has changed\nhead taxonomy/taxonomy.tsv\n\n\n\nbiom add-metadata \\\n    --input-fp feature-table/feature-table.biom \\\n    --observation-metadata-fp taxonomy/taxonomy.tsv \\\n    --output-fp biom-with-taxonomy.biom\n\n\n\nbiom convert \\\n    --input-fp biom-with-taxonomy.biom \\\n    --output-fp ASV_table.tsv \\\n    --to-tsv --header-key taxonomy\nThe output of these scripts will be two different files, a ASV_table_MA.txt file and a taxonomy_MA.txt file that are ready to upload in the MicrobiomeAnalyst program.\n\n\n\n\nUnfortunately the ASV table is not directly compatible with the MA. We need to split it into separate abundance and taxonomy tables. In the GH repo, there are two scripts to do that without going into the details. Both have hardcoded input ASV table name ASV_table.tsv which you need to conform too. Outputs will be ASV_table_MA.txt and taxonomy_MA.txt, respectively.\n\n\n\n\n\n\nPermissions\n\n\n\n\n\nRunning the shell script requires that executable (x) permissions for the file, which probably after scp is not the case. To enable execution, run:\nchmod +x convert_ASV_abundances.sh\nchmod +x convert_ASV_taxonomy.sh\nwith ls, the executable files should appear in green. The scripts accept one and one argument only which points to the ASV_table.tsv generated by QIIME2. Below it assumes your in the directory where the ASV table resides, otherwise include absolute or relative path.\n\n\n\n./convert_ASV_abundances.sh ASV_table.tsv\n./convert_ASV_taxonomy.sh  ASV_table.tsv\nTogether with the provided metadata_MA.txt, which assigns each sample to a coral species, those are the three files you need to scp to your local machine for MA web interface upload. MA can do too many things for you, the subset we will use is described in Tania’s presentation available on GH too.\n\n\n\n\n\n\nImportant\n\n\n\nIt is very important that the metadata file columns are tab separated, and the first column header is #NAME.\n\n\nThe fun starts Microbiome analyst page, Click here to start the session and select Marker data profiling.\n\n\n\nMA inputs\n\n\nYou need to select the first three input files (OTU/ASV table, Metadata file, Taxonomy table), and from the dropdown, choose Taxonomy labels to be QIIME.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#setup",
    "href": "2025_spring/250507_taires.html#setup",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "This tutorial requires you to be able to run QIIME2. We provide you with access to redi server, where it is preinstalled. However if you want to set it up yourself, the full instructions areavailable at qiime2 webpage. Beware that below is UNIX specific setup. We did these parts:\n\n\n\nDownload installer from anaconda\nYour downloaded file is executable (.exe on Win, .pkg on Mac and .sh on Linux). Run the install\n\n\n\n\n# update your conda\nconda update conda\n\n# create conda environment for amplicon qiime2\nconda env create -n qiime2-amplicon-2024.10 --file https://data.qiime2.org/distro/amplicon/qiime2-amplicon-2024.10-py310-linux-conda.yml",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#input-files",
    "href": "2025_spring/250507_taires.html#input-files",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "In the repository, you can find almost all the necessary input files, including pdf version of Tania’s introductory presentation.\nDownload the files to your local mahcine one by one from Github. Or better, setup yourself a Gihub account and you can clone the whole biohap repository.\n\n\nTo get the data tables from the server to your PC or opposite, ssh protocol exists.\n\nLinux: should be part of the core installation\nPutty on Windows, or ssh client\nMac: part of the install\n\nOnce you have ssh setup, do ssh &lt;USERNAME&gt;@10.36.5.158, and then enter your password. You are the username connecting(@) to the server (10.36.5.158, redi).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you are not at the University internet (eduroam or fixed), first you will need to connect to it via UAlg VPN service.\n\n\n\nIn your /home/&lt;USERNAME&gt; directory (you can see the location using pwd command), create a new folder\nmkdir workshop_qiime2\n\n# and navigate to it\ncd workshop_qiime2\nTo copy the files to the server, instead of standard cp we need to use secure scp. Note that the server address is most probbably different but the structure of the command is &lt;USERNAME&gt;@&lt;SERVER IP address&gt;:&lt;ABSOLUTE-PATH&gt;\n# scp from to, -r means with the subfolder structure (recursive)\nscp -r \"&lt;path-to-your-downloads-perhaps&gt;\\Samples\"  &lt;USERNAME&gt;@10.36.5.158:/home/&lt;USERNAME&gt;/workshop_qiime2/\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote different slashes used on different operating systems. On windows, the path will be something like “C:”, macOS similar to “/Users/USERNAME/Downloads/”, unix “/home/USERNAME/Downloads”\n\n\n\nTransfer the other files in the similar manner.\n\n\n\n\n\n\nTip\n\n\n\n\n\nEvery time you are lost on the server, use pwd to print current directory you are in, ls to list contents of the directory and cd to navigate to other directories (going up the tree one level with cd ..)\n\n\n\n\n\n\nFirst look at your manifest file, which specifies sample ids, and reads filenames, in this case we use paired ends, which means two file_paths per line are present.\ncat Manifest_file_Species.txt\nThe path needs to be changed to conform with your file system location. You see weird ^M characters, let’s remove those\n\nEither use dos2unix Manifest_file_Species.txt\nOr replace them using sed\n\n# \\r/ refers to Windows line ending\nsed -i 's/\\r//g' Manifest_file_Species.txt\nRun cat command again to confirm successful substitution. Select part of the path you want to replace next.\n\nSelect with the cursor the path (this automatically enters into your clipboard), or right mouse click and select copy\nprobably something like /Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/\n\nNow we substitute the string with correct path\n\n\n\n\n\n\nTip\n\n\n\n\nMiddle mouse button click pastes your mouse selection, ie clipboard on the command line\nAlternatively also right click and select paste.\nYou can get your correct path using pwd to print current folder path (assuming you are where your Samples folder is)\n\n\n\nsed -i 's|/Users/microbiomes/Documents/Microbiomes/Coral_Microbiome_Workshop_sps_comparison/|&lt;YOUR_PATH_TO_FOLDER_Samples&gt;|g' Manifest_file_Species.txt\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that since our strings contained / character we used | to separate the parts of the sed command. That was not necessary in the case above s/\\r//g where / plays a role of the separator\nsed command s: substitute our flag g means replace all occurance on the line.\ns/pattern/subtitution/flags\n\n\n\nConfirm again using cat that your manifest file is correct.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#qiime2",
    "href": "2025_spring/250507_taires.html#qiime2",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "QIIME2 is installed in a sort of container (conda environment), which allows you to work on different project which need different dependencies in parallel. First, activate the correct conda environment, which has QIIME2 installed.\nconda activate qiime2-amplicon-2024.10\nYour command line should look similar to (qiime2-amplicon-2024.10) [XXXX@redi ~]$ now. Make sure that you are in your workshop_qiime2 directory where you have also copied the files which serve as inputs for the below section (flags -i, --input-path or similar)\n\n\nqiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path Manifest_file_Species.txt \\\n    --output-path demux.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\ntype describes that we have paired ends with Q scores (fastq) inputs. input-path is our manifest file, describing where all the files are. demux.qza will be our output-path file and input-format describes your sequencing technology, PairedEndFastq means that your data was sequenced with Forward and Reverse primers and you have two fastq files per sample that should be joined. Phred33 refers to the type of Phred scores in your fastq file. This is the lastest version and used for most recent Illumina sequencers. V2 is the version of the manifest file formatting we conform to.\n\n\n\nqiime demux summarize --i-data demux.qza --o-visualization demux.qzv\n\n--i-data: input, which is output of importing data previously\n--o-visualization: output filename\n\n\n\n\nEvery time you create .qzv table, it is to be visualized at QIIME2 webpage for quality control and taking decisions for next steps. Open QIIME2 webpage and drag the qzv files in there. Use the ssh as described above to get the data to your PC.\nThis is easiest to do from your local machine side, for which you need to open additional powershell for Windows or terminal for mac users (NOT logged in to the server).\nscp &lt;USERNAME&gt;@10.36.5.158:/home/&lt;USERNAME&gt;/workshop_qiime2/demux.qzv \"absolute-path-on-your-PC-to-folder-you-want-your-file\"\nGo to QIIME2 and drag any .qzv file to visualize it.\n\n\n\ndada2 is an R package, which will be used here. This step can take substantial time and resource.\nqiime dada2 denoise-paired \\\n    --i-demultiplexed-seqs demux.qza \\\n    --p-trim-left-f x \\\n    --p-trim-left-r y \\\n    --p-trunc-len-f z \\\n    --p-trunc-len-r n \\\n    --p-n-threads 0 \\\n    --o-table table.qza \\\n    --o-representative-sequences rep-seqs.qza \\\n    --o-denoising-stats denoising-stats.qza \\\n    --verbose\n\n\n\n\n\n\nTrimming\n\n\n\nNote: When choosing where to trim the sequences, after looking at the demux file, besides taking into account that low quality nucleotides should be removed, we need to make sure that the forward (R1) and reverse (R2) sequences, after trimming, can still overlap. A good overlap may be between 20 and 100bp.\nLet’s say:\n\nYour amplicon is 450 bp\nYour reads are 250 bp each (paired-end)\n\nIf both reads are high-quality all the way: 250 (R1) + 250 (R2) = 500 bp, with expected overlap: ~50 bp, 🍏 But if you truncate reads too aggressively (e.g., with –p-trunc-len-f 200 –p-trunc-len-r 180), R1 becomes 200 bp, R2 becomes 180 bp. Then:\n200 + 180 = 380 &lt; 450 The sequences won’t be able to overlap,🍎\nIf, for example, all your reverse reads are very low quality (usually the quality of R2 reads are lower than R1) the analysis can be done using only R1 sequences as single-end sequences. But first, complain to your sequencing centre.\n\n\n\n--p-trim-left-f: xxx (see demux.qzv file)\n--p-trim-left-r: xxx (see demux.qzv file)\n--p-trunc-len-f: xxx (see demux.qzv file)\n--p-trunc-len-r: xxx (see demux.qzv file)\n--p-n-threads 0: xxx (the number of CPUs threads the tool should use during processing, zero means “use all the threads available”)\n--o-table: output table filename\n--o-representative-sequences: representative sequences file name\n--o-denoising-stats: statistics filename specification\n--verbose: write extensive progress on the screen (since this script does several things it is advisable to run it so we can follow the flow)\n\n\n\n\n\n\n\nSample output on the screen\n\n\n\n\n\nR version 4.3.3 (2024-02-29) Loading required package: Rcpp DADA2: 1.30.0 / Rcpp: 1.0.13.1 / RcppParallel: 5.1.9 2) Filtering …………………… 3) Learning Error Rates 208608800 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 219039240 total bases in 1043044 reads from 17 samples will be used for learning the error rates. 3) Denoise samples …………………… …………………… 5) Remove chimeras (method = consensus) 6) Report read numbers through the pipeline 7) Write output Saved FeatureTable[Frequency] to: table.qza Saved FeatureData[Sequence] to: rep-seqs.qza Saved SampleData[DADA2Stats] to: denoising-stats.qza\n\n\n\n\n\n\nCreate .qzv files for visualizations again.\nqiime metadata tabulate \\\n    --m-input-file denoising-stats.qza \\\n    --o-visualization denoising-stats.qzv\nArguments specify input and output files.\nqiime feature-table summarize \\\n    --i-table table.qza \\\n    --o-visualization table.qzv\nAnd finally tabulate the sequences.\nqiime feature-table tabulate-seqs \\\n    --i-data rep-seqs.qza \\\n    --o-visualization rep-seqs.qzv\nUse your scp skills again to transfer denoising-stats.qzv, rep-seqs.qzv and table.qzv to your PC. Again, do it from your PC’s command line.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nscp &lt;USERNAME&gt;@10.36.5.158:/home/&lt;USERNAME&gt;/workshop_qiime2/*.qzv \"path-to-destination-folder\"\n\n\n\n\n\n\nThe database is now in the repo because of its size, download it. If you work on redi, the database is located at &lt;fill in path DP&gt;. This is also a time-consuming step\nThe classifier needs to match our scikit-learn package version used by the QIIME2 version. At redi, the version is 1.4.2, and we have already put in the shared location path /opt/shared/silva-138-99-nb-classifier.qza.\nIn other use case, download the appropriate classfier from qiime. If you use your own classifier, the below specifications of the classifier might change.\nqiime feature-classifier classify-sklearn \\\n    --i-classifier /opt/shared/silva-138-99-nb-classifier.qza \\\n    --i-reads rep-seqs.qza \\\n    --o-classification taxonomy.qza\n\n\n\nqiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n\n\n\nqiime tools export \\\n    --input-path table.qza \\\n    --output-path feature-table\n\n\n\n\n\n\nNote\n\n\n\nNote: We first create a biom table from the .qza file so we can then integrate the taxonomic assignments. Biom tables can also be used in other programs for downstream analysis if you want.\n\n\n\n\n\nqiime tools export \\\n    --input-path taxonomy.qza \\\n    --output-path taxonomy\nTo move further with the analysis, open the taxonomy file and change the header. When you open it, you’ll see the header looks like this:\nFeature ID Taxon   Confidence\nYou need to change it to this:\n#otu-id    taxonomy    Confidence\n\n\n\n\n\n\nImportant\n\n\n\nALL spaces are tabs\n\n\nCan you figure out the sed command do do that?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nsed -i '1s/.*/#otu-id\\ttaxonomy\\tConfidence/' ~/the_name_of_the_folder_where_you_are_working/taxonomy/taxonomy.tsv\n\n\n\nConfirm if the header has changed\nhead taxonomy/taxonomy.tsv\n\n\n\nbiom add-metadata \\\n    --input-fp feature-table/feature-table.biom \\\n    --observation-metadata-fp taxonomy/taxonomy.tsv \\\n    --output-fp biom-with-taxonomy.biom\n\n\n\nbiom convert \\\n    --input-fp biom-with-taxonomy.biom \\\n    --output-fp ASV_table.tsv \\\n    --to-tsv --header-key taxonomy\nThe output of these scripts will be two different files, a ASV_table_MA.txt file and a taxonomy_MA.txt file that are ready to upload in the MicrobiomeAnalyst program.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_spring/250507_taires.html#microbiomeanalyst-ma",
    "href": "2025_spring/250507_taires.html#microbiomeanalyst-ma",
    "title": "Microbiome Analysis of Amplicon Sequencing",
    "section": "",
    "text": "Unfortunately the ASV table is not directly compatible with the MA. We need to split it into separate abundance and taxonomy tables. In the GH repo, there are two scripts to do that without going into the details. Both have hardcoded input ASV table name ASV_table.tsv which you need to conform too. Outputs will be ASV_table_MA.txt and taxonomy_MA.txt, respectively.\n\n\n\n\n\n\nPermissions\n\n\n\n\n\nRunning the shell script requires that executable (x) permissions for the file, which probably after scp is not the case. To enable execution, run:\nchmod +x convert_ASV_abundances.sh\nchmod +x convert_ASV_taxonomy.sh\nwith ls, the executable files should appear in green. The scripts accept one and one argument only which points to the ASV_table.tsv generated by QIIME2. Below it assumes your in the directory where the ASV table resides, otherwise include absolute or relative path.\n\n\n\n./convert_ASV_abundances.sh ASV_table.tsv\n./convert_ASV_taxonomy.sh  ASV_table.tsv\nTogether with the provided metadata_MA.txt, which assigns each sample to a coral species, those are the three files you need to scp to your local machine for MA web interface upload. MA can do too many things for you, the subset we will use is described in Tania’s presentation available on GH too.\n\n\n\n\n\n\nImportant\n\n\n\nIt is very important that the metadata file columns are tab separated, and the first column header is #NAME.\n\n\nThe fun starts Microbiome analyst page, Click here to start the session and select Marker data profiling.\n\n\n\nMA inputs\n\n\nYou need to select the first three input files (OTU/ASV table, Metadata file, Taxonomy table), and from the dropdown, choose Taxonomy labels to be QIIME.",
    "crumbs": [
      "Spring 2025",
      "Microbiome Analysis of Amplicon Sequencing"
    ]
  },
  {
    "objectID": "2025_fall/call.html",
    "href": "2025_fall/call.html",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "Since the world is spinning, the next edition is uncertain. Therefore, we call for your ideas and contributions already now for the Autumn 2025.\n\nLoading…",
    "crumbs": [
      "Fall 2025",
      "Call for contributions and feedback"
    ]
  },
  {
    "objectID": "2025_fall/call.html#call-for-contributions-and-feedback",
    "href": "2025_fall/call.html#call-for-contributions-and-feedback",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "Since the world is spinning, the next edition is uncertain. Therefore, we call for your ideas and contributions already now for the Autumn 2025.\n\nLoading…",
    "crumbs": [
      "Fall 2025",
      "Call for contributions and feedback"
    ]
  },
  {
    "objectID": "biodata_pt/training_2511.html",
    "href": "biodata_pt/training_2511.html",
    "title": "EMO-BON Metagenomics: From Backend Integration to Frontend Processing",
    "section": "",
    "text": "Last change\n\n\n\n\n\nNovember 16, 2025\nThis course should give you set of minimal examples on how to generate a knowledge graph from set of RO-Crates, and use it as a sparQL endpoint either directly or using python rdflib.\nSecond part focused on a specific use case of EMO-BON data, the pilot implementation of the Virtual Research Environment is introduced, basic analysis performed, issues reported, extended analysis proposed.\nWhat is omitted here is how to organize the data and build the RO-Crates themselves.",
    "crumbs": [
      "BioData.pt",
      "EMO-BON Metagenomics: From Backend Integration to Frontend Processing"
    ]
  },
  {
    "objectID": "biodata_pt/training_2511.html#hands-on-tutorial",
    "href": "biodata_pt/training_2511.html#hands-on-tutorial",
    "title": "EMO-BON Metagenomics: From Backend Integration to Frontend Processing",
    "section": "Hands-on tutorial",
    "text": "Hands-on tutorial\n\nBackend data integrationFrontend data processing\n\n\nThere are many parallel technologies to employ in each step. The pipeline here relies on fuseki for exposing the SPARQL endpoint and uses python to query the graph.\n\nRO-CratesFuseki ServerSPARQLPython ImplementationLocal + Public Endpoints\n\n\nA RO-Crate is an integrated view through which you can see an entire Research Object; the methods, the data, the output and the outcomes of a project or a piece of work. Linking all this together enables the sharing of research outputs with their context, as a coherent whole. https://www.researchobject.org\n\n\n\nImage credit: Goble, C. (2024, February 16). FAIR Digital Research Objects: Metadata Journeys. University of Auckland Seminar, Auckland. Zenodo. https://doi.org/10.5281/zenodo.10710142\n\n\nNote that we already have some of the EMO-BON RO-Crates locall in /emobon_demo/ro-crates/analysis-results-cluster-01-crate/.\n\n\nExpose your triples as a SPARQL end-point accessible over HTTP. Fuseki provides REST-style interaction with your RDF data. Navigate to your downloaded fuseki files, open the archive if you have not done it yet and start the server as follows\ncd apache-jena-fuseki-5.6.0/\n\n# start the server\n./fuseki-server\nThis should show you similar to\n15:46:47 INFO  Config          :: Fuseki Base = /home/david-palecek/coding/apache-jena-fuseki-5.6.0/run\n15:46:47 INFO  Config          :: No databases: dir=/home/david-palecek/coding/apache-jena-fuseki-5.6.0/run/configuration\n15:46:48 INFO  Config          :: UI Base = fuseki-server.jar\n15:46:48 INFO  Shiro           :: Shiro configuration: file:/home/david-palecek/coding/apache-jena-fuseki-5.6.0/run/shiro.ini\nNow opening your localhost, http://localhost:3030/, you should see the following page\n\n\n\nfuseki server\n\n\n\nUpload Dataset / Graph\nIn fuseki, go new dataset -&gt; give it a name emobon -&gt; add data -&gt; select the .ttl files and upload all or one by one. This is a shortcut, because many times you will not have access to the .ttl serialized version of the RO-Crate. In that case, the starting point is ro-crate-metadata.json file, which is always at the root of every RO-Crate.\n\n\n\n\n\n\nPseudo python code\n\n\n\n# read the json file and convert it to a graph\nfile = \".../ro-crate-metadata.json\"\nwith open(file, \"r\", encoding=\"utf-8\") as f:\n    jsonld_text = f.read()\ng = jsonld_to_rdflib(jsonld_text)\n\n# upload data to the endpoint\nresp = requests.put(\n  fuseki_url,\n  data=g.serialize(format=\"turtle\").encode(\"utf-8\"),\n  headers={\"Content-Type\": \"text/turtle\"},\n  timeout=60,\n)\n\n\nThe full example is in this notebook.\n\n\n\nThe direct way from fuseki is to edit the query in actions. See more detailed but accesible introduction with examples. When you click query, the default query is shown\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nSELECT * WHERE {\n  ?sub ?pred ?obj .\n} LIMIT 10\n, which queries all the triples in the graph.\n\n\n\n\n\n\nHow many triples do we have?\n\n\n\n\n\nSELECT (COUNT(*) as ?c)\nWHERE {\n  ?subject ?predicate ?object .\n}\nLIMIT 10\n\n\n\n\n\n\n\n\n\nFilter out all the text/html files.\n\n\n\n\n\nPREFIX sdo: &lt;http://schema.org/&gt;\n\nSELECT ?x ?dtype\nWHERE {\n  ?x sdo:encodingFormat ?dtype .\n  FILTER regex(str(?dtype), \"^text/html\", \"i\")\n}\n\n\n\n\n\n\n\n\n\nReturn also the sdo:downloadUrl of those files.\n\n\n\n\n\nPREFIX sdo: &lt;http://schema.org/&gt;\n\nSELECT ?x ?dtype ?durl\nWHERE {\n  ?x sdo:encodingFormat ?dtype ;\n    sdo:downloadUrl ?durl .\n  FILTER regex(str(?dtype), \"^text/html\", \"i\")\n}\n\n\n\nNow you can click the link of one of the Krona files, open them in the browser and with no surprise, it is a Krona plot.\nNow let’s get the real metaGOflow outputs, specifically SSU taxonomy tables. There are several ways how to do it.\n1. RO-Crate browser EMBRC hosts the RO-Crate viewer for the EMO-BON data\n2. Local SPARQL Write a query to get the sdo:downloadUrl links, put them into your browser, which automatically triggers the download. Hint for the exercise below, match regex of the object on “SSU-taxonomy-summary”.\n\n\n\n\n\n\nReturn SSU taxonomy download links.\n\n\n\n\n\nPREFIX sdo: &lt;http://schema.org/&gt;\n\nSELECT ?subject ?predicate ?object ?durl\nWHERE {\n  ?subject ?predicate ?object .\n  FILTER regex(str(?object), \"SSU-taxonomy-summary\", \"i\")\n  OPTIONAL { ?object sdo:downloadUrl ?durl }\n}\nLIMIT 50\n\n\n\n3. Use data version control (DVC) tool Shown in the python implementation of the above, which follows in the next section. For more on dvc, check its documentation.\n\n\nIt is possible to export the tables form the fuseki for subsequent work, but let’s do everything seamlessly from a jupyter notebook.\nFirst we reproduce what we did until now from Jupyter notebook which is part of the momics-demos repository, therefore you have it already locally with all installed dependencies too.\nSince we have already ingested the triples from the RO-Crates, we just need to query the existing endpoint\nq = \"\"\"\nSELECT (COUNT(*) AS ?c) \nWHERE { \n  ?s ?p ?o\n}\n\"\"\"\n\nr = requests.get(\"http://localhost:3030/emobon/query\", params={\"query\": q}, headers={\"Accept\": \"application/sparql-results+json\"})\nprint(r.json())\nreturned json is relatively easy to convert to a dataframe (sparql_json_to_df function)\ndef sparql_json_to_df(sparql_json):\n    \"\"\"\n    Convert a SPARQL SELECT query JSON result to a pandas DataFrame.\n    \n    Parameters\n    ----------\n    sparql_json : dict\n        JSON returned by Fuseki / SPARQL endpoint with Accept: application/sparql-results+json\n    \n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    vars_ = sparql_json.get(\"head\", {}).get(\"vars\", [])\n    rows = []\n\n    for binding in sparql_json.get(\"results\", {}).get(\"bindings\", []):\n        row = {}\n        for var in vars_:\n            # Some results might not bind all variables\n            if var in binding:\n                row[var] = binding[var][\"value\"]\n            else:\n                row[var] = None\n        rows.append(row)\n\n    df = pd.DataFrame(rows, columns=vars_)\n    return df\n\n\nSecond notebook includes all the steps of setting up the SparQL endpoint and also combining local queries with public endpoints from wikidata and UniProt.\n\n\n\n\n\n\nTip\n\n\n\nThere is a python wrapper for SPARQL SPARQLWrapper, which is pip installable and can be used as a standard python module but also as a command line script.\n\n\n\n\n\n\n\n\nBecause (a) public SPARQL endpoint with all the EMO-BON data, we completaly separate this second part from the first part, and data starting point will be standard .csv tables, just compressed into .parquet files. Since Blue-Cloud 2026 virtual research environment does not exist, we will run all the analysis locally.\n\nBasic Panel DashboardDeeper Interactive ModeHackathon\n\n\nAll dependencies were installed in the setup. Initialize the jupyter server with\ncd momics-demos\npython -m jupyterlab\nOpen wf2_diversity/diversities_panel.ipynb. It looks and it is a native python notebook code. However clicking the panel icon  initiates the dashboard.\nAfter getting familiar with the functionality, pick your favourite combination of taxon level + categorical / numerical variables showing certain pattern on the first two PCA components.\n\n\nEvery dashboard is also available for interactive mode ..._interactive.ipynb. After discovering one of the main drivers, we would like to see what drives this difference.\nSelect one sample from each category and try to identify the taxa responsible from the variance.\nPossible options\n\nOne approach could be the permonova test removing the taxa one by one. Permonova is implemented in marine-omics-methods marine-omics on PyPI.\nAlternatively compute correlation of each taxon with the PC1 and PC2, rank the features\nFit features as vectors onto PCoA (envfit). This is the most widely used method, especially in ecology (vegan package in R). In Python (via scikit-bio):\n\n    from skbio.stats.ordination import cca_scores\n\n    # envfit equivalent not built-in, but you can manually regress:\n    import scipy.stats as st\n\n    results = {}\n    for feature in abund.columns:\n        slope_x, _, r_x, _, _ = st.linregress(coords[\"PC1\"], abund[feature])\n        slope_y, _, r_y, _, _ = st.linregress(coords[\"PC2\"], abund[feature])\n        results[feature] = (r_x, r_y)\n\n    vectors = pd.DataFrame(results, index=[\"r_PC1\", \"r_PC2\"]).T\n\n\nThis is a free time to try your own ideas with the data building upon the existing functionality of the momics-demos. Do not hesitate to raise issues, and reach out.",
    "crumbs": [
      "BioData.pt",
      "EMO-BON Metagenomics: From Backend Integration to Frontend Processing"
    ]
  },
  {
    "objectID": "biodata_pt/intro.html",
    "href": "biodata_pt/intro.html",
    "title": "Training on accessing, processing, and curating datasets 2025",
    "section": "",
    "text": "EMO-BON establishes a long-term omics observatory for marine biodiversity through bimonthly sampling of coastal waters and both soft and hard sediments across more than 20 stations in Europe. Standard operating procedures are applied for sample collection, sequencing, and workflow analysis to ensure consistent taxonomic and functional annotation. FAIR principles are implemented using appropriate ontologies, RO-Crates, and a Python-based data analysis toolkit prepared for deployment in Virtual Research Environments (VREs).\n\n\nOnline session hosted online on 3rd November (slides)\n\n\n\nFaculty of Pharmacy of the University of Porto, 17th November during the ELIXIR Portugal All Hands 2025",
    "crumbs": [
      "BioData.pt",
      "Training on accessing, processing, and curating datasets 2025"
    ]
  },
  {
    "objectID": "biodata_pt/intro.html#session-1",
    "href": "biodata_pt/intro.html#session-1",
    "title": "Training on accessing, processing, and curating datasets 2025",
    "section": "",
    "text": "Online session hosted online on 3rd November (slides)",
    "crumbs": [
      "BioData.pt",
      "Training on accessing, processing, and curating datasets 2025"
    ]
  },
  {
    "objectID": "biodata_pt/intro.html#session-2",
    "href": "biodata_pt/intro.html#session-2",
    "title": "Training on accessing, processing, and curating datasets 2025",
    "section": "",
    "text": "Faculty of Pharmacy of the University of Porto, 17th November during the ELIXIR Portugal All Hands 2025",
    "crumbs": [
      "BioData.pt",
      "Training on accessing, processing, and curating datasets 2025"
    ]
  },
  {
    "objectID": "biodata_pt/251117_part1.html",
    "href": "biodata_pt/251117_part1.html",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "There are many parallel technologies to employ in each step. The pipeline here relies on fuseki for exposing the SPARQL endpoint and uses python to query the graph.\n\nRO-CratesFuseki ServerSPARQLPython ImplementationLocal + Public Endpoints\n\n\nA RO-Crate is an integrated view through which you can see an entire Research Object; the methods, the data, the output and the outcomes of a project or a piece of work. Linking all this together enables the sharing of research outputs with their context, as a coherent whole. https://www.researchobject.org\n\n\n\nImage credit: Goble, C. (2024, February 16). FAIR Digital Research Objects: Metadata Journeys. University of Auckland Seminar, Auckland. Zenodo. https://doi.org/10.5281/zenodo.10710142\n\n\nNote that we already have some of the EMO-BON RO-Crates locall in /emobon_demo/ro-crates/analysis-results-cluster-01-crate/.\n\n\nExpose your triples as a SPARQL end-point accessible over HTTP. Fuseki provides REST-style interaction with your RDF data. Navigate to your downloaded fuseki files, open the archive if you have not done it yet and start the server as follows\ncd apache-jena-fuseki-5.6.0/\n\n# start the server\n./fuseki-server\nThis should show you similar to\n15:46:47 INFO  Config          :: Fuseki Base = /home/david-palecek/coding/apache-jena-fuseki-5.6.0/run\n15:46:47 INFO  Config          :: No databases: dir=/home/david-palecek/coding/apache-jena-fuseki-5.6.0/run/configuration\n15:46:48 INFO  Config          :: UI Base = fuseki-server.jar\n15:46:48 INFO  Shiro           :: Shiro configuration: file:/home/david-palecek/coding/apache-jena-fuseki-5.6.0/run/shiro.ini\nNow opening your localhost, http://localhost:3030/, you should see the following page\n\n\n\nfuseki server\n\n\n\nUpload Dataset / Graph\nIn fuseki, go new dataset -&gt; give it a name emobon -&gt; add data -&gt; select the .ttl files and upload all or one by one. This is a shortcut, because many times you will not have access to the .ttl serialized version of the RO-Crate. In that case, the starting point is ro-crate-metadata.json file, which is always at the root of every RO-Crate.\n\n\n\n\n\n\nPseudo python code\n\n\n\n# read the json file and convert it to a graph\nfile = \".../ro-crate-metadata.json\"\nwith open(file, \"r\", encoding=\"utf-8\") as f:\n    jsonld_text = f.read()\ng = jsonld_to_rdflib(jsonld_text)\n\n# upload data to the endpoint\nresp = requests.put(\n  fuseki_url,\n  data=g.serialize(format=\"turtle\").encode(\"utf-8\"),\n  headers={\"Content-Type\": \"text/turtle\"},\n  timeout=60,\n)\n\n\nThe full example is in this notebook.\n\n\n\nThe direct way from fuseki is to edit the query in actions. See more detailed but accesible introduction with examples. When you click query, the default query is shown\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nSELECT * WHERE {\n  ?sub ?pred ?obj .\n} LIMIT 10\n, which queries all the triples in the graph.\n\n\n\n\n\n\nHow many triples do we have?\n\n\n\n\n\nSELECT (COUNT(*) as ?c)\nWHERE {\n  ?subject ?predicate ?object .\n}\nLIMIT 10\n\n\n\n\n\n\n\n\n\nFilter out all the text/html files.\n\n\n\n\n\nPREFIX sdo: &lt;http://schema.org/&gt;\n\nSELECT ?x ?dtype\nWHERE {\n  ?x sdo:encodingFormat ?dtype .\n  FILTER regex(str(?dtype), \"^text/html\", \"i\")\n}\n\n\n\n\n\n\n\n\n\nReturn also the sdo:downloadUrl of those files.\n\n\n\n\n\nPREFIX sdo: &lt;http://schema.org/&gt;\n\nSELECT ?x ?dtype ?durl\nWHERE {\n  ?x sdo:encodingFormat ?dtype ;\n    sdo:downloadUrl ?durl .\n  FILTER regex(str(?dtype), \"^text/html\", \"i\")\n}\n\n\n\nNow you can click the link of one of the Krona files, open them in the browser and with no surprise, it is a Krona plot.\nNow let’s get the real metaGOflow outputs, specifically SSU taxonomy tables. There are several ways how to do it.\n1. RO-Crate browser EMBRC hosts the RO-Crate viewer for the EMO-BON data\n2. Local SPARQL Write a query to get the sdo:downloadUrl links, put them into your browser, which automatically triggers the download. Hint for the exercise below, match regex of the object on “SSU-taxonomy-summary”.\n\n\n\n\n\n\nReturn SSU taxonomy download links.\n\n\n\n\n\nPREFIX sdo: &lt;http://schema.org/&gt;\n\nSELECT ?subject ?predicate ?object ?durl\nWHERE {\n  ?subject ?predicate ?object .\n  FILTER regex(str(?object), \"SSU-taxonomy-summary\", \"i\")\n  OPTIONAL { ?object sdo:downloadUrl ?durl }\n}\nLIMIT 50\n\n\n\n3. Use data version control (DVC) tool Shown in the python implementation of the above, which follows in the next section. For more on dvc, check its documentation.\n\n\nIt is possible to export the tables form the fuseki for subsequent work, but let’s do everything seamlessly from a jupyter notebook.\nFirst we reproduce what we did until now from Jupyter notebook which is part of the momics-demos repository, therefore you have it already locally with all installed dependencies too.\nSince we have already ingested the triples from the RO-Crates, we just need to query the existing endpoint\nq = \"\"\"\nSELECT (COUNT(*) AS ?c) \nWHERE { \n  ?s ?p ?o\n}\n\"\"\"\n\nr = requests.get(\"http://localhost:3030/emobon/query\", params={\"query\": q}, headers={\"Accept\": \"application/sparql-results+json\"})\nprint(r.json())\nreturned json is relatively easy to convert to a dataframe (sparql_json_to_df function)\ndef sparql_json_to_df(sparql_json):\n    \"\"\"\n    Convert a SPARQL SELECT query JSON result to a pandas DataFrame.\n    \n    Parameters\n    ----------\n    sparql_json : dict\n        JSON returned by Fuseki / SPARQL endpoint with Accept: application/sparql-results+json\n    \n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    vars_ = sparql_json.get(\"head\", {}).get(\"vars\", [])\n    rows = []\n\n    for binding in sparql_json.get(\"results\", {}).get(\"bindings\", []):\n        row = {}\n        for var in vars_:\n            # Some results might not bind all variables\n            if var in binding:\n                row[var] = binding[var][\"value\"]\n            else:\n                row[var] = None\n        rows.append(row)\n\n    df = pd.DataFrame(rows, columns=vars_)\n    return df\n\n\nSecond notebook includes all the steps of setting up the SparQL endpoint and also combining local queries with public endpoints from wikidata and UniProt.\n\n\n\n\n\n\nTip\n\n\n\nThere is a python wrapper for SPARQL SPARQLWrapper, which is pip installable and can be used as a standard python module but also as a command line script."
  },
  {
    "objectID": "biodata_pt/251117_part2.html",
    "href": "biodata_pt/251117_part2.html",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "Because (a) public SPARQL endpoint with all the EMO-BON data, we completaly separate this second part from the first part, and data starting point will be standard .csv tables, just compressed into .parquet files. Since Blue-Cloud 2026 virtual research environment does not exist, we will run all the analysis locally.\n\nBasic Panel DashboardDeeper Interactive ModeHackathon\n\n\nAll dependencies were installed in the setup. Initialize the jupyter server with\ncd momics-demos\npython -m jupyterlab\nOpen wf2_diversity/diversities_panel.ipynb. It looks and it is a native python notebook code. However clicking the panel icon  initiates the dashboard.\nAfter getting familiar with the functionality, pick your favourite combination of taxon level + categorical / numerical variables showing certain pattern on the first two PCA components.\n\n\nEvery dashboard is also available for interactive mode ..._interactive.ipynb. After discovering one of the main drivers, we would like to see what drives this difference.\nSelect one sample from each category and try to identify the taxa responsible from the variance.\nPossible options\n\nOne approach could be the permonova test removing the taxa one by one. Permonova is implemented in marine-omics-methods marine-omics on PyPI.\nAlternatively compute correlation of each taxon with the PC1 and PC2, rank the features\nFit features as vectors onto PCoA (envfit). This is the most widely used method, especially in ecology (vegan package in R). In Python (via scikit-bio):\n\n    from skbio.stats.ordination import cca_scores\n\n    # envfit equivalent not built-in, but you can manually regress:\n    import scipy.stats as st\n\n    results = {}\n    for feature in abund.columns:\n        slope_x, _, r_x, _, _ = st.linregress(coords[\"PC1\"], abund[feature])\n        slope_y, _, r_y, _, _ = st.linregress(coords[\"PC2\"], abund[feature])\n        results[feature] = (r_x, r_y)\n\n    vectors = pd.DataFrame(results, index=[\"r_PC1\", \"r_PC2\"]).T\n\n\nThis is a free time to try your own ideas with the data building upon the existing functionality of the momics-demos. Do not hesitate to raise issues, and reach out."
  },
  {
    "objectID": "biodata_pt/251117_part1.html#outline",
    "href": "biodata_pt/251117_part1.html#outline",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "",
    "text": "Pull finished ro-crates\nfusekin setup\ngenerate a graph\nbasics of SPARQL\nsetup NB for sparQL querries locally\nFilter the emobon data"
  },
  {
    "objectID": "biodata_pt/training_2511.html#outline",
    "href": "biodata_pt/training_2511.html#outline",
    "title": "EMO-BON Metagenomics: From Backend Integration to Frontend Processing",
    "section": "",
    "text": "Pull finished ro-crates\nfusekin setup\ngenerate a graph\nbasics of SPARQL\nsetup NB for sparQL querries locally\nFilter the emobon data",
    "crumbs": [
      "BioData.pt",
      "EMO-BON Metagenomics: From Backend Integration to Frontend Processing"
    ]
  },
  {
    "objectID": "biodata_pt/251117_part1.html#pull-ro-crates",
    "href": "biodata_pt/251117_part1.html#pull-ro-crates",
    "title": "BioHap: Bioinformatics, Handy and Practical",
    "section": "Pull RO-Crates",
    "text": "Pull RO-Crates\nA RO-Crate is an integrated view through which you can see an entire Research Object; the methods, the data, the output and the outcomes of a project or a piece of work. Linking all this together enables the sharing of research outputs with their context, as a coherent whole. [https://www.researchobject.org/ro-crate/about_ro_crate]\nImage credit: Goble, C. (2024, February 16). FAIR Digital Research Objects: Metadata Journeys. University of Auckland Seminar, Auckland. Zenodo. https://doi.org/10.5281/zenodo.10710142"
  },
  {
    "objectID": "biodata_pt/training_2511.html#pull-ro-crates",
    "href": "biodata_pt/training_2511.html#pull-ro-crates",
    "title": "EMO-BON Metagenomics: From Backend Integration to Frontend Processing",
    "section": "",
    "text": "A RO-Crate is an integrated view through which you can see an entire Research Object; the methods, the data, the output and the outcomes of a project or a piece of work. Linking all this together enables the sharing of research outputs with their context, as a coherent whole. [https://www.researchobject.org/ro-crate/about_ro_crate]\nImage credit: Goble, C. (2024, February 16). FAIR Digital Research Objects: Metadata Journeys. University of Auckland Seminar, Auckland. Zenodo. https://doi.org/10.5281/zenodo.10710142",
    "crumbs": [
      "BioData.pt",
      "EMO-BON Metagenomics: From Backend Integration to Frontend Processing"
    ]
  },
  {
    "objectID": "biodata_pt/training_2511.html#environmental-setup",
    "href": "biodata_pt/training_2511.html#environmental-setup",
    "title": "EMO-BON Metagenomics: From Backend Integration to Frontend Processing",
    "section": "Environmental setup",
    "text": "Environmental setup\nThis setup serves is ubiquitus for both parts of the tutorial. We will clone two repositories and install one of them. To keep everything exactly the same, we create a designated folder\nmkdir emobon_demo\ncd emobon_demo\nCreate a dedicated conda/python environment\n# if you are using conda\nconda create -n \"emobon\" python=3.10  # or higher\nconda activate emobon\n\nPython resources\n# create a folder\nmkdir momics-demos\ncd momics-demos\n\n# clone the repository into newly created folder\ngit clone https://github.com/emo-bon/momics-demos.git\n\n# step into the repository\ncd momics-demos\n\n# install dependencies using pip\npip install -e .\n\n# setup jupyter kernel\nipython kernel install --user --name \"emobon\"\n\n\nBackend setup:\nAs we are going to work with EMO-BON metagenomics data, one way is to download some of the RO-Crates GitHub repository manually.\nEasier, however. is to clone the whole repository. Navigate back to the ..../emobon_demo/ and do:\nmkdir ro-crates\ncd ro-crates\n\ngit clone https://github.com/emo-bon/analysis-results-cluster-01-crate.git\nAs you see in the GH as well, you have now the RO-Crates per sample and corresponding .ttl files to simplify your life.\nWe will use fuseki server for a SPARQL endpoint. It is a java ndpoint, therefore, you might need to install java first if java -version command does not return anything or your version is &lt;17.0.\n\nLinuxWindows\n\n\nsudo apt update\nsudo apt install -y openjdk-17-jre-headless\njava -version\nHere comes the fuseki download itself. Please chose appropriate folder for this. For direct download visit this page.\nwget https://dlcdn.apache.org/jena/binaries/apache-jena-fuseki-5.6.0.tar.gz\n\n# open the archive\ntar -xvf apache-jena-fuseki-5.6.0.tar.gz\n\n\nFor java, if you have winget, you can follow steps here (not tested by the authors). Otherwise, download OpenJDK .exe file from here and install it. You might need to close and reopen the command line window to see the updated version with java --version command in your PowerShell.\nDownload directly the fuseki 5.6.0 zip file from apache and extract the archive.",
    "crumbs": [
      "BioData.pt",
      "EMO-BON Metagenomics: From Backend Integration to Frontend Processing"
    ]
  }
]